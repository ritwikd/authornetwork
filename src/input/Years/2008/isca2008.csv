"http://ieeexplore.ieee.org/search/searchresult.jsp?bulkSetSize=2000&queryText%3DComputer+Architecture%2C+2008.+ISCA+%2708.+35th+International+Symposium+on",2015/06/23 15:56:18
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN","ISBN","EISBN","DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"ISCA-35 Sponsors","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","xix","xix","The conference organizers greatly appreciate the support of the various corporate sponsors listed.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.48","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556710","","","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Counting Dependence Predictors","Roesner, F.; Burger, D.; Keckler, S.W.","Dept. of Comput. Sci., Univ. of Texas at Austin, Austin, TX","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","215","226","Modern processors rely on memory dependence prediction to execute load instructions as early as possible, speculating that they are not dependent on an earlier, unissued store. To date, the most sophisticated dependence predictors, such as Store Sets, have been tightly coupled to the fetch and execution streams, requiring global knowledge of the in-flight stream of stores to synchronize loads with specific stores. This paper proposes a new dependence predictor design, called a Counting Dependence Predictor (CDP). The key feature of CDPs is that the prediction mechanism predicts some set of events for which a particular dynamic load should wait, which may include some number of matching stores. By waiting for local events only, this dependence predictor can work effectively in a distributed microarchitecture where centralized fetch and execution streams are infeasible or undesirable. We describe and evaluate a distributed Counting Dependence Predictor and protocol that achieves 92% of the performance of perfect memory disambiguation. It outperforms a load-wait table, similar to the Alpha 21264, by 11%. Idealized, centralized implementations of Store Sets and the Exclusive Collision Predictor, both of which would be difficult to implement in a distributed microarchitecture, achieve 97% and 94% of oracular performance, respectively.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.6","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556728","dependence prediction;memory systems;multiprocessor and multicore architectures","Computer aided instruction;Computer architecture;Costs;Counting circuits;Delay;Microarchitecture;Microprocessors;Routing protocols;Tiles;Wire","microprocessor chips;resource allocation;storage management","counting dependence predictors;distributed microarchitecture;load instructions;load-wait table;memory dependence prediction;memory disambiguation;store sets","","4","1","23","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Globally-Synchronized Frames for Guaranteed Quality-of-Service in On-Chip Networks","Lee, J.W.; Man Cheuk Ng; Asanovic, K.","Comput. Sci. & Artificial Intell. Lab., Massachusetts Inst. of Technol., Cambridge, MA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","89","100","Future chip multiprocessors (CMPs) may have hundreds to thousands of threads competing to access shared resources, and will require quality-of-service (QoS) support to improve system utilization. Although there has been significant work in QoS support within resources such as caches and memory controllers, there has been less attention paid to QoS support in the multi-hop on-chip networks that will form an important component in future systems. In this paper we introduce globally-synchronized frames (GSF), a framework for providing guaranteed QoS in on-chip networks in terms of minimum bandwidth and a maximum delay bound. The GSF framework can be easily integrated in a conventional virtual channel (VC) router without significantly increasing the hardware complexity. We rely on a fast barrier network, which is feasible in an on-chip environment, to efficiently implement GSF. Performance guarantees are verified by both analysis and simulation. According to our simulations, all concurrent flows receive their guaranteed minimum share of bandwidth in compliance with a given bandwidth allocation. The average throughput degradation of GSF on a 8times8 mesh network is within 10% compared to the conventional best-effort VC router in most cases.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.31","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556718","chip multiprocessors;interconnects;multicores;on-chip network;quality-of-service;resource management;router;software interface","Bandwidth;Control systems;Hardware;Network-on-a-chip;Performance analysis;Quality of service;Spread spectrum communication;System-on-a-chip;Virtual colonoscopy;Yarn","network routing;network-on-chip","chip multiprocessors;fast barrier network;globally-synchronized frames;guaranteed quality-of-service;multihop on-chip networks;virtual channel router","","17","2","32","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"[Title page iii]","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","iii","iii","Conference proceedings title page.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.1","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556703","","","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Fetch-Criticality Reduction through Control Independence","Agarwal, M.; Navale, N.; Malik, K.; Frank, M.I.","Coordinated Sci. Lab., Univ. of Illinois at Urbana-Champaign, Urbana, IL","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","13","24","Architectures that exploit control independence (CI) promise to remove in-order fetch bottlenecks, like branch mispredicts, instruction-cache misses and fetch unit stalls, from the critical path of single-threaded execution. By exposing more fetch options, however, CI architectures also expose more performance tradeoffs. These tradeoffs make it hard to design policies that deliver good performance. This paper presents a criticality-based model for reasoning about CI architectures, and uses that model to describe the tradeoffs between gains from control independence versus increased costs of honoring data dependences. The model is then used to derive the design of a criticality-aware task selection policy that strikes the right balance between fetch-criticality and execute-criticality. Finally, the paper validates the model by attacking branch-misprediction induced fetch-criticality through the above derived spawn policy. This leads to as high as 100% improvements in performance, and in the region of 40% or more improvements for four of the benchmarks where this is the main problem. Criticality analysis shows that this improvement arises due to reduced fetch-criticality.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556712","Control Independence;Fetch-Criticality;Implicit Parallelization","Computer architecture;Costs;Degradation;Delay;Process control;Robustness","computer architecture","branch mispredicts;control independence;criticality-aware task selection policy;data dependences;fetch unit stalls;fetch-criticality reduction;instruction-cache misses;single-threaded execution","","2","","39","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Author index","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","465","466","The author index contains an entry for each author and coauthor included in the proceedings record.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556748","","","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Trading off Cache Capacity for Reliability to Enable Low Voltage Operation","Wilkerson, C.; Hongliang Gao; Alameldeen, A.R.; Chishti, Z.; Khellah, M.; Shih-Lien Lu","Microprocessor Technol. Lab., Intel Corp., Santa Clara, CA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","203","214","One of the most effective techniques to reduce a processor's power consumption is to reduce supply voltage. However, reducing voltage in the context of manufacturing-induced parameter variations can cause many types of memory circuits to fail. As a result, voltage scaling is limited by a minimum voltage, often called Vccmin, beyond which circuits may not operate reliably. Large memory structures (e.g., caches) typically set Vccmin for the whole processor. In this paper, we propose two architectural techniques that enable microprocessor caches (L1 and L2), to operate at low voltages despite very high memory cell failure rates. The Word-disable scheme combines two consecutive cache lines, to form a single cache line where only non-failing words are used. The Bit-fix scheme uses a quarter of the ways in a cache set to store positions and fix bits for failing bits in other ways of the set. During high voltage operation, both schemes allow use of the entire cache. During low voltage operation, they sacrifice cache capacity by 50% and 25%, respectively, to reduce Vccmin below 500mV. Compared to current designs with a Vccmin of 825 mV, our schemes enable a 40% voltage reduction, which reduces power by 85% and energy per instruction (EPI) by 53%.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556727","SRAM;Vccmin;cache;cache design;low power;low voltage;reliability;stability","Low voltage","cache storage;memory architecture;microprocessor chips;power aware computing;reliability","Vccmin minimum voltage;bit-fix scheme;cache capacity;low voltage operation;manufacturing-induced parameter variation;memory cell failure rate;memory circuit;microprocessor cache architectural technique;processor power consumption reduction;voltage scaling;word-disable scheme","","58","3","17","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Running a Quantum Circuit at the Speed of Data","Isailovic, N.; Whitney, M.; Patel, Y.; Kubiatowicz, J.","Comput. Sci. Div., Univ. of California, Berkeley, CA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","177","188","We analyze circuits for kernels from popular quantum computing applications, characterizing the hardware resources necessary to take ancilla preparation off the critical path. The result is a chip entirely dominated by ancilla generation circuits. To address this issue, we introduce optimized ancilla factories and analyze theirstructure and physical layout for ion trap technology. We introduce a new quantum computing architecture with highly concentrated data-only regions surrounded by shared ancilla factories. The results are a reduced dependence on costly teleportation, more efficient distribution of generated ancillae and more than five times speedup over previous proposals.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.5","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556725","ancilla factory;microarchitecture;quantum","Circuit analysis;Circuit analysis computing;Computer applications;Computer architecture;Data analysis;Hardware;Kernel;Production facilities;Quantum computing;Teleportation","particle traps;quantum computing","ancilla generation circuits;data-only regions;hardware resources;ion trap technology;quantum circuit;quantum computing applications;quantum computing architecture","","1","","24","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Polymorphic On-Chip Networks","Kim, M.M.; Davis, J.D.; Oskin, M.; Austin, T.","Comput. Sci. & Eng., Washington Univ., Seattle, WA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","101","112","As the number of cores per die increases, be they processors, memory blocks, or custom accelerators, the on-chip interconnect the cores use to communicate gains importance. We begin this study with an area-performance analysis of the interconnect design space. We find that there is no single network design that yields optimal performance across a range of traffic patterns. This indicates that there is an opportunity to gain performance by customizing the interconnect to a particular application or workload. We propose polymorphic on-chip networks to enable per-application network customization. This network can be configured prior to application runtime, to have the topology and buffering of arbitrary network designs. This paper proposes one such polymorphic network architecture. We demonstrate its modes of configurability, and evaluate the polymorphic network architecture design space, producing polymorphic fabrics that minimize the network area overhead. Finally, we expand the network on chip design space to include a polymorphic network design, showing that a single polymorphic network is capable of implementing all of the Pareto optimal fixed-network designs.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.25","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556719","configurable hardware;on-chip network","Bandwidth;Computer science;Hardware;Network topology;Network-on-a-chip;Pareto analysis;Runtime;Space exploration;Telecommunication traffic;Traffic control","integrated circuit design;integrated circuit interconnections;network topology;network-on-chip","Pareto optimal fixed-network designs;arbitrary network design buffering;arbitrary network design topology;custom accelerators;interconnect design space;memory blocks;per-application network customization;polymorphic fabrics;polymorphic on-chip networks;processors;traffic patterns","","15","","26","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"[Front cover]","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","","","Presents the front cover or splash screen of the proceedings.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.49","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556701","","","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Understanding and Designing New Server Architectures for Emerging Warehouse-Computing Environments","Lim, K.; Ranganathan, P.; Jichuan Chang; Patel, C.; Mudge, T.; Reinhardt, S.","Univ. of Michigan, Ann Arbor, MI","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","315","326","This paper seeks to understand and design next-generation servers for emerging ""warehouse-computing"" environments. We make two key contributions. First, we put together a detailed evaluation infrastructure including a new benchmark suite for warehouse-computing workloads, and detailed performance, cost, and power models, to quantitatively characterize bottlenecks. Second, we study a new solution that incorporates volume non-server-class components in novel packaging solutions, with memory sharing and flash-based disk caching. Our results show that this approach has promise, with a 2X improvement on average in performance-per-dollar for our benchmark suite.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.37","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556736","Server architecture;evaluation;warehouse-computing","Computer architecture;Costs;Data processing;Distributed computing;Packaging;Power system modeling;Quality of service;Reservoirs;Web and internet services;Web server","Internet;computer architecture;network servers;warehousing","flash-based disk caching;memory sharing;new server architectures;next-generation servers;packaging solutions;volume nonserver-class components;warehouse-computing environments","","27","","40","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"[Copyright notice]","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","iv","iv","Conference copyright notice.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.2","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556704","","","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"A Comprehensive Memory Modeling Tool and Its Application to the Design and Analysis of Future Memory Hierarchies","Thoziyoor, S.; Ahn, Jung Ho; Monchiero, M.; Brockman, J.B.; Jouppi, N.P.","Notre Dame, Univ., Notre Dame, IN","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","51","62","In this paper we introduce CACTI-D, a significant enhancement of CACTI 5.0. CACTI-D adds support for modeling of commodity DRAM technology and support for main memory DRAM chip organization. CACTI-D enables modeling of the complete memory hierarchy with consistent models all the way from SRAM based L1 caches through main memory DRAMs on DIMMs. We illustrate the potential applicability of CACTI-D in the design and analysis of future memory hierarchies by carrying out a last level cache study for a multicore multithreaded architecture at the 32nm technology node. In this study we use CACTI-D to model all components of the memory hierarchy including L1, L2, last level SRAM, logic process based DRAM or commodity DRAM L3 caches, and main memory DRAM chips. We carry out architectural simulation using benchmarks with large data sets and present results of their execution time, breakdown of power in the memory hierarchy, and system energy-delay product for the different system configurations. We find that commodity DRAM technology is most attractive for stacked last level caches, with significantly lower energy-delay products.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556715","CACTI;LLC;SRAM;cache;commodity DRAM;logic-process based DRAM","Application software;Bandwidth;Circuits;Computer architecture;Fabrication;Logic;Multicore processing;Propagation delay;Random access memory;Stacking","DRAM chips;SRAM chips;cache storage","CACTI-D;DIMM;DRAM technology;LI caches;SRAM;comprehensive memory modeling tool;memory hierarchies;memory hierarchy","","33","2","41","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Variation-Aware Application Scheduling and Power Management for Chip Multiprocessors","Teodorescu, R.; Torrellas, J.","Dept. of Comput. Sci., Illinois Univ., Champaign, IL","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","363","374","Within-die process variation causes individual cores in a chip multiprocessor (CMP) to differ substantially in both static power consumed and maximum frequency supported. In this environment, ignoring variation effects when scheduling applications or when managing power with dynamic voltage and frequency scaling (DVFS) is suboptimal. This paper proposes variation-aware algorithms for application scheduling and power management. One such power management algorithm, called LinOpt, uses linear programming to find the best voltage and frequency levels for each of the cores in the CMP - maximizing throughput at a given power budget. In a 20-core CMP, the combination of variation-aware application scheduling and LinOpt increases the average throughput by 12-17% and reduces the average ED<sup>2</sup> by 30-38% - all relative to using variation-aware scheduling together with a simple extension to Intel's Foxton power management algorithm.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.40","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556740","Process variation;application scheduling;power management","Application software;Dynamic voltage scaling;Energy management;Frequency;Job shop scheduling;Linear programming;Processor scheduling;Scheduling algorithm;Technology management;Throughput","linear programming;multiprocessing systems;power aware computing;processor scheduling","Intel Foxton power management algorithm;LinOpt;chip multiprocessor;dynamic frequency scaling;dynamic voltage scaling;linear programming;power management;variation-aware application scheduling","","59","5","39","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Online Estimation of Architectural Vulnerability Factor for Soft Errors","Xiaodong Li; Adve, S.V.; Bose, P.; Rivers, J.A.","Illinois Univ., Urbana, IL","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","341","352","As CMOS technology scales and more transistors are packed on to the same chip, soft error reliability has become an increasingly important design issue for processors. Prior research has shown that there is significant architecture-level masking, and many soft error solutions take advantage of this effect. Prior work has also shown that the degree of such masking can vary significantly across workloads and between individual workload phases, motivating dynamic adaptation of reliability solutions for optimal cost and benefit. For such adaptation, it is important to be able to accurately estimate the amount of masking or the architecture vulnerability factor (AVF) online, while the program is running. Unfortunately, existing solutions for estimating AVF are often based on offline simulators and hard to implement in real processors. This paper proposes a novel way of estimating AVF online, using simple modifications to the processor. The estimation method applies to both logic and storage structures on the processor. Compared to previous methods for estimating AVF, our method does not require any offline simulation or calibration for different workloads. We tested our method with a widely used simulator from industry, for four processor structures and for 100 to 200 intervals of each of eleven SPEC benchmarks. The results show that our method provides acceptably accurate AVF estimates at runtime. The absolute error rarely exceeds 0.08 across all application intervals for all structures, and the mean absolute error for a given application and structure combination is always within 0.05.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.9","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556738","AVF estimation;processor reliability;soft error","CMOS process;CMOS technology;Calibration;Computer architecture;Computer errors;Cost function;Logic;Material storage;Process design;Rivers","circuit reliability;logic design;logic testing;microprocessor chips","CMOS technology;architecture-level masking;microprocessor design;offline simulation;online architectural vulnerability factor estimation;soft error reliability;workload calibration","","15","3","20","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"A Two-Level Load/Store Queue Based on Execution Locality","Pericas, M.; Cristal, A.; Cazorla, F.J.; Gonzalez, R.; Veidenbaum, A.; Jimenez, D.A.; Valero, M.","Univ. Politec. de Catalunya, Barcelona","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","25","36","Multicore processors have emerged as a powerful platform on which to efficiently exploit thread-level parallelism (TLP). However, due to Amdahlpsilas law, such designs will be increasingly limited by the remaining sequential components of applications. To overcome this limitation it is necessary to design processors with many lower-performance cores for TLP and some high-performance cores designed to execute sequential algorithms. Such cores will need to address the memory-wall by implementing kilo-instruction windows. Large window processors require large load/store queues that would be too slow if implemented using current CAM-based designs. This paper proposes an epoch-based load store queue (ELSQ), a new design based on execution locality. It is integrated into a large-window processor that has a fast, out-of-order core operating only on L1/L2 cache hits and N slower cores that process L2 misses and their dependent instructions. The large LSQ is coupled with the slow cores and is partitioned into N small and local LSQs, one per core. We evaluate ELSQ in a large-window environment, finding that it enables high performance at low power. By exploiting locality among loads and stores, ELSQ outperforms even an idealized central LSQ when implemented on top of a decoupled processor design.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.10","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556713","Execution Locality;Kilo-Instruction Processors;Load/Store Queue;Power-Efficiency","Algorithm design and analysis;Bandwidth;Computer architecture;Energy efficiency;Failure analysis;Filtering;Multicore processing;Out of order;Process design;Proposals","cache storage;parallel processing","Amdahls law;CAM-based designs;L1 cache;L2 cache;epoch-based load store queue;execution locality;kilo-instruction windows;memory-wall;multicore processors;processor design;sequential algorithms;thread-level parallelism","","2","","25","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"3D-Stacked Memory Architectures for Multi-core Processors","Loh, G.H.","Coll. of Comput., Georgia Inst. of Technol., Athens, GA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","453","464","Three-dimensional integration enables stacking memory directly on top of a microprocessor, thereby significantly reducing wire delay between the two. Previous studies have examined the performance benefits of such an approach, but all of these works only consider commodity 2D DRAM organizations. In this work, we explore more aggressive 3D DRAM organizations that make better use of the additional die-to-die bandwidth provided by 3D stacking, as well as the additional transistor count. Our simulation results show that with a few simple changes to the 3D-DRAM organization, we can achieve a 1.75x speedup over previously proposed 3D-DRAM approaches on our memory-intensive multi-programmed workloads on a quad-core processor. The significant increase in memory system performance makes the L2 miss handling architecture (MHA) a new bottleneck, which we address by combining a novel data structure called the Vector Bloom Filter with dynamic MSHR capacity tuning. Our scalable L2 MHA yields an additional 17.8% performance improvement over our 3D-stacked memory architecture.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.15","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556747","3D integration;memory;multi-core","Bandwidth;Data structures;Delay;Memory architecture;Microprocessors;Multicore processing;Random access memory;Stacking;System performance;Wire","DRAM chips;memory architecture;microprocessor chips","3D-stacked memory architectures;DRAM;microprocessor;miss handling architecture;multicore processors;vector bloom filter;wire delay","","99","1","42","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"VEAL: Virtualized Execution Accelerator for Loops","Clark, N.; Hormati, A.; Mahlke, S.","Georgia Inst. of Technol., Athens, GA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","389","400","Performance improvement solely through transistor scaling is becoming more and more difficult, thus it is increasingly common to see domain specific accelerators used in conjunction with general purpose processors to achieve future performance goals. There is a serious drawback to accelerators, though: binary compatibility. An application compiled to utilize an accelerator cannot run on a processor without that accelerator, and applications that do not utilize an accelerator will never use it. To overcome this problem, we propose decoupling the instruction set architecture from the underlying accelerators. Computation to be accelerated is expressed using a processorpsilas baseline instruction set, and light-weight dynamic translation maps the representation to whatever accelerators are available in the system. In this paper, we describe the changes to a compilation framework and processor system needed to support this abstraction for an important set of accelerator designs that support innermost loops. In this analysis, we investigate the dynamic overheads associated with abstraction as well as the static/dynamic tradeoffs to improve the dynamic mapping of loop-nests. As part of the exploration, we also provide a quantitative analysis of the hardware characteristics of effective loop accelerators. We conclude that using a hybrid static-dynamic compilation approach to map computation on to loop-level accelerators is an practical way to increase computation efficiency, without the overheads associated with instruction set modification.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.33","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556742","","Acceleration;Algorithm design and analysis;Application software;Application specific integrated circuits;Computer aided instruction;Computer architecture;Costs;Energy consumption;Hardware;Multicore processing","program compilers;program control structures","dynamic overheads;general purpose processors;hybrid static-dynamic compilation approach;instruction set architecture;instruction set modification;light-weight dynamic translation maps;loop-level accelerators;performance improvement solely;virtualized execution accelerator","","12","","30","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"[Publisher's information]","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","468","468","Provides a listing of current committee members and society officers.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556749","","","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Atom-Aid: Detecting and Surviving Atomicity Violations","Lucia, B.; Devietti, J.; Strauss, K.; Ceze, L.","Comput. Sci. & Eng. Univ. of Washington, Seattle, WA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","277","288","Writing shared-memory parallel programs is error-prone. Among the concurrency errors that programmers often face are atomicity violations, which are especially challenging. They happen when programmers make incorrect assumptions about atomicity and fail to enclose memory accesses that should occur atomically inside the same critical section. If these accesses happen to be interleaved with conflicting accesses from different threads, the program might behave incorrectly. Recent architectural proposals arbitrarily group consecutive dynamic memory operations into atomic blocks to enforce memory ordering at a coarse grain. This provides what we call implicit atomicity, as the atomic blocks are not derived from explicit program annotations. In this paper, we make the fundamental observation that implicit atomicity probabilistically hides atomicity violations by reducing the number of interleaving opportunities between memory operations. We then propose Atom-Aid, which creates implicit atomic blocks intelligently instead of arbitrarily, dramatically reducing the probability that atomicity violations will manifest themselves. Atom-Aid is also able to report where atomicity violations might exist in the code, providing resilience and debuggability. We evaluate Atom-Aid using buggy code from applications including Apache, MySQL, and XMMS, showing that Atom-Aid virtually eliminates the manifestation of atomicity violations.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.4","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556733","bug;multiprocessors;parallel programming;software reliability","Computer architecture;Computer bugs;Computer errors;Concurrent computing;Counting circuits;Interleaved codes;Programming profession;Proposals;Writing;Yarn","parallel programming;shared memory systems;software architecture","Apache;MySQL;andXMMS;atomicity violations;coarse grain;concurrency errors;explicit program annotations;memory accesses;memory ordering;shared-memory parallel programs","","3","","31","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"ReVIVaL: A Variation-Tolerant Architecture Using Voltage Interpolation and Variable Latency","Xiaoyao Liang; Gu-Yeon Wei; Brooks, D.","Sch. of Eng. & Appl. Sci., Harvard Univ., Cambridge, MA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","191","202","Process variations are poised to significantly degrade performance benefits sought by moving to the next nanoscale technology node. Parameter fluctuations in devices can introduce large variations in peak operation among chips, among cores on a single chip, and among microarchitectural blocks within one core. Hence, it will be difficult to only rely on traditional frequency binning to efficiently cover the large variations that are expected. Furthermore, multiple voltage/frequency domains introduce significant hardware overhead and alone cannot address the full extent of delay variations expected in future multi-core systems. In this paper, we present ReVIVaL, which combines two fine-grained post-fabrication tuning techniques---voltage interpolation(VI) and variable latency(VL). We show that the frequency variation between chips, between cores on one chip, and between functional units within cores can be reduced to a very small range. The effectiveness of these techniques are further verified through experiments on test chips fabricated in a 130 nm CMOS process. Detailed architectural simulations of multi-core processors demonstrate significant performance and power advantages are possible by combining variable latency with voltage interpolation.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.27","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556726","Chip Multiprocessor;Microarchitecture;Process Variations","Degradation;Delay;Fluctuations;Frequency domain analysis;Hardware;Interpolation;Microarchitecture;Testing;Tuning;Voltage","CMOS digital integrated circuits;interpolation;logic design","CMOS process;ReVIVaL;delay variations;microarchitectural blocks;multicore systems;post-fabrication tuning techniques;process variations;variable latency;variation-tolerant architecture;voltage interpolation","","8","4","28","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Using Hardware Memory Protection to Build a High-Performance, Strongly-Atomic Hybrid Transactional Memory","Baugh, L.; Neelakantam, N.; Zilles, C.","Dept. of Comput. Sci., Illinois Univ., Urbana, IL","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","115","126","We demonstrate how fine-grained memory protection can be used in support of transactional memory systems: first showing how a software transactional memory system (STM) can be made strongly atomic by using memory protection on transactionally-held state, then showing how such a strongly-atomic STM can be used with a bounded hardware TM system to build a hybrid TM system in which zero-overhead hardware transactions may safely run concurrently with potentially-conflicting software transactions. We experimentally demonstrate how this hybrid TM organization avoids the common-case overheads associated with previous hybrid TM proposals, achieving performance rivaling an unbounded HTM system without the hardware complexity of ensuring completion of arbitrary transactions in hardware. As part of our findings, we identify key policies regarding contention management within and across the hardware and software TM components that are key to achieving robust performance with a hybrid TM.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556720","Abort Handler;Hybrid;Memory Protection;Primitives;Strong Atomicity;Transactional Memory","Computer architecture;Computer science;Content management;Hardware;Proposals;Protection;Read-write memory;Software performance;Software safety;Software systems","storage management;transaction processing","contention management;hardware memory protection;high-performance strongly-atomic hybrid transactional memory;software transactional memory system","","8","2","40","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"From Speculation to Security: Practical and Efficient Information Flow Tracking Using Speculative Hardware","Haibo Chen; Xi Wu; Liwei Yuan; Binyu Zang; Pen-Chung Yew; Chong, F.T.","Parallel Process. Inst., Fudan Univ., Shanghai","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","401","412","Dynamic information flow tracking (also known as taint tracking) is an appealing approach to combat various security attacks. However, the performance of applications can severely degrade without hardware support for tracking taints. This paper observes that information flow tracking can be efficiently emulated using deferred exception tracking in microprocessors supporting speculative execution. Based on this observation, we propose SHIFT, a low-overhead, software-based dynamic information flow tracking system to detect a wide range of attacks. The key idea is to treat tainted state (describing untrusted data) as speculative state (describing deferred exceptions). SHIFT leverages existing architectural support for speculative execution to track tainted state in registers and needs to instrument only load and store instructions to track tainted state in memory using a bitmap, which results in significant performance advantages. Moreover, by decoupling mechanisms for taint tracking from security policies, SHIFT can detect a wide range of exploits, including high-level semantic attacks. We have implemented SHIFT using the Itanium processor, which has support for deferred exceptions, and by modifying GCC to instrument loads and stores. A security assessment shows that SHIFT can detect both low-level memory corruption exploits as well as high-level semantic attacks with no false positives. Performance measurements show that SHIFT incurs about 1% overhead for server applications. The performance slowdown for SPEC-INT2000 is 2.81X and 2.27X for tracking at byte-level and wordlevel respectively. Minor architectural improvements to the Itanium processor (adding three simple instructions) can reduce the performance slowdown down to 2.32X and 1.8X for byte-level and word-level tracking, respectively.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556743","Deferred Exception;Dynamic Information Flow Tracking;Speculative Execution;Taint tracking","Application software;Computer science;Computer security;Hardware;Information security;Instruments;Measurement;Prototypes;Registers;Software prototyping","data flow analysis;exception handling;parallel architectures;parallel programming;security of data","decoupling mechanism;deferred exception tracking;load instruction;microprocessor;parallel architecture;security attack;software-based dynamic information flow tracking system;speculative hardware;store instruction;taint tracking","","6","2","27","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"list-reviewer","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","xv","xviii","The conference offers a note of thanks and lists its reviewers.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.47","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556709","","IEEE","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Technology-Driven, Highly-Scalable Dragonfly Topology","Kim, J.; Dally, W.J.; Scott, S.; Abts, D.","Northwestern Univ., Evanston, IL","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","77","88","Evolving technology and increasing pin-bandwidth motivate the use of high-radix routers to reduce the diameter, latency, and cost of interconnection networks. High-radix networks, however, require longer cables than their low-radix counterparts. Because cables dominate network cost, the number of cables, and particularly the number of long, global cables should be minimized to realize an efficient network. In this paper, we introduce the dragonfly topology which uses a group of high-radix routers as a virtual router to increase the effective radix of the network. With this organization, each minimally routed packet traverses at most one global channel. By reducing global channels, a dragonfly reduces cost by 20% compared to a flattened butterfly and by 52% compared to a folded Clos network in configurations with ges 16K nodes.We also introduce two new variants of global adaptive routing that enable load-balanced routing in the dragonfly. Each router in a dragonfly must make an adaptive routing decision based on the state of a global channel connected to a different router. Because of the indirect nature of this routing decision, conventional adaptive routing algorithms give degraded performance. We introduce the use of selective virtual-channel discrimination and the use of credit round-trip latency to both sense and signal channel congestion. The combination of these two methods gives throughput and latency that approaches that of an ideal adaptive routing algorithm.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.19","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556717","dragonfly;interconnection networks;topology","Bandwidth;Cables;Computer architecture;Costs;Delay;Multiprocessor interconnection networks;Network topology;Optical network units;Optical sensors;Routing","multiprocessor interconnection networks;network routing;network topology","adaptive routing decision;credit round-trip latency;flattened butterfly;folded Clos network;global adaptive routing;high-radix routers;highly-scalable dragonfly topology;interconnection networks;load-balanced routing;pin-bandwidth;selective virtual-channel discrimination;signal channel congestion;technology-driven dragonfly topology;virtual router","","49","1","33","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"[Title page i]","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","i","i","The following topics are dealt with: computer architecture; microarchitectures; memory systems; interconnect networks; transactional memory; emergent technology; parallel program debugging; system architecture and I/O; reliability; application acceleration; performance evaluation; multicore design; manycore design; system-on-chip.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556702","","","computer architecture;logic design;multiprocessor interconnection networks;system-on-chip","application acceleration;computer architecture;emergent technology;interconnect networks;manycore design;microarchitectures;multicore design;parallel program debugging;performance evaluation;reliability;system architecture;system-on-chip;transactional memory","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Learning and Leveraging the Relationship between Architecture-Level Measurements and Individual User Satisfaction","Shye, A.; Ozisikyilmaz, B.; Mallik, Arindam; Memik, G.; Dinda, P.A.; Dick, R.P.; Choudhary, A.N.","Dept. Electr. Eng. & Comput. Sci., Northwestern Univ., Evanston, IL","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","427","438","The ultimate goal of computer design is to satisfy the end-user. In particular computing domains, such as interactive applications, there exists a variation in user expectations and user satisfaction relative to the performance of existing computer systems. In this work, we leverage this variation to develop more efficient architectures that are customized to end-users. We first investigate the relationship between microarchitectural parameters and user satisfaction. Specifically, we analyze the relationship between hardware performance counter (HPC) readings and individual satisfaction levels reported by users for representative applications. Our results show that the satisfaction of the user is strongly correlated to the performance of the underlying hardware. More importantly, the results show that user satisfaction is highly user-dependent. To take advantage of these observations, we develop a framework called Individualized Dynamic Voltage and Frequency Scaling (iDVFS). We study a group of users to characterize the relationship between the HPCs and individual user satisfaction levels. Based on this analysis, we use artificial neural networks to model the function from HPCs to user satisfaction for individual users. This model is then used online to predict user satisfaction and set the frequency level accordingly. A second set of user studies demonstrates that iDVFS reduces the CPU power consumption by over 25% in representative applications as compared to the Windows XP DVFS algorithm.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.29","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556745","Dynamic Power Management;Hardware Performance Counters;Learning User Satisfaction;User-aware Architectures","Application software;Artificial neural networks;Computer architecture;Counting circuits;Dynamic voltage scaling;Frequency;Hardware;Microarchitecture;Performance analysis;Predictive models","computer architecture;neural nets;performance evaluation;power aware computing","architecture-level measurements;artificial neural networks;computer design;hardware performance counter;individual user satisfaction;individualized dynamic voltage and frequency scaling;microarchitectural parameters","","9","4","28","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Table of contents","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","v","ix","Presents the table of contents of the proceedings.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.3","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556705","","","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Parallelism-Aware Batch Scheduling: Enhancing both Performance and Fairness of Shared DRAM Systems","Mutlu, O.; Moscibroda, T.","Microsoft Res., Redmond, WA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","63","74","In a chip-multiprocessor (CMP) system, the DRAM system is shared among cores. In a shared DRAM system, requests from a thread can not only delay requests from other threads by causing bank/bus/row-buffer conflicts but they can also destroy other threadspsilaDRAM-bank-level parallelism. Requests whose latencies would otherwise have been overlapped could effectively become serialized. As are sult both fairness and system throughput degrade, and some thread scan starve for long time periods. This paper proposes a fundamentally new approach to designing a shared DRAM controller that provides quality of service to threads,while also improving system throughput. Our parallelism-aware batch scheduler (PAR-BS) design is based on two key ideas. First, PARBS processes DRAM requests in batches to provide fairness and to avoid starvation of requests. Second, to optimize system throughput,PAR-BS employs a parallelism-aware DRAM scheduling policy that aims to process requests from a thread in parallel in the DRAM banks, thereby reducing the memory-related stall-time experienced by the thread. PAR-BS seamlessly incorporates support for system-level thread priorities and can provide different service levels, including purely opportunistic service, to threads with different priorities.We evaluate the design trade-offs involved in PAR-BS and compare it to four previously proposed DRAM scheduler designs on 4-, 8-, and16-core systems. Our evaluations show that, averaged over 100 4-core workloads, PAR-BS improves fairness by 1.11X and system through put by 8.3% compared to the best previous scheduling technique, Stall-Time Fair Memory (STFM) scheduling. Based on simple request prioritization rules, PAR-BS is also simpler to implement than STFM.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.7","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556716","DRAM systems;Memory systems;chip multiprocessors;fairness;memory scheduling;memory-level parallelism;multi-core systems;quality of service","Computer architecture;Control systems;Degradation;Delay;Interference;Random access memory;Scheduling algorithm;System software;Throughput;Yarn","DRAM chips;microcontrollers;processor scheduling;shared memory systems","chip-multiprocessor system;memory-related stall-time fair memory scheduling;parallelism-aware batch scheduling;shared DRAM controller;shared DRAM system","","52","8","44","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Self-Optimizing Memory Controllers: A Reinforcement Learning Approach","Ipek, E.; Mutlu, O.; Martinez, J.F.; Caruana, R.","Cornell Univ., Ithaca, NY","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","39","50","Efficiently utilizing off-chip DRAM bandwidth is a critical issue in designing cost-effective, high-performance chip multiprocessors (CMPs). Conventional memory controllers deliver relatively low performance in part because they often employ fixed, rigid access scheduling policies designed for average-case application behavior. As a result, they cannot learn and optimize the long-term performance impact of their scheduling decisions,and cannot adapt their scheduling policies to dynamic workload behavior.We propose a new, self-optimizing memory controller design that operates using the principles of reinforcement learning (RL)to overcome these limitations. Our RL-based memory controller observes the system state and estimates the long-term performance impact of each action it can take. In this way, the controller learns to optimize its scheduling policy on the fly to maximize long-term performance. Our results show that an RL-based memory controller improves the performance of a set of parallel applications run on a 4-core CMP by 19% on average (upto 33%), and it improves DRAM bandwidth utilization by 22%compared to a state-of-the-art controller.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556714","Chip Multiprocessors;Machine Learning;Memory Controller;Memory Systems;Reinforcement Learning","Automatic control;Bandwidth;Control systems;Delay;Dynamic scheduling;Job shop scheduling;Machine learning;Moore's Law;Random access memory;State estimation","DRAM chips;integrated circuit design;learning (artificial intelligence);microcontrollers","access scheduling policies;high-performance chip multiprocessors;off-chip DRAM bandwidth;reinforcement learning;self-optimizing memory controllers","","30","7","49","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Improving NAND Flash Based Disk Caches","Taeho Kgil; Roberts, D.; Mudge, T.","Adv. Comput. Archit. Lab., Univ. of Michigan, Ann Arbor, MI","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","327","338","Flash is a widely used storage device that provides high density and low power, appealing properties for general purpose computing. Today, its usual application is in portable special purpose devices such as MP3 players. In this paper we examine its use in the server domain - a more general purpose environment. Aggressive process scaling and the use of multi-level cells continues to improve density ahead of Moorepsilas Law predictions, making flash even more attractive as a general purpose memory solution. Unfortunately, reliability limits the use of flash. To seriously consider flash in the server domain, architectural support must exist to address this concern. This paper first shows how flash can be used in todaypsilas server platforms as a disk cache. It then proposes two improvements. The first improves performance and reliability by splitting flash based disk caches into separate read and write regions. The second improves reliability by employing a programmable flash memory controller. It can change the error code strength (number of correctable bits) and the number of bits that a memory cell can store (cell density) according to the demands of the application. Our studies show that flash reduces overall power consumed by the system memory and hard disk drive up to 3 times while maintaining performance. We also show that flash lifetime can be improved by a factor of 20 when using a programmable flash memory controller, if some performance degradation (below 5%) is acceptable.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.32","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556737","Flash;Flash memory controller;NAND Flash;data center;disk cache","Computer architecture;Digital audio players;Disk drives;Error correction codes;Flash memory;Handheld computers;Hardware;Power system reliability;Space exploration;Split gate flash memory cells","NAND circuits;flash memories","MP3 players;Moorepsilas Law predictions;NAND flash disk caches;aggressive process scaling;general purpose computing;multi-level cells;programmable flash memory controller;server domain;storage device","","45","5","26","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Rerun: Exploiting Episodes for Lightweight Memory Race Recording","Hower, D.R.; Hill, M.D.","Comput. Sci. Dept., Univ. of Wisconsin-Madison, Madison, WI","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","265","276","Multiprocessor deterministic replay has many potential uses in the era of multicore computing, including enhanced debugging, fault tolerance, and intrusion detection. While sources of nondeterminism in a uniprocessor can be recorded efficiently in software, it seems likely that hardware support will be needed in a multiprocessor environment where the outcome of memory races must also be recorded. We develop a memory race recording mechanism, called Rerun, that uses small hardware state (~166 bytes/core), writes a small race log (~4 bytes/kilo- instruction), and operates well as the number of cores per system scales (e.g., to 16 cores). Rerun exploits the dual of conventional wisdom in race recording: Rather than record information about individual memory accesses that conflict, we record how long a thread executes without conflicting with other threads. In particular, Rerun passively creates atomic episodes. Each episode is a dynamic instruction sequence that a thread happens to execute without interacting with other threads. Rerun uses Lamport Clocks to order episodes and enable replay of an equivalent execution.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556732","Determinism;Multicore;Race Recording","Application software;Clocks;Computer architecture;Debugging;Fault tolerance;Hardware;Intrusion detection;Multicore processing;Virtual machine monitors;Yarn","data recording;deterministic algorithms;multiprocessing systems;storage management","Rerun;lightweight memory race recording;memory accesses;memory race recording;multicore computing;multiprocessor deterministic replay","","15","3","42","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Atomic Vector Operations on Chip Multiprocessors","Kumar, S.; Daehyun Kim; Smelyanskiy, M.; Yen-Kuang Chen; Chhugani, J.; Hughes, C.J.; Changkyu Kim; Lee, V.W.; Nguyen, A.D.","Microprocessor Technol. Labs., Intel Corp., Santa Clara, CA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","441","452","The current trend is for processors to deliver dramatic improvements in parallel performance while only modestly improving serial performance. Parallel performance is harvested through vector/SIMD instructions as well as multithreading (through both multithreaded cores and chip multiprocessors). Vector parallelism can be more efficiently supported than multithreading, but is often harder for software to exploit. In particular, code with sparse data access patterns cannot easily utilize the vector/SIMD instructions of mainstream processors. Hardware to scatter and gather sparse data has previously been proposed to enable vector execution for these codes. However, on multithreaded architectures, a number of applications spend significant time on atomic operations (e.g., parallel reductions), which cannot be vectorized using previously proposed schemes. This paper proposes architectural support for atomic vector operations (referred to as GLSC) that addresses this limitation. GLSC extends scatter-gather hardware to support atomic memory operations. Our experiments show that the GLSC provides an average performance improvement on a set of important RMS kernels of 54% for 4-wide SIMD.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.38","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556746","SIMD;locks;multiprocessors;reductions;vector","Application software;Computer architecture;Hardware;Microprocessors;Multithreading;Parallel processing;Read-write memory;Scattering;Software performance;Yarn","multi-threading;vector processor systems","GLSC;SIMD instructions;atomic vector operations;chip multiprocessors;multithreaded architectures;parallel performance;vector parallelism","","5","3","27","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Flexible Hardware Acceleration for Instruction-Grain Program Monitoring","Chen, S.; Kozuch, M.; Strigkos, T.; Falsafi, B.; Gibbons, P.B.; Mowry, T.C.; Ramachandran, V.; Ruwase, O.; Ryan, M.; Vlachos, E.","Intel Res. Pittsburgh, Pittsburgh, PA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","377","388","Instruction-grain program monitoring tools, which check and analyze executing programs at the granularity of individual instructions, are invaluable for quickly detecting bugs and security attacks and then limiting their damage (via containment and/or recovery). Unfortunately, their fine-grain nature implies very high monitoring overheads for software-only tools, which are typically based on dynamic binary instrumentation. Previous hardware proposals either focus on mechanisms that target specific bugs or address only the cost of binary instrumentation. In this paper, we propose a flexible hardware solution for accelerating a wide range of instruction-grain monitoring tools. By examining a number of diverse tools (for memory checking, security tracking, and data race detection), we identify three significant common sources of overheads and then propose three novel hardware techniques for addressing these overheads: Inheritance Tracking, Idempotent Filters, and Metadata-TLBs. Together, these constitute a general-purpose hardware acceleration framework. Experimental results show our framework reduces overheads by 2-3X over the previous state-of-the-art, while supporting the needed flexibility.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556741","Hardware Acceleration;Idempotent Filter;Inheritance Tracking;Instruction-grain Program Monitoring;LBA;Lifeguards;Log-Based Architectures;Metadata-TLB","Acceleration;Computer bugs;Costs;Data security;Filters;Hardware;Instruments;Monitoring;Proposals;Target tracking","program debugging;program diagnostics;security of data","dynamic binary instrumentation;flexible hardware acceleration;flexible hardware solution;instruction-grain program monitoring;security attacks;software-only tools","","9","","42","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"A Proactive Wearout Recovery Approach for Exploiting Microarchitectural Redundancy to Extend Cache SRAM Lifetime","Shin, J.; Zyuban, V.; Bose, P.; Pinkston, T.M.","Univ. of Southern California, Los Angeles, CA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","353","362","Microarchitectural redundancy has been proposed as a means of improving chip lifetime reliability. It is typically used in a reactive way, allowing chips to maintain operability in the presence of failures by detecting and isolating, correcting, and/or replacing components on a first-come, first-served basis only after they become faulty. In this paper, we explore an alternative, more preferred method of exploiting microarchitectural redundancy to enhance chip lifetime reliability. In our proposed approach, redundancy is used proactively to allow non-faulty microarchitecture components to be temporarily deactivated, on a rotating basis, to suspend and/or recover from certain wearout effects. This approach improves chip lifetime reliability by warding off the onset of wearout failures as opposed to reacting to them posteriorly. Applied to on-chip cache SRAM for combating NBTI-induced wearout failure, our proactive wearout recovery approach increases lifetime reliability (measured in mean-time-to-failure) of the cache by about a factor of seven relative to no use of microarchitectural redundancy and a factor of five relative to conventional reactive use of redundancy having similar area overhead.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.30","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556739","lifetime reliability;microarchitectural redundancy;proactive approach;wearout recovery","Degradation;Failure analysis;Fault detection;Maintenance;Microarchitecture;Niobium compounds;Random access memory;Redundancy;Stress;Titanium compounds","SRAM chips;cache storage;failure analysis;integrated circuit reliability;redundancy","NBTI-induced wearout failure;cache SRAM lifetime;chip lifetime reliability;first-come first-served basis;microarchitectural redundancy;proactive wearout recovery;wearout failures","","21","7","28","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Officers","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","xii","xiv","Provides a listing of current committee members and society officers.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556708","","","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"TokenTM: Efficient Execution of Large Transactions with Hardware Transactional Memory","Bobba, J.; Goyal, N.; Hill, M.D.; Swift, M.M.; Wood, D.A.","Dept. of Comput. Sci, Univ. of Wisconsin-Madison, Madison, WI","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","127","138","Current hardware transactional memory systems seek to simplify parallel programming, but assume that large transactions are rare, so it is acceptable to penalize their performance or concurrency. However, future programmers may wish to use large transactions more often in order to integrate with higher-level programming models (e.g., database transactions) or perform selected I/O operations. To prevent the ""small transactions are common"" assumption from becoming self-fulfilling, this paper contributes TokenTM - an unbounded HTM that uses the abstraction of tokens to precisely track conflicts on an unbounded number of memory blocks. TokenTM implements tokens with new mechanisms, including metastate fission/fusion and fast token release. TokenTM executes small transactions fast, executes concurrent large transactions with no penalty to nonconflicting transactions, and gracefully handles paging, context switching, and System-V-style shared memory.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556721","coherence protocols;hardware transactional memory;metastates;tokens;transactional memory;unbounded transactions","Concurrent computing;Degradation;Feedback;Hardware;Metastasis;Parallel programming;Programming profession;Protocols;Transaction databases;Yarn","paged storage;parallel programming;shared memory systems;transaction processing","I/O operations;System-V-style shared memory;TokenTM unbounded HTM;concurrent large transaction execution;context switching;fast token release;hardware transactional memory systems;metastate fission;metastate fusion;page handling;parallel programming","","11","1","30","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Message from the Program Chair","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","xi","xi","Presents the introductory welcome message from the conference proceedings.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.45","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556707","","","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Microcoded Architectures for Ion-Tap Quantum Computers","Kreger-Stickles, L.; Oskin, M.","Dept. of Comput. Sci. & Eng., Univ. of Washington, Seattle, WA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","165","176","In this paper we present the first ever systematic design space exploration of microcoded software fault tolerant ion-trap quantum computers. This exploration reveals the critical importance of a well-tuned microcode for providing high performance and ensuring system reliability. In addition, we find that, despite recent advances in the reliability of quantum memory, the impact of errors due to stored quantum data is now, and will continue to be, a major source of systemic error. Finally, our exploration reveals a single design which out performs all others we considered in run time, fidelity and area. For completeness our design space exploration includes designs from prior work [13] and we find a novel design that is 1/2 the size, 3 times as fast, and an order of magnitude more reliable.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.28","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556724","Architecture;Ion-Trap;Microcoded;Quantum","Computer architecture;Computer errors;Error correction;Fault tolerance;Large-scale systems;Microarchitecture;Quantum computing;Space exploration;Space technology;Timing","microprogramming;particle traps;quantum computing;software fault tolerance","ion-trap quantum computers;microcoded architectures;software fault tolerant;space exploration;system reliability","","0","","22","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"MIRA: A Multi-layered On-Chip Interconnect Router Architecture","Dongkook Park; Eachempati, S.; Das, R.; Mishra, A.K.; Yuan Xie; Vijaykrishnan, N.; Das, C.R.","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","251","261","Recently, Network-on-Chip (NoC) architectures have gained popularity to address the interconnect delay problem for designing CMP / multi-core/SoC systems in deep sub-micron technology. However, almost all prior studies have focused on 2D NoC designs. Since three dimensional (3D) integration has emerged to mitigate the interconnect delay problem, exploring the NoC design space in 3D can provide ample opportunities to design high performance and energy-efficient NoC architectures. In this paper, we propose a 3D stacked NoC router architecture, called MIRA, which unlike the 3D routers in previous works, is stacked into multiple layers and optimized to reduce the overall area requirements and power consumption. We discuss the design details of a four-layer 3D NoC and its enhanced version with additional express channels, and compare them against a (6times6) 2D design and a baseline 3D design. All the designs are evaluated using a cycle-accurate 3D NoC simulator, and integrated with the Orion power model for performance and power analysis. The simulation results with synthetic and application traces demonstrate that the proposed multi-layered NoC routers can outperform the 2D and naive 3D designs in terms of performance and power. It can achieve up to 42% reduction in power consumption and up to 51% improvement in average latency with synthetic workloads. With real workloads, these benefits are around 67% and 38%, respectively.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.13","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556731","3D;Network-on-Chip;NoC;express channel;express path;on-chip interconnect;router architecture","Computer architecture;Delay;Energy consumption;Energy efficiency;Network-on-a-chip;Performance analysis;Space technology;Stacking;System-on-a-chip;Temperature","computer architecture;integrated circuit interconnections;logic design;network routing;network-on-chip","2D NoC design;CMP;MIRA-3D stacked NoC router architecture;chip multiprocessor design;interconnect delay problem;multicore/SoC system;multilayered on-chip interconnect router architecture;network-on-chip architecture","","42","3","53","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Intra-disk Parallelism: An Idea Whose Time Has Come","Sankar, S.; Gurumurthi, S.; Stan, M.R.","Dept. of Comput. Sci., Univ. of Virginia, Charlottesville, VA","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","303","314","Server storage systems use a large number of disks to achieve high performance, thereby consuming a significant amount of power. In this paper, we propose to significantly reduce the power consumed by such storage systems via intra-disk parallelism, wherein disk drives can exploit parallelism in the I/O request stream. Intra-disk parallelism can facilitate replacing a large disk array with a smaller one, using the minimum number of disk drives needed to satisfy the capacity requirements. We show that the design space of intra-disk parallelism is large and present a taxonomy to formulate specific implementations within this space. Using a set of commercial workloads, we perform a limit study to identify the key performance bottlenecks that arise when we replace a storage array that is tuned to provide high performance with a single high-capacity disk drive. We show that it is possible to match, and even surpass, the performance of a storage array for these workloads by using a single disk drive of sufficient capacity that exploits intra-disk parallelism, while significantly reducing the power consumed by the storage system. We evaluate the performance and power consumption of disk arrays composed of intra-disk parallel drives, and discuss engineering and cost issues related to the implementation and deployment of such disk drives.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556735","I/O;disk;parallelism;power;storage","Computer architecture;Computer science;Costs;Disk drives;Energy consumption;Energy management;Parallel processing;Power engineering and energy;Power system management;Taxonomy","parallel memories","I-O request stream;disk drives;high-capacity disk drive;intradisk parallelism;server storage systems","","5","","45","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"DeLorean: Recording and Deterministically Replaying Shared-Memory Multiprocessor Execution Ef?ciently","Montesinos, P.; Ceze, L.; Torrellas, J.","Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","289","300","Support for deterministic replay of multithreaded execution can greatly help in finding concurrency bugs. For highest effectiveness, replay schemes should (i) record at production-run speed, (ii) keep their logging requirements minute, and (iii) replay at a speed similar to that of the initial execution. In this paper, we propose a new substrate for deterministic replay that provides substantial advances along these axes. In our proposal, processors execute blocks of instructions atomically, as in transactional memory or speculative multithreading, and the system only needs to record the commit order of these blocks. We call our scheme DeLorean. Our results show that DeLorean records execution at a speed similar to that of release consistency (RC) execution and replays at about 82% of its speed. In contrast, most current schemes only record at the speed of Sequential Consistency (SC) execution. Moreover, DeLorean only needs 7.5% of the log size needed by a state-of-the-art scheme. Finally, DeLorean can be configured to need only 0.6% of the log size of the state-of-the-art scheme at the cost of recording at 86% of RCpsilas execution speed - still faster than SC. In this configuration, the log of an 8-processor 5-GHz machine is estimated to be only about 20GB per day.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.36","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556734","","Computer architecture;Computer bugs;Computer science;Concurrent computing;Costs;Debugging;Hardware;Multithreading;Proposals;Timing","multi-threading;program debugging;shared memory systems","DeLorean records execution;concurrency bugs;logging requirements;multithreaded execution;production-run speed;release consistency execution;sequential consistency execution;shared-memory multiprocessor execution efficiently","","28","2","16","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Message from the General Chairs","","","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","x","x","Presents the introductory welcome message from the conference proceedings.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.44","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556706","","","","","","0","","","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Corona: System Implications of Emerging Nanophotonic Technology","Vantrease, D.; Schreiber, R.; Monchiero, M.; McLaren, M.; Jouppi, N.P.; Fiorentino, M.; Davis, A.; Binkert, N.; Beausoleil, R.G.; Ahn, J.H.","Univ. of Wisconsin - Madison, Madison, WI","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","153","164","We expect that many-core microprocessors will push performance per chip from the 10 gigaflop to the 10 teraflop range in the coming decade. To support this increased performance, memory and inter-core bandwidths will also have to scale by orders of magnitude. Pin limitations, the energy cost of electrical signaling, and the non-scalability of chip-length global wires are significant bandwidth impediments. Recent developments in silicon nanophotonic technology have the potential to meet these off- and on-stack bandwidth requirements at acceptable power levels. Corona is a 3 D many-core architecture that uses nanophotonic communication for both inter-core communication and off-stack communication to memory or I/O devices. Its peak floating-point performance is 10 teraflops. Dense wavelength division multiplexed optically connected memory modules provide 10 terabyte per second memory bandwidth. A photonic crossbar fully interconnects its 256 low-power multithreaded cores at 20 terabyte per second bandwidth. We have simulated a 1024 thread Corona system running synthetic benchmarks and scaled versions of the SPLASH-2 benchmark suite. We believe that in comparison with an electrically-connected many-core alternative that uses the same on-stack interconnect power, Corona can provide 2 to 6 times more performance on many memory intensive workloads, while simultaneously reducing power.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556723","3D stacking;Many-core CMP;Nanophotonics;On-chip Networks","Bandwidth;Corona;Costs;Impedance;Microprocessors;Nanoscale devices;Power system interconnection;Silicon;Wavelength division multiplexing;Wires","multiprocessing systems;nanotechnology;parallel architectures","3D many-core architecture;Corona;electrical signaling;energy cost;intercore bandwidths;intercore communication;low-power multithreaded cores;many-core microprocessors;nanophotonic communication;pin limitation;silicon nanophotonic technology","","102","8","35","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"iDEAL: Inter-router Dual-Function Energy and Area-Efficient Links for Network-on-Chip (NoC) Architectures","Kodi, A.K.; Sarathy, A.; Louri, A.","Dept. of Electr. Eng. & Comput. Sci., Ohio Univ., Athens, OH","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","241","250","Network-on-Chip (NoC) architectures have been adopted by a growing number of multi-core designs as a flexible and scalable solution to the increasing wire delay constraints in the deep sub-micron regime. However, the shrinking feature size limits the performance of NoCs due to power and area constraints. Research into the optimization of NoCs has shown that a reduction in the number of buffers in the NoC routers reduces the power and area overhead but degrades the network performance. In this paper, we propose iDEAL, a low-power area-efficient NoC architecture by reducing the number of buffers within the router. To overcome the performance degradation caused by the reduced buffer size, we propose to use adaptive dual-function links capable of data transmission as well as data storage when required. Simulation results for the proposed architecture show that reducing the router buffer size in half and using the adaptive dual-function links achieves nearly 40% savings in buffer power, 30% savings in overall network power and about 41% savings in the router area, with only a marginal 1-3% drop in performance. Moreover, the performance in iDEAL can be further improved by aggressive and speculative flow control techniques.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556730","Interconnects;Low-Power architecture;Network-on-Chip","Computer architecture;Computer networks;Computer science;Degradation;Design optimization;Energy consumption;Network-on-a-chip;Power engineering and energy;Very large scale integration;Wire","buffer storage;computer architecture;low-power electronics;multiprocessor interconnection networks;network-on-chip","adaptive inter-router dual-function energy link;area-efficient link;data storage;data transmission;deep sub micron regime;low-power area-efficient NoC architecture;multi core design;network-on-chip architecture;optimization;performance degradation","","10","","34","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Flexible Decoupled Transactional Memory Support","Shriraman, A.; Dwarkadas, S.; Scott, M.L.","Dept. of Comput. Sci., Rochester, Univ., Rochester, NY","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","139","150","A high-concurrency transactional memory (TM) implementation needs to track concurrent accesses, buffer speculative updates, and manage conflicts. We present a system, FlexTM (FLEXible Transactional Memory), that coordinates four decoupled hardware mechanisms: read and write signatures, which summarize per-thread access sets; per-thread conflict summary tables (CSTs), which identify the threads with which conflicts have occurred; Programmable Data Isolation, which maintains speculative updates in the local cache and employs a thread-private buffer (in virtual memory) in the rare event of overflow; and Alert-On-Update, which selectively notifies threads about coherence events. All mechanisms are software- accessible, to enable visualization and to support transactions of arbitrary length. FlexTM allows software to determine when to manage conflicts (either eagerly or lazily), and to employ a variety of conflict management and commit protocols. We describe an STM-inspired protocol that uses CSTs to manage conflicts in a distributed manner (no global arbitration) and allows parallel commits. In experiments with a prototype on Simics/GEMS, FlexTM exhibits ~5times speedup over high-quality software TM, with no loss in policy flexibility. Its distributed commit protocol is also more efficient than a central hardware manager. Our results highlight the importance of flexibility in determining when to manage conflicts: lazy maximizes concurrency and helps to ensure forward progress while eager provides better overall utilization in a multi-programmed system.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.17","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556722","Cache coherence;Conflict detection;FlexTM;Hardware;Multiprocessors;RTM;Transactional memory","Computer architecture;Computer science;Concurrent computing;Delay systems;Hardware;Memory management;Protocols;Read-write memory;Software prototyping;Yarn","concurrency control;storage management;transaction processing","FlexTM;buffer speculative updates;flexible decoupled transactional memory support;high-concurrency transactional memory;multi-programmed system;per-thread conflict summary tables;programmable data isolation","","22","1","37","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Software-Controlled Priority Characterization of POWER5 Processor","Boneti, C.; Cazorla, F.J.; Gioiosa, R.; Buyuktosunoglu, A.; Chen-Yong Cher; Valero, M.","Univ. Politec. de Catalunya, Catalunya","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","415","426","Due to the limitations of instruction-level parallelism, thread-level parallelism has become a popular way to improve processor performance. One example is the IBM POWER5TM processor, a two-context simultaneous-multithreaded dual-core chip. In each SMT core, the IBM POWER5 features two levels of thread resource balancing and prioritization. The first level provides automatic in-hardware resource balancing, while the second level is a software-controlled priority mechanism that presents eight levels of thread priorities. Currently, software-controlled prioritization is only used in limited number of cases in the software platforms due to lack of performance characterization of the effects of this mechanism. In this work, we characterize the effects of the software-based prioritization on several different workloads. We show that the impact of the prioritization significantly depends on the workloads coscheduled on a core. By prioritizing the right task, it is possible to obtain more than two times of throughput improvement for synthetic workloads compared to the baseline. We also present two application case studies targeting two different performance metrics: the first case study improves overall throughput by 23.7% and the second case study reduces the total execution time by 9.3%. In addition, we show the circumstances when a background thread can be run transparently without affecting the performance of the foreground thread.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.8","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556744","IBM POWER5;SMT;performance characterization;simultaneous multithreading;software-controlled prioritization","Application software;Computer architecture;Hardware;Linux;Parallel processing;Performance loss;Software performance;Surface-mount technology;Throughput;Yarn","instruction sets;microprocessor chips;multi-threading;resource allocation;software performance evaluation","IBM POWER5 processor;instruction-level parallelism;performance metrics;software-controlled priority characterization;thread resource balancing;thread-level parallelism;two-context simultaneous-multithreaded dual-core chip","","7","1","25","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Virtual Circuit Tree Multicasting: A Case for On-Chip Hardware Multicast Support","Jerger, N.E.; Li-Shiuan Peh; Lipasti, M.","Electr. & Comput. Eng. Dept., Univ. of Wisconsin-Madison, Madison, WI","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","229","240","Current state-of-the-art on-chip networks provide efficiency, high throughput, and low latency for one-to-one (unicast) traffic. The presence of one-to-many (multicast) or one-to-all (broadcast) traffic can significantly degrade the performance of these designs, since they rely on multiple unicasts to provide one-to-many communication. This results in a burst of packets from a single source and is a very inefficient way of performing multicast and broadcast communication. This inefficiency is compounded by the proliferation of architectures and coherence protocols that require multicast and broadcast communication. In this paper, we characterize a wide array of on-chip communication scenarios that benefit from hardware multicast support. We propose Virtual Circuit Tree Multicasting (VCTM) and present a detailed multicast router design that improves network performance by up to 90% while reducing network activity (hence power) by up to 53%.Our VCTM router is flexible enough to improve interconnect performance for a broad spectrum of multicasting scenarios,and achieves these benefits with straightforward and inexpensive extensions to a state-of-the-art packet-switched router.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556729","Cache Coherence Protocol;Interconnection Network;Multiprocessor","Broadcasting;Circuits;Degradation;Delay;Hardware;Multicast protocols;Network-on-a-chip;Telecommunication traffic;Throughput;Unicast","multicast communication;telecommunication network routing","broadcast communication;multicast communication;multicast router;on-chip communication;on-chip hardware multicast support;packet-switched router;virtual circuit tree multicasting","","35","","44","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
"Achieving Out-of-Order Performance with Almost In-Order Complexity","Tseng, F.; Patt, Y.N.","Dept. of Electr. & Comput. Eng., Univ. of Texas, Austin, TX","Computer Architecture, 2008. ISCA '08. 35th International Symposium on","20080715","2008","","","3","12","There is still much performance to be gained by out-of-order processors with wider issue widths. However, traditional methods of increasing issue width do not scale; that is, they drastically increase design complexity and power requirements. This paper introduces the braid, a compile-time identified entity that enables the execution core to scale to wider widths by exploiting the small fanout and short lifetime of values produced by the program. Braid processing requires identification by the compiler, minor extensions to the ISA, and support by the microarchitecture. The result from processing braids is performance within 9% of a very aggressive conventional out-of-order microarchitecture with almost the complexity of an in-order implementation.","1063-6897","978-0-7695-3174-8","","10.1109/ISCA.2008.23","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556711","","Bars;Computer architecture;Hardware;Instruction sets;Microarchitecture;Out of order;Performance loss;Pipeline processing;Process design;Processor scheduling","computer architecture;program compilers","ISA;almost in-order complexity;braid processing;compile-time identified entity;design complexity;out-of-order performance;power requirements","","7","","29","","","21-25 June 2008","","IEEE","IEEE Conference Publications"
