"http://ieeexplore.ieee.org/search/searchresult.jsp?bulkSetSize=2000&queryText%3DComputer+Architecture%2C+2005.+ISCA+%2705.++32nd+International+Symposium+on",2015/06/23 15:49:50
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN","ISBN","EISBN","DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Computing architectural vulnerability factors for address-based structures","Biswas, A.; Racunas, P.; Cheveresan, R.; Emer, J.; Mukherjee, S.S.; Rangan, R.","FACT Group, Intel Corp., Hudson, MA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","532","543","Processor designers require estimates of the architectural vulnerability factor (AVF) of on-chip structures to make accurate soft error rate estimates. AVF is the fraction of faults from alpha particle and neutron strikes that result in user-visible errors. This paper shows how to use a performance model to calculate the AVF of address-based structures, using a data cache, a data translation buffer, and a store buffer as examples. We describe how to perform a detailed breakdown of lifetime components (e.g., fill-to-read, read-to-evict) of bits in these structures into ACE (required for architecturally correct execution), un-ACE (unnecessary for ACE), and unknown components. This lifetime analysis produces best estimate AVFs for these three structures' data arrays of 6%, 36%, and 4%, respectively. We then present a new technique, hamming-distance-one analysis, and show that it predicts surprisingly low best estimate AVFs of 0.41%, 3%, and 7.7% for the structures' tag arrays. Finally, using our lifetime analysis framework, we show how two AVF reduction techniques - periodic flushing and incremental scrubbing - can reduce the AVF by converting ACE lifetime components into un-ACE without affecting performance significantly.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431584","","Buffer storage;CADCAM;Computer aided manufacturing;Computer architecture;Computer errors;Computer science;Data analysis;Hamming distance;Statistics;Sun","computer architecture;error handling;system recovery","ACE lifetime component;address-based structure;architectural vulnerability factor;architecturally correct execution;data cache;data translation buffer;fill-to-read;hamming-distance-one analysis;incremental scrubbing;lifetime analysis framework;lifetime component breakdown;on-chip structure;periodic flushing;processor designer;read-to-evict;soft error rate estimate;store buffer;structure data array;structure tag array;user-visible error","","66","1","14","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Virtualizing transactional memory","Rajwar, R.; Herlihy, M.; Konrad Lai","Microarchit. Res. Lab., Intel Corp., Santa Clara, CA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","494","505","Writing concurrent programs is difficult because of the complexity of ensuring proper synchronization. Conventional lock-based synchronization suffers from well-known limitations, so researchers have considered nonblocking transactions as an alternative. Recent hardware proposals have demonstrated how transactions can achieve high performance while not suffering limitations of lock-based mechanisms. However, current hardware proposals require programmers to be aware of platform-specific resource limitations such as buffer sizes, scheduling quanta, as well as events such as page faults, and process migrations. If the transactional model is to gain wide acceptance, hardware support for transactions must be virtualized to hide these limitations in much the same way that virtual memory shields the programmer from platform-specific limitations of physical memory. This paper proposes virtual transactional memory (VTM), a user-transparent system that shields the programmer from various platform-specific resource limitations. VTM maintains the performance advantage of hardware transactions, incurs low overhead in time, and has modest costs in hardware support. While many system-level challenges remain, VTM takes a step toward making transactional models more widely acceptable.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.54","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431581","","Computer architecture;Delay;Hardware;Microarchitecture;Programming profession;Proposals;Read-write memory;Scheduling;Switches;Yarn","digital storage;memory architecture;multiprocessing programs;storage allocation;virtual storage","concurrent programs;conventional lock-based synchronization;hardware transaction;lock-based mechanism;nonblocking transaction;physical memory;platform-specific resource limitation;synchronization complexity;transaction hardware support;transactional memory virtualization;user-transparent system;virtual memory;virtual transactional memory","","51","27","26","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Adaptive mechanisms and policies for managing cache hierarchies in chip multiprocessors","Speight, E.; Shafi, H.; Lixin Zhang; Rajamony, R.","Novel Syst. Archit. Group, IBM Res., Austin, TX, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","346","356","With the ability to place large numbers of transistors on a single silicon chip, manufacturers have begun developing chip multiprocessors (CMPs) containing multiple processor cores, varying amounts of level 1 and level 2 caching, and on-chip directory structures for level 3 caches and memory. The level 3 cache may be used as a victim cache for both modified and clean lines evicted from on-chip level 2 caches. Efficient area and performance management of this cache hierarchy is paramount given the projected increase in access latency to off-chip memory. This paper proposes simple architectural extensions and adaptive policies for managing the L2 and L3 cache hierarchy in a CMP system. In particular, we evaluate two mechanisms that improve cache effectiveness. First, we propose the use of a small history table to provide hints to the L2 caches as to which lines are resident in the L3 cache. We employ this table to eliminate some unnecessary clean write backs to the L3 cache, reducing pressure on the L3 cache and utilization of the on-chip bus. Second, we exam-ine the performance benefits of allowing write backs from L2 caches to be placed in neighboring, on-chip L2 caches rather than forcing them to be absorbed by the L3 cache. This not only reduces the capacity pressure on the L3 cache but also makes subsequent accesses faster since L2-W-L2 cache transfers have typically lower latencies than accesses to a large L3 cache array. We evaluate the performance improvement of these two designs, and their combined effect, on four commercial workloads and observe a reduction in the overall execution time of up to 13%.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.8","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431569","","Delay;Hardware;History;Manufacturing processes;Memory management;Project management;Silicon;Surface-mount technology;System-on-a-chip;Yarn","cache storage;capacity management (computers);microprocessor chips;performance evaluation;storage management;system buses","access latency;adaptive mechanisms;cache array;cache hierarchy;chip multiprocessors;multiple processor cores;off-chip memory;on-chip bus;on-chip directory structures;single silicon chip","","13","19","14","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Victim replication: maximizing capacity while hiding wire delay in tiled chip multiprocessors","Zhang, M.; Asanovic, K.","MIT Comput. Sci. & Artificial Intelligence Lab., Cambridge, MA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","336","345","In this paper, we consider tiled chip multiprocessors (CMP) where each tile contains a slice of the total on-chip L2 cache storage and tiles are connected by an on-chip network. The L2 slices can be managed using two basic schemes: 1) each slice is treated as a private L2 cache for the tile; 2) all slices are treated as a single large L2 cache shared by all tiles. Private L2 caches provide the lowest hit latency but reduce the total effective cache capacity, as each tile creates local copies of any line it touches. A shared L2 cache increases the effective cache capacity for shared data, but incurs long hit latencies when L2 data is on a remote tile. We present a new cache management policy, victim replication, which combines the advantages of private and shared schemes. Victim replication is a variant of the shared scheme which attempts to keep copies of local primary cache victims within the local L2 cache slice. Hits to these replicated copies reduce the effective latency of the shared L2 cache, while retaining the benefits of a higher effective capacity for shared data. We evaluate the various schemes using full-system simulation of both single-threaded and multi-threaded benchmarks running on an 8-processor tiled CMP. We show that victim replication reduces the average memory access latency of the shared L2 cache by an average of 16% for multi-threaded benchmarks and 24% for single-threaded benchmarks, providing better overall performance than either private or shared schemes.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.53","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431568","","Artificial intelligence;Cache storage;Clocks;Computer science;Delay;Laboratories;Throughput;Tiles;Wire;Yarn","benchmark testing;cache storage;delays;microprocessor chips;multi-threading;performance evaluation;program slicing;shared memory systems;storage management","average memory access latency;cache capacity;cache management policy;cache storage;multithreaded benchmarks;on-chip network;single-threaded benchmarks;tiled chip multiprocessors;victim replication;wire delay","","68","8","27","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"32nd International Symposium on Computer Architecture - Title Page","","","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","i","iii","Conference proceedings title page.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.4","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431534","","","","","","0","","","","","8-8 June 2005","","IEEE","IEEE Conference Publications"
"Disk drive roadmap from the thermal perspective: a case for dynamic thermal management","Gurumurthi, S.; Anand Sivasubramaniam; Natarajan, V.K.","Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","38","49","The importance of pushing the performance envelope of disk drives continues to grow, not just in the server market but also in numerous consumer electronics products. One of the most fundamental factors impacting disk drive design is the heat dissipation and its effect on drive reliability, since high temperatures can cause off-track errors, or even head crashes. Until now, drive manufacturers have continued to meet the 40% annual growth target of the internal data rates (IDR) by increasing RPMs, and shrinking platter sizes, both of which have counter-acting effects on the heat dissipation within a drive. As this paper shows, we are getting to a point where it is becoming very difficult to stay on this roadmap. This paper presents an integrated disk drive model that captures the close relationships between capacity, performance and thermal characteristics over time. Using this model, we quantify the drop off in IDR growth rates over the next decade if we are to adhere to the thermal envelope of drive design. We present two mechanisms for buying back some of this IDR loss with dynamic thermal management (DTM). The first DTM technique exploits any available thermal slack, between what the drive was intended to support and the currently lower operating temperature, to ramp up the RPM. The second DTM technique assumes that the drive is only designed for average case behavior, thus allowing higher RPMs than the thermal envelope, and employs dynamic throttling of disk drive activities to remain within this envelope.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431544","","Computer aided software engineering;Computer crashes;Computer science;Disk drives;Engineering management;Storage area networks;Temperature;Thermal engineering;Thermal management;Thermal management of electronics","cooling;disc drives;thermal management (packaging)","IDR growth rate;consumer electronics product;disk drive roadmap;drive reliability;dynamic thermal management;dynamic throttling;heat dissipation;internal data rate;shrinking platter size","","14","37","49","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Optimizing replication, communication, and capacity allocation in CMPs","Chishti, Z.; Powell, M.D.; Vijaykumar, T.N.","Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","357","368","Chip multiprocessors substantially increase capacity pressure on the on-chip memory hierarchy while requiring fast access. Neither private nor shared caches can provide both large capacity and fast access in CMPs. We observe that compared to symmetric multiprocessors (SMPs), CMPs change the latency capacity tradeoff in two significant ways. We propose three novel ideas to exploit the changes: (1) Through placing copies close to requestors allows fast access for read-only sharing, the copies also reduce the already-limited on-chip capacity in CMPs. We propose controlled replication to reduce capacity pressure by not making extra copies in some cases, and obtaining the data from an existing on-chip copy. This option is not suitable for SMPs because obtaining data from another processor is expensive and capacity is not limited to on-chip storage. (2) Unlike SMPs, CMPs allow fast on-chip communication between processors for read-write sharing. Instead of incurring slow access to read-write shared data through coherence misses as do SMPs, we propose in situ communication to provide fast access without making copies or incurring coherence misses. (3) Accessing neighbor's caches is not as expensive in CMPs as it is in SMPs. We propose capacity stealing in which private data that exceeds a core's capacity is placed in a neighboring cache with less capacity demand. To incorporate our ideas, we use a hybrid of private, per-processor tag arrays and a shared data array. Because the shared data is slow, we employ non-uniform access and distance associativity from previous proposals to hold frequently-accessed data in regions close to the requestor. We extend the previously-proposed non-uniform access with replacement and placement using distance associativity (NuRAPID) ro CMPs, and call our cache CMP-NuRAPID. Our result show that for a 4-core CMP with 8 MB cache, CMP-NuRAPID improves performance by 13% over a shared cache and 8% over private caches for three commercial multithreaded workloads.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431570","","Costs;Delay effects;Pressure control;Proposals;Wire","cache storage;microprocessor chips;multi-threading;parallel processing;read-only storage;shared memory systems;storage allocation","CMP-NuRAPID;capacity allocation;chip multiprocessors;coherence misses;commercial multithreaded workloads;distance associativity;latency capacity tradeoff;on-chip communication;on-chip memory hierarchy;on-chip storage;per-processor tag arrays;read-only sharing;read-write sharing;replication;shared caches;shared data array;symmetric multiprocessors","","59","1","31","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Improving program efficiency by packing instructions into registers","Hines, S.; Green, J.; Tyson, G.; Whalley, D.","Comput. Sci. Dept., Florida State Univ., Tallahassee, FL, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","260","271","New processors, both embedded and general purpose, often have conflicting design requirements involving space, power, and performance. Architectural features and compiler optimizations often target one or more design goals at the expense of the others. This paper presents a novel architectural and compiler approach to simultaneously reduce power requirements, decrease code size, and improve performance by integrating an instruction register file (IRF) into the architecture. Frequently occurring instructions are placed in the IRF. Multiple entries in the IRF can be referenced by a single packed instruction in ROM or LI instruction cache. Unlike conventional code compression, our approach allows the frequent instructions to be referenced in arbitrary combinations. The experimental results show significant improvements in space and power, as well as some improvement in execution time when using only 32 entries. These advantages make packing instructions into registers an effective approach for improving overall efficiency.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.32","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431562","","Costs;Design optimization;Encoding;Energy consumption;Instruction sets;Logic;Optimizing compilers;Process design;Read only memory;Registers","cache storage;computer architecture;embedded systems;instruction sets;optimising compilers","LI instruction cache;ROM;architectural features;code compression;compiler optimizations;instruction register file;registers;single packed instruction","","5","1","25","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Microarchitecture of a high radix router","Kim, J.; Dally, W.J.; Towles, B.; Gupta, A.K.","Comput. Syst. Lab., Stanford Univ., CA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","420","431","Evolving semiconductor and circuit technology has greatly increased the pin bandwidth available to a router chip. In the early 90s, routers were limited to 10Gb/s of pin bandwidth. Today 1Tb/s is feasible, and we expect 20Tb/s of I/O bandwidth by 2010. A high-radix router that provides many narrow Dalports is more effective in converting pin band-width to reduced latency and reduced cost than the alternative of building a router with a few wide ports. However, increasing the radix (or degree) of a router raises several challenges as internal switches and allocators scale as the square of the radix. This paper addresses these challenges by proposing and evaluating alternative microarchitectures for high radix routers. We show that the use of a hierarchical switch organization with per-virtual-channel buffers in each subswitch enables an area savings of 40% compared to a fully buffered crossbar and a throughput increase of 20-60% compared to a conventional crossbar implementation.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431575","","Bandwidth;Buildings;Costs;Delay;Laboratories;Microarchitecture;Multiprocessor interconnection networks;Research and development;Switches;Throughput","buffer storage;microprocessor chips;pipeline arithmetic;telecommunication network routing;telecommunication switching","allocators;buffered crossbar;circuit technology;hierarchical switch organization;high radix router;internal switches;microarchitecture;per-virtual-channel buffers;pin bandwidth;router chip;semiconductor technology","","33","1","","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"32nd International Symposium on Computer Architecture - Table of contents","","","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","v","viii","Presents the table of contents of the proceedings.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.3","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431536","","","","","","0","","","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Increased scalability and power efficiency by using multiple speed pipelines","Talpes, E.; Marculescu, D.","Dept. of Comput. & Electr. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","310","321","One of the most important problems faced by microarchitecture designers is the poor scalability of some of the current solutions with increased clock frequencies and wider pipelines. As several studies show, internal processor structures scale differently with decreasing device sizes. While in some cases the access latency is determined by the speed of the logic circuitry, for others it is dominated by the interconnect delay. Furthermore, while some stages can be super-pipelined with relatively small performance loss, others must be kept atomic. This paper proposes a possible solution to this problem, avoiding the traditional trade-off between parallelism and clock speed. First, allowing instructions to enter and leave the Issue Window in an asynchronously manner enables faster speeds in the front-end at the expense of small synchronization latencies. Second, using an Execution Cache for storing instructions that are already scheduled allows for bypassing the issue circuitry and thus clocking the execution core at higher frequencies. Combined, these two mechanisms result in a 50% to 60% performance increase for our test microarchitecture, without requiring a completely new scheduling mechanism. Furthermore, the proposed microarchitecture requires significantly less energy, with 30% reduction in a 0.1 Sum or 20% in a 0.06um process technology over the original baseline.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.33","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431566","","Clocks;Delay;Frequency;Integrated circuit interconnections;Logic circuits;Logic devices;Microarchitecture;Performance loss;Pipelines;Scalability","instruction sets;logic circuits;multiprocessor interconnection networks;performance evaluation;pipeline processing;processor scheduling;program processors;program testing;synchronisation","access latency;execution cache;interconnect delay;internal processor structures;logic circuitry;microarchitecture designers;multiple speed pipelines;power efficiency;scalability;synchronization latency","","1","1","28","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"list-reviewer","","","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","xvii","xviii","The conference offers a note of thanks and lists its reviewers.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.45","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431540","","IEEE","","","","0","","","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"An integrated memory array processor architecture for embedded image recognition systems","Kyo, S.; Okazaki, S.; Arai, T.","Media & Inf. Res. Labs., NEC Corp., Japan","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","134","145","Embedded processors for video image recognition require to address both the cost (die size and power) versus real-time performance issue, and also to achieve high flexibility due to the immense diversity of recognition targets, situations, and applications. This paper describes IMAP, a highly parallel SIMD linear processor and memory array architecture that addresses these trading-off requirements. By using parallel and systolic algorithmic techniques, despite of its simple architecture IMAP achieves to exploit not only the straightforward per image row data level parallelism (DLP), but also the inherent DLP of other memory access patterns frequently found in various image recognition tasks, under the use of an explicit parallel C language (IDC). We describe and evaluate IMAP-CE, a latest IMAP processor, which integrates 128 of 100MHz 8 bit 4-way VLIW PEs, 128 of 2KByte RAMs, and one 16 bit RISC control processor, into a single chip. The PE instruction set is enhanced for supporting IDC codes. IMAP-CE is evaluated mainly by comparing its performance running IDC codes with that of a 2.4GHz Intel P4 running optimized C codes. Based on the use of parallelizing techniques, benchmark results show a speedup of up to 20 for image filter kernels, and of 4 for a full image recognition application.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431552","","Costs;Filters;Image recognition;Kernel;Memory architecture;Process control;Random access memory;Reduced instruction set computing;Target recognition;VLIW","C language;image recognition;memory architecture;microprocessor chips;parallel architectures;parallelising compilers;program processors;random-access storage;reduced instruction set computing","IDC code;IMAP processor;IMAP-CE;PE instruction set;RAM;RISC control processor;SIMD linear processor;VLIW PE;data level parallelism;embedded image recognition system;image filter kernel;memory array processor architecture;parallel C language;parallel algorithmic technique;systolic algorithmic technique;video image recognition","","19","3","37","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Direct cache access for high bandwidth network I/O","Huggahalli, R.; Iyer, R.; Tetrick, S.","","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","50","59","Recent I/O technologies such as PCI-Express and 10 Gb Ethernet enable unprecedented levels of I/O bandwidths in mainstream platforms. However, in traditional architectures, memory latency alone can limit processors from matching 10 Gb inbound network I/O traffic. We propose a platform-wide method called direct cache access (DCA) to deliver inbound I/O data directly into processor caches. We demonstrate that DCA provides a significant reduction in memory latency and memory bandwidth for receive intensive network I/O applications. Analysis of benchmarks such as SPECWeb9, TPC-W and TPC-C shows that overall benefit depends on the relative volume of I/O to memory traffic as well as the spatial and temporal relationship between processor and I/O memory accesses. A system level perspective for the efficient implementation of DCA is presented.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.23","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431545","","Application software;Bandwidth;Computer architecture;Delay;Ethernet networks;Internet;Protocols;TCPIP;Telecommunication traffic;Throughput","cache storage;local area networks;memory architecture;peripheral interfaces","Ethernet;I/O bandwidth network;I/O memory access;PCI-express;SPECWeb9;TPC-C;TPC-W;direct cache access;inbound network I/O traffic;memory bandwidth;memory latency;platform-wide method;processor cache;spatial-temporal relationship","","15","6","21","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Store buffer design in first-level multibanked data caches","Torres, E.F.; Ibanez, P.; Vinals, V.; Llaberia, J.M.","HiPEAC NoE, Zaragoza Univ., Spain","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","469","480","This paper focuses on how to design a store buffer (STB) well suited to first-level multibanked data caches. Our goal is to forward data from in-flight stores to dependent loads with the latency of a cache bank. For that we propose a particular two-level STB design in which forwarding is done speculatively from a distributed first-level STB made of extremely small banks, while a centralized, second-level STB enforces correct store-load ordering a few cycles later. To that end we have identified several important design decisions: i) delaying allocation of first-level STB entries until stores execute; ii) deallocating first-level STB entries before stores commit; and iii) selecting a recovery policy well-matched to data forwarding misspeculations. Moreover, the two-level STB admits two enhancements that simplify the design leaving performance almost unchanged: i) removing the data forwarding capability from the second-level STB; and ii) not checking instruction age in first-level STB prior to forwarding data to loads. Following our guidelines and running SPECint-2K over an 8-way out-of-order processor, a two-level STB (first level with four STB banks of 8 entries each) performs similarly to an ideal, single-level STB with 128-entry banks working at the first-level cache latency.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.47","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431579","","Buffer storage;CADCAM;Circuits;Computer aided manufacturing;Delay;Logic;Out of order;Pipelines;US Department of Transportation;Writing","cache storage;memory architecture;storage allocation;system recovery","cache bank;data forwarding;first-level multibanked data caches;latency;store buffer;store buffer design;store-load ordering","","9","","22","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"The impact of performance asymmetry in emerging multicore architectures","Balakrishnan, S.; Rajwar, R.; Upton, M.; Konrad Lai","Comput. Sci. Dept., Univ. of Wisconsin-Madison, Madison, WI, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","506","517","Performance asymmetry in multicore architectures arises when individual cores have different performance. Building such multicore processors is desirable because many simple cores together provide high parallel performance while a few complex cores ensure high serial performance. However, application developers typically assume computational cores provide equal performance, and performance asymmetry breaks this assumption. This paper is concerned with the behavior of commercial applications running on performance asymmetric systems. We present the first study investigating the impact of performance asymmetry on a wide range of commercial applications using a hardware prototype. We quantify the impact of asymmetry on an application's performance variance when run multiple times, and the impact on the application's scalability. Performance asymmetry adversely affects behavior of many workloads. We study ways to eliminate these effects. In addition to asymmetry-aware operating system kernels, the application often itself needs to be aware of performance asymmetry for stable and scalable performance.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.51","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431582","","Application software;Computer architecture;Concurrent computing;Frequency;Hardware;Microarchitecture;Multicore processing;Operating systems;Prototypes;Scalability","computer architecture;performance evaluation","application developer;asymmetry-aware operating system kernel;commercial application behavior;computational core;equal performance;hardware prototype;high parallel performance;high serial performance;multicore architecture;multicore processor;performance asymmetric system;performance asymmetry","","49","2","21","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Interconnections in multi-core architectures: understanding mechanisms, overheads and scaling","Kumar, R.; Zyuban, V.; Tullsen, D.M.","Dept. of Comput. Sci. & Eng., California Univ., San Diego, CA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","408","419","This paper examines the area, power, performance, and design issues for the on-chip interconnects on a chip multiprocessor, attempting to present a comprehensive view of a class of interconnect architectures. It shows that the design choices for the interconnect have significant effect on the rest of the chip, potentially consuming a significant fraction of the real estate and power budget. This research shows that designs that treat interconnect as an entity that can be independently architected and optimized would not arrive at the best multi-core design. Several examples are presented showing the need for careful co-design. For instance, increasing interconnect bandwidth requires area that then constrains the number of cores or cache sizes, and does not necessarily increase performance. Also, shared level-2 caches become significantly less attractive when the overhead of the resulting crossbar is accounted for. A hierarchical bus structure is examined which negates some of the performance costs of the assumed base-line architecture.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431574","","Bandwidth;Computer architecture;Computer science;Delay;Design engineering;Joining processes;Power engineering and energy;Power system interconnection;Space exploration;Space technology","cache storage;microprocessor chips;multiprocessor interconnection networks;parallel architectures;performance evaluation;system buses","chip multiprocessor;hierarchical bus structure;interconnect architectures;interconnect bandwidth;interconnections;multicore architectures;multicore design;on-chip interconnects","","92","15","31","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"An evaluation framework and instruction set architecture for ion-trap based quantum micro-architectures","Balensiefer, S.; Kregor-Stickles, L.; Oskin, M.","Dept. of Comput. Sci. & Eng., Univ. of Washington, WA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","186","196","The theoretical study of quantum computation has yielded efficient algorithms for some traditionally hard problems. Correspondingly, experimental work on the underlying physical implementation technology has progressed steadily. However, almost no work has yet been done which explores the architecture design space of large scale quantum computing systems. In this paper, we present a set of tools that enable the quantitative evaluation of architectures for quantum computers. The infrastructure we created comprises a complete compilation and simulation system for computers containing thousands of quantum bits. We begin by compiling complete algorithms into a quantum instruction set. This ISA enables the simple manipulation of quantum state. Another tool we developed automatically transforms quantum software into an equivalent, fault-tolerant version required to operate on real quantum devices. Next, our infrastructure transforms the ISA into a set of low-level micro architecture specific control operations. In the future, these operations can be used to directly control a quantum computer. For now, our simulation framework quickly uses them to determine the reliability of the application for the target micro architecture. Finally, we propose a simple, regular architecture for ion-trap based quantum computers. Using our software infrastructure, we evaluate the design trade offs of this micro architecture.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.10","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431556","","Automatic control;Computational modeling;Computer architecture;Computer simulation;Instruction sets;Large-scale systems;Quantum computing;Quantum mechanics;Software tools;Space technology","computer architecture;fault tolerant computing;instruction sets;quantum computing","evaluation framework;fault-tolerant version;instruction set architecture;ion-trap based quantum micro-architecture;large scale quantum computing;physical implementation technology;quantum software","","10","","27","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"High efficiency counter mode security architecture via prediction and precomputation","Weidong Shi; Lee, H.-H.S.; Ghosh, M.; Chenghuai Lu; Boldyreva, A.","Sch. of Electr. & Comput. Eng., Technol. Inst., Atlanta, GA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","14","24","Encrypting data in unprotected memory has gained much interest lately for digital rights protection and security reasons. Counter mode is a well-known encryption scheme. It is a symmetric-key encryption scheme based on any block cipher, e.g. AES. The scheme's encryption algorithm uses a block cipher, a secret key and a counter (or a sequence number) to generate an encryption pad which is XORed with the data stored in memory. Like other memory encryption schemes, this method suffers from the inherent latency of decrypting encrypted data when loading them into the on-chip cache. In this paper, we present a novel technique to hide the latency overhead of decrypting counter mode encrypted memory by predicting the sequence number and pre-computing the encryption pad that we call one-time-pad or OTP. In contrast to the prior techniques of sequence number caching, our mechanism solves the latency issue by using idle decryption engine cycles to speculatively predict and pre-compute OTPs before the corresponding sequence number is loaded. This technique incurs very little area overhead. In addition, a novel adaptive OTP prediction technique is also presented to further improve our regular OTP prediction and precomputation mechanism. This adaptive scheme is not only able to predict encryption pads associated with static and infrequently updated cache lines but also those frequently updated ones as well. Experimental results using SPEC2000 benchmark show an 82% prediction rate. Moreover, we also explore several optimization techniques for improving the prediction accuracy. Two specific techniques, two-level prediction and context-based prediction are presented and evaluated.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.30","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431542","","Computer architecture;Coprocessors;Counting circuits;Cryptography;Data engineering;Data privacy;Data security;Delay;Information security;Protection","cache storage;cryptography;optimisation","OTP precomputation;OTP prediction;block cipher;context-based prediction;counter mode security architecture;data encryption;digital rights protection;idle decryption engine cycle;one-time-pad;optimization;sequence number caching;symmetric-key encryption;two-level prediction","","1","5","26","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Mitigating Amdahl's law through EPI throttling","Annavaram, M.; Grochowski, E.; Shen, J.","Microarchitecture Res. Lab., Intel Corp., Santa Clara, CA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","298","309","This paper is motivated by three recent trends in computer design. First, chip multi-processors (CMPs) with increasing numbers of CPU cores per chip are becoming common. Second, multi-threaded software that can take advantage of CMPs will soon become prevalent. Due to the nature of the algorithms, these multi-threaded programs inherently will have phases of sequential execution; Amdahl's law dictates that the speedup of such parallel programs will be limited by the sequential portion of the computation. Finally, increasing levels of on-chip integration coupled with a slowing rate of reduction in supply voltage make power consumption a first order design constraint. Given this environment, our goal is to minimize the execution times of multi-threaded programs containing nontrivial parallel and sequential phases, while keeping the CMP's total power consumption within a fixed budget. In order to mitigate the effects of Amdahl's law, in this paper we make a compelling case for varying the amount of energy expended to process instructions according to the amount of available parallelism. Using the equation, Power-Energy per instruction (EPI) * Instructions per second (IPS), we propose that during phases of limited parallelism (low IPS) the chip multi-processor will spend more EPI; similarly, during phases of higher parallelism (high IPS) the chip multi-processor will spend less EPI; in both scenarios power is fixed. We evaluate the performance benefits of an EPI throttle on an asymmetric multiprocessor (AMP) prototyped from a physical 4-way Xeon SMP server. Using a wide range of multi-threaded programs, we show a 38% wall clock speedup on an AMP compared to a standard SMP that uses the same power. We also measure the supply current on a 4-way SMP server while running the multi-threaded programs and use the measured data as input to a software simulator that implements a more flexible EPI throttle. The results from the measurement-driven simulation show performance benefits comparable to the AMP prototype. We analyze the results from both techniques, explain why and when an EPI throttle works well, and conclude with a discussion of the challenges in building practical EPI throttles.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.36","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431565","","Clocks;Concurrent computing;Current measurement;Energy consumption;Equations;Parallel processing;Power supplies;Prototypes;Software measurement;Voltage","multi-threading;multiprocessing systems;power consumption","AMP prototype;Amdahl law;EPI throttling;Xeon SMP server;asymmetric multiprocessor;chip multiprocessors;computer design;first order design constraint;instructions per second;multithreaded programs;multithreaded software;on-chip integration;parallel programs;power consumption;power-energy per instruction;sequential execution;software simulator;supply voltage reduction","","41","3","","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Techniques for efficient processing in runahead execution engines","Mutlu, O.; Hyesoon Kim; Patt, Y.N.","Dept. of Electr. & Comput. Eng., Univ. of Texas at Austin, TX, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","370","381","Runahead execution is a technique that improves processor performance by pre-executing the running application instead of stalling the processor when a long-latency cache miss occurs. Previous research has shown that this technique significantly improves processor performance. However, the efficiency of runahead execution, which directly affects the dynamic energy consumed by a runahead processor, has not been explored. A runahead processor executes significantly more instructions than a traditional out-of-order processor, sometimes without providing any performance benefit, which makes it inefficient. In this paper, we describe the causes of inefficiency in runahead execution and propose techniques to make a runahead processor more efficient, thereby reducing its energy consumption and possibly increasing its performance. Our analyses and results provide two major insights: (1) the efficiency of runahead execution can be greatly improved with simple techniques that reduce the number of short, overlapping, and useless runahead periods, which we identify as the three major causes of inefficiency; (2) simple optimizations targeting the increase of useful prefetches generated in runahead mode can increase both the performance and efficiency of a runahead processor. The techniques we propose reduce the increase in the number of instructions executed due to runahead execution from 26.5% to 6.2%, on average, without significantly affecting the performance improvement provided by runahead execution.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.49","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431571","","Application software;Clocks;Delay;Energy consumption;Engines;Out of order;Performance analysis;Prefetching;Process design;Terminology","cache storage;instruction sets;performance evaluation;power consumption","dynamic energy;energy consumption;long-latency cache miss;out-of-order processor;runahead execution engines;runahead processor","","11","2","15","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Near-optimal worst-case throughput routing for two-dimensional mesh networks","DaeHo Seo; Akif Ali; Won-Taek Lim; Rafique, N.","Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","432","443","Minimizing latency and maximizing throughput are important goals in the design of routing algorithms for interconnection networks. Ideally, we would like a routing algorithm to (a) route packets using the minimal number of hops to reduce latency and preserve communication locality, (b) deliver good worst-case and average-case throughput and (c) enable low-complexity (and hence, low latency) router implementation. In this paper, we focus on routing algorithms for an important class of interconnection networks: two dimensional (2D) mesh networks. Existing routing algorithms for mesh networks fail to satisfy one or more of design goals mentioned above. Variously, the routing algorithms suffer from poor worst case throughput (ROMM by Nesson, and Johnsson (1995), DOR by Sullivan and Bashkow, 1977), poor latency due to increased packet hops (VALIANT by Valiant and Brebner (1981)) or increased latency due to hardware complexity (minimal-adaptive by Duato, (1993), Upadhyay, Varavithya and Mohapatra, (1997)). The major contribution of this paper is the design of an oblivious routing algorithm-O1TURN-with provable near-optimal worst-case throughput, good average-case through-put, low design complexity and minimal number of network hops for 2D-mesh networks, thus satisfying all the stated design goals.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.37","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431576","","Algorithm design and analysis;Computer networks;Delay;Mesh networks;Multiprocessor interconnection networks;Network topology;Routing;Switches;Telecommunication traffic;Throughput","communication complexity;multiprocessor interconnection networks;telecommunication network routing","2D mesh networks;O1TURN;communication locality;hardware complexity;interconnection networks;low-complexity router implementation;near-optimal worst-case throughput routing;packet hops;reduce latency;route packets;routing algorithms;worst case throughput","","6","","32","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"General Chair's message","","","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","ix","ix","Presents the welcome message from the conference proceedings.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.29","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431537","","","","","","0","","","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Opportunistic transient-fault detection","Gomaa, M.A.; Vijaykumar, T.N.","Sch. of Electr. & Comput. Eng., Purdue Univ., USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","172","183","CMOS scaling increases susceptibility of microprocessors to transient faults. Most current proposals for transient-fault detection use full redundancy to achieve perfect coverage while incurring significant performance degradation. However, most commodity systems do not need or provide perfect coverage. A recent paper explores this leniency to reduce the soft-error rate of the issue queue during L2 misses while incurring minimal performance degradation. Whereas the previous paper reduces soft-error rate without using any redundancy, we target better coverage while incurring similarly-minimal performance degradation by opportunistically using redundancy. We propose two semi-complementary techniques, called partial explicit redundancy (PER) and implicit redundancy through reuse (IRTR), to explore the trade-off between soft-error rate and performance. PER opportunistically exploits low-ILP phases and L2 misses to introduce explicit redundancy with minimal performance degradation. Because PER covers the entire pipeline and exploits not only L2 misses but all low-ILP phases, PER achieves better coverage than the previous work. To achieve coverage in high-ILP phases as well, we propose implicit redundancy through reuse (IRTR). Previous work exploits the phenomenon of instruction reuse to avoid redundant execution while falling back on redundant execution when there is no reuse. IRTR takes reuse to the extreme of performance-coverage trade-off and completely avoids explicit redundancy by exploiting reuse's implicit redundancy within the main thread for fault detection with virtually no performance degradation. Using simulations with SPEC2000, we show that PER and IRTR achieve better tradeoff between soft-error rate and performance degradation than the previous schemes.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.38","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431555","","Degradation;Fault detection;Microprocessors;Packaging;Pipelines;Power system reliability;Proposals;Redundancy;Voltage;Yarn","CMOS integrated circuits;fault tolerance;microprocessor chips;redundancy","CMOS scaling;ILP phase;SPEC2000;implicit redundancy through reuse;instruction reuse;partial explicit redundancy;performance degradation;performance-coverage trade-off;reuse implicit redundancy;soft-error rate;transient-fault detection","","37","","25","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"An architecture framework for transparent instruction set customization in embedded processors","Clark, N.; Blome, J.; Chu, M.; Mahlke, S.; Biles, S.; Flautner, K.","Adv. Comput. Archit. Lab., Michigan Univ., Ann Harbor, MI, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","272","283","Instruction set customization is an effective way to improve processor performance. Critical portions of application data-flow graphs are collapsed for accelerated execution on specialized hardware. Collapsing dataflow subgraphs will compress the latency along critical paths and reduces the number of intermediate results stored in the register file. While custom instructions can be effective, the time and cost of designing a new processor for each application is immense. To overcome this roadblock, this paper proposes a flexible architectural framework to transparently integrate custom instructions into a general-purpose processor. Hardware accelerators are added to the processor to execute the collapsed subgraphs. A simple microarchitectural interface is provided to support a plug-and-play model for integrating a wide range of accelerators into a pre-designed and verified processor core. The accelerators are exploited using an approach of static identification and dynamic realization. The compiler is responsible for identifying profitable subgraphs, while the hardware handles discovery, mapping, and execution of compatible subgraphs. This paper presents the design of a plug-and-play transparent accelerator system arid evaluates the cost/performance implications of the design.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.9","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431563","","Acceleration;Application software;Application specific processors;Computer architecture;Costs;Delay;Hardware;Laboratories;Process design;Registers","data flow graphs;embedded systems;instruction sets;performance evaluation;program compilers;reduced instruction set computing","architecture framework;collapsed subgraphs;collapsing dataflow subgraphs;data-flow graphs;dynamic realization;embedded processors;flexible architectural framework;hardware accelerators;microarchitectural interface;plug-and-play transparent accelerator system;processor core;static identification;transparent instruction set customization","","24","1","28","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Energy-effectiveness of pre-execution and energy-aware p-thread selection","Petric, V.; Roth, A.","Dept. of Comput. & Inf. Sci., Pennsylvania Univ., Philadelphia, PA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","322","333","Pre-execution removes the microarchitectural latency of ""problem"" loads from a program's critical path by redundantly executing copies of their computations in parallel with the main program. There have been several proposed pre-execution systems, a quantitative framework (PTHSEL) for analytical pre-execution thread (p-thread) selection, and even a research prototype. To date, however, the energy aspects of pre-execution have not been studied. Cycle-level performance and energy simulations on SPEC2000 integer benchmarks that suffer from L2 misses show that energy-blind pre-execution naturally has a linear latency/energy trade-off, improving performance by 13.8% while increasing energy consumption by 11.9%. To improve this trade-off, we propose two extensions to PTHSEL. First, we replace the flat cycle-for-cycle load cost model with a model based on a critical-path estimation. This extension increases p-thread efficiency in an energy-independent way. Second, we add a parameterized energy model to PTHSEL (forming PTHSEL<sub>+E</sub>) that allows it to actively select p-threads that reduce energy rather than (or in combination with) execution latency. Experiments show that PTHSEL<sub>+E</sub> manipulates pre-execution's latency/energy more effectively. Latency targeted selection benefits from the improved load cost model: its performance improvements grow to an average of 16.4% while energy costs drop to 8.7%. ED targeted selection produces p-threads that improve performance by only 12.9%, but ED by 8.8%. Targeting p-thread selection for energy reduction, results in ""energy-free"" pre-execution, with average speedup of 5.4%, and a small decrease in total energy consumption (0.7%).","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.27","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431567","","Computer aided instruction;Computer architecture;Delay;Energy consumption;Hardware;Information science;Multithreading;Pareto optimization;Prefetching;Yarn","microprogramming;performance evaluation","SPEC2000 integer benchmarks;critical-path estimation;energy consumption;energy trade-off;energy-aware p-thread selection;energy-blind pre-execution;execution latency;flat cycle-for-cycle load cost model;latency targeted selection;linear latency;microarchitectural latency;parameterized energy model;pre-execution p-thread selection","","0","","25","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Temporal streaming of shared memory","Wenisch, T.F.; Somogyi, S.; Hardavellas, N.; Jangwoo Kim; Ailamaki, A.; Babak Falsafi","Comput. Archit. Lab., Carnegie Mellon Univ., USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","222","233","Coherent read misses in shared-memory multiprocessors account for a substantial fraction of execution time in many important scientific and commercial workloads. We propose temporal streaming, to eliminate coherent read misses by streaming data to a processor in advance of the corresponding memory accesses. Temporal streaming dynamically identifies address sequences to be streamed by exploiting two common phenomena in shared-memory access patterns: (1) temporal address correlation-groups of shared addresses tend to be accessed together and in the same order; and (2) temporal stream locality-recently-accessed address streams are likely to recur. We present a practical design for temporal streaming. We evaluate our design using a combination of trace-driven and cycle-accurate full-system simulation of a cache-coherent distributed shared-memory system. We show that temporal streaming can eliminate 98% of coherent read misses in scientific applications, and between 43% and 60% in database and Web server workloads. Our design yields speedups of 1.07 to 3.29 in scientific applications, and 1.06 to 1.21 in commercial workloads.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.50","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431559","","Computer architecture;Databases;Delay;Fabrication;Hardware;Laboratories;Microarchitecture;Prefetching;Proposals;Web server","distributed memory systems;shared memory systems;storage allocation","Web server workload;address sequences;cache-coherent distributed shared memory system;coherent read misses;commercial workload;cycle-accurate full-system simulation;scientific workload;shared-memory access pattern;shared-memory multiprocessor;temporal streaming;trace-driven simulation","","23","1","31","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"A robust main-memory compression scheme","Ekman, M.; Stenstrom, P.","Dept. of Comput. Sci. & Eng., Chalmers Univ. of Technol., Goteborg, Sweden","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","74","85","Lossless data compression techniques can potentially free up more than 50% of the memory resources. However, previously proposed schemes suffer from high access costs. The proposed main-memory compression scheme practically eliminates performance losses of previous schemes by exploiting a simple and yet effective compression scheme, a highly-efficient structure for locating a compressed block in memory, and a hierarchical memory layout that allows compressibility of blocks to vary with a low fragmentation overhead. We have evaluated an embodiment of the proposed scheme in detail using 14 integer and floating point applications from the SPEC2000 suite along with two server applications and we show that the scheme robustly frees up 30% of the memory resources, on average, with a negligible impact on the performance of only 0.2% on average.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.6","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431547","","","data compression;floating point arithmetic;memory architecture","SPEC2000;data compression;floating point application;fragmentation overhead;hierarchical memory layout;main-memory compression;memory resources","","20","1","24","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Deconstructing commodity storage clusters","Gunawi, H.S.; Agrawal, N.; Arpaci-Dusseau, A.C.; Schindler, J.; Arpaci-Dusseau, R.H.","Dept. of Comput. Sci., Wisconsin Univ., Madison, WI, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","60","71","The traditional approach for characterizing complex systems is to run standard workloads and measure the resulting performance as seen by the end user. However, unique opportunities exist when characterizing a system that is itself constructed from standardized components: one can also look inside the system itself by instrumenting each of the components. In this paper, we show how intra-box instrumentation can help one understand the behavior of a large-scale storage cluster, the EMC Centera. In our analysis, we leverage standard tools for tracing both the disk and network traffic emanating from each node of the cluster. By correlating this traffic with the running workload, we are able to infer the structure of the software system (e.g., its write update protocol) as well as its policies (e.g., how it performs caching, replication, and load-balancing). Further, by imposing variable intra-box delays on network and disk traffic, we can confirm the causal relationships between network and disk events. Thus, we are able to infer the semantics of the messages between nodes without examining a single line of source code.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431546","","Electromagnetic compatibility;Instruction sets;Instruments;Large-scale systems;Measurement standards;Microprocessors;Protocols;Servers;Software systems;Telecommunication traffic","large-scale systems;storage area networks;telecommunication traffic;workstation clusters","EMC Centera;commodity storage cluster;complex system;intra-box instrumentation;large-scale storage cluster;load-balancing;network traffic;software system;write update protocol","","5","1","34","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Continuous optimization","Fahs, B.; Rafacz, T.; Patel, S.J.; Lumetta, Steven S.","Center for Reliable & High Performance Comput., Illinois Univ. at Urbana-Champaign, IL, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","86","97","This paper presents a hardware-based dynamic optimizer that continuously optimizes an application's instruction stream. In continuous optimization, dataflow optimizations are performed using simple, table-based hardware placed in the rename stage of the processor pipeline. The continuous optimizer reduces dataflow height by performing constant propagation, reassociation, redundant load elimination, store forwarding, and silent store removal. To enhance the impact of the optimizations, the optimizer integrates values generated by the execution units back into the optimization process. Continuous optimization allows instructions with input values known at optimization time to be executed in the optimizer, leaving less work for the out-of-order portion of the pipeline. Continuous optimization can detect branch mispredictions earlier and thus reduce the misprediction penalty. In this paper, we present a detailed description of a hardware optimizer and evaluate it in the context of a contemporary microarchitecture running current workloads. Our analysis of SPECint, SPECfp, and mediabench workloads reveals that a hardware optimizer can directly execute 33% of instructions, resolve 29% of mispredicted branches, and generate addresses for 76% of memory operations. These positive effects combine to provide speed ups in the range 0.99 to 1.27.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.19","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431548","","Decoding;Hardware;High performance computing;Microarchitecture;Optimizing compilers;Out of order;Pipelines;Program processors;Registers;Uninterruptible power systems","data flow computing;instruction sets;optimisation;optimising compilers;pipeline processing","SPECfp;SPECint;dataflow optimization;hardware-based dynamic optimizer;mediabench workload;processor pipeline;redundant load elimination;silent store removal;store forwarding;table-based hardware","","8","1","28","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Design and evaluation of hybrid fault-detection systems","Reis, G.A.; Chang, J.; Vachharajani, N.; Mukherjee, S.S.; Rangan, R.; August, D.I.","Dept. of Electr. Eng. & Comput. Sci., Princeton Univ., NJ, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","148","159","As chip densities and clock rates increase, processors are becoming more susceptible to transient faults that can affect program correctness. Up to now, system designers have primarily considered hardware-only and software-only fault-detection mechanisms to identify and mitigate the deleterious effects of transient faults. These two fault-detection systems, however, are extremes in the design space, representing sharp trade-offs between hardware cost, reliability, and performance. In this paper, we identify hybrid hardware/software fault-detection mechanisms as promising alternatives to hardware-only and software-only systems. These hybrid systems offer designers more options to fit their reliability needs within their hardware and performance budgets. We propose and evaluate CRAFT, a suite of three such hybrid techniques, to illustrate the potential of the hybrid approach. For fair, quantitative comparisons among hardware, software, and hybrid systems, we introduce a new metric, mean work to failure, which is able to compare systems for which machine instructions do not represent a constant unit of work. Additionally, we present a new simulation framework which rapidly assesses reliability and does not depend on manual identification of failure modes. Our evaluation illustrates that CRAFT, and hybrid techniques in general, offer attractive options in the fault-detection design space.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431553","","Clocks;Costs;Error correction codes;Fault diagnosis;Fault tolerance;Hardware;Microprocessors;Protection;Software systems;Space technology","fault diagnosis;fault tolerant computing;hardware-software codesign;instruction sets;performance evaluation;program processors","CRAFT;hardware cost;hardware reliability;hardware-only system;hybrid hardware-software fault-detection system;mean work to failure;software-only system;transient fault","","26","","29","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Rescue: a microarchitecture for testability and defect tolerance","Schuchman, E.; Vijaykumar, T.N.","Sch. of Electr. & Comput. Eng., Purdue Univ., USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","160","171","Scaling feature size improves processor performance but increases each device's susceptibility to defects (i.e., hard errors). As a result, fabrication technology must improve significantly to maintain yields. Redundancy techniques in memory have been successful at improving yield in the presence of defects. Apart from core sparing which disables faulty cores in a chip multiprocessor, little has been done to target the core logic. While previous work has proposed that either inherent or added redundancy in the core logic can be used to tolerate defects, the key issues of realistic testing and fault isolation have been ignored. This paper is the first to consider testability and fault isolation in designing modern high-performance, defect-tolerant microarchitectures. We define intra-cycle logic independence (ICI) as the condition needed for conventional scan test to isolate faults quickly to the microarchitectural-block granularity. We propose logic transformations to redesign conventional superscalar microarchitecture to comply with ICI. We call our novel, testable, and defect-tolerant microarchitecture Rescue.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.44","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431554","","CMOS technology;Circuit faults;Fabrication;Isolation technology;Logic testing;Microarchitecture;Power generation economics;Redundancy;Technological innovation;Throughput","computer architecture;design for testability;fault tolerance;microprocessor chips;redundancy","Rescue microarchitecture;chip multiprocessor;defect tolerance;fault isolation;hard error;intra-cycle logic independence;microarchitectural-block granularity;redundancy technique;superscalar microarchitecture","","7","3","28","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Proceedings. 32nd International Symposium on Computer Architecture","","","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","","","The following topics are dealt with: security; interacting with disks and networks; memory compression and renamer optimizations; specialized processors; detecting faults; quantum computing and very low power; coherence; applying compilers and debugging support; power; chip multiprocessor memory hierarchies; runahead and branch prediction; interconnection networks; load and store queues; multiprocessor issues; reliability and a cache organization.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.2","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431533","","","cache storage;disc drives;fault diagnosis;memory architecture;microprocessor chips;multiprocessor interconnection networks;optimisation;program compilers;program debugging;quantum computing;queueing theory;reliability;security of data","branch prediction;cache organization;chip multiprocessor memory hierarchies;coherence;compiler application;debugging support;fault detection;interconnection network;memory compression;multiprocessor issue;quantum computing;reliability;renamer optimization;runahead prediction;security;specialized processor;store queues","","0","","","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Dynamic verification of sequential consistency","Meixner, A.; Sorin, D.J.","Dept. of Comput. Sci., Duke Univ., Durnham, NC, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","482","493","In this paper, we develop the first feasibly implemental scheme for end-to-end dynamic verification of multithreaded memory systems. For multithreaded (including multiprocessor) memory systems, end-to-end correctness is defined by its memory consistency model. One such consistency model is sequential consistency (SC), which specifies that all loads and stores appear to execute in a total order that respects program order for each thread. Our design, DVSC-Indirect, performs dynamic verification of SC (DVSC) by dynamically verifying a set of sub-invariants that, when taken together, have been proven equivalent to SC. We evaluate DVSC-Indirect with full-system simulation and commercial workloads. Our results for multiprocessor systems with both directory and snooping cache coherence show that DVSC-Indirect detects all injected errors that affect system correctness (i.e., SC). We show that it uses only a small amount more bandwidth (less than 25%) than an unprotected system and thus can achieve comparable performance when provided with only modest additional link bandwidth.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.25","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431580","","Bandwidth;Bit error rate;Coherence;Computer architecture;Computer science;Control systems;Hardware;Multiprocessor interconnection networks;Protocols;System recovery","digital storage;memory architecture;multi-threading;multiprocessing systems;program verification","DVSC-Indirect;SC dynamic verification;commercial workload;directory cache coherence;end-to-end correctness;end-to-end dynamic verification;full-system simulation;injected errors detection;link bandwidth;memory consistency model;multiprocessor system;multithreaded memory system;sequential consistency;snooping cache coherence;system correctness;unprotected system","","11","","19","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Exploiting structural duplication for lifetime reliability enhancement","Srinivasan, J.; Adve, S.V.; Bose, P.; Rivers, J.A.","Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, IL, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","520","531","Increased power densities (and resultant temperatures) and other effects of device scaling are predicted to cause significant lifetime reliability problems in the near future. In this paper, we study two techniques that leverage microarchitectural structural redundancy for lifetime reliability enhancement. First, in structural duplication (SD), redundant microarchitectural structures are added to the processor and designated as spares. Spare structures can be turned on when the original structure fails, increasing the processor's lifetime. Second, graceful performance degradation (GPD) is a technique which exploits existing microarchitectural redundancy for reliability. Redundant structures that fail are shut down while still maintaining functionality, thereby increasing the processor's lifetime, but at a lower performance. Our analysis shows that exploiting structural redundancy can provide significant reliability benefits, and we present guidelines for efficient usage of these techniques by identifying situations where each is more beneficial. We show that GPD is the superior technique when only limited performance or cost resources can be sacrificed for reliability. Specifically, on average for our systems and applications, GPD increased processor reliability to 1.42 times the base value for less than a 5% loss in performance. On the other hand, for systems where reliability is more important than performance or cost, SD is more beneficial. SD increases reliability to 3.17 times the base value for 2.25 times the base cost, for our applications. Finally, a combination of the two techniques (SD+GPD) provides the highest reliability benefit.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.28","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431583","","Computer science;Costs;Degradation;Guidelines;Maintenance;Microarchitecture;Process design;Redundancy;Rivers;Temperature","computer architecture;configuration management;program control structures;program processors;reliability theory","SD+GPD;device scaling;graceful performance degradation;lifetime reliability enhancement;lifetime reliability problem;microarchitectural structural redundancy;power density;processor lifetime;processor reliability;redundant microarchitectural structure;resultant temperature;spare structure;structural duplication;system reliability","","57","1","22","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"32nd International Symposium on Computer Architecture - Copyright Page","","","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","iv","iv","Copyright and Reprint Permissions: Abstracting is permitted with credit to the source. Libraries may photocopy beyond the limits of US copyright law, for private use of patrons, those articles in this volume that carry a code at the bottom of the first page, provided that the per-copy fee indicated in the code is paid through the Copyright Clearance Center. The papers in this book comprise the proceedings of the meeting mentioned on the cover and title page. They reflect the authors' opinions and, in the interests of timely dissemination, are published as presented and without change. Their inclusion in this publication does not necessarily constitute endorsement by the editors or the Institute of Electrical and Electronics Engineers, Inc.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.1","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431535","","","","","","0","","","","","8-8 June 2005","","IEEE","IEEE Conference Publications"
"Analysis of the O-GEometric history length branch predictor","Seznec, A.","IRISA/INRIA/HIPEAC, Rennes, France","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","394","405","In this paper, we introduce and analyze the Optimized GEometric History Length (O-GEHL) branch Predictor that efficiently exploits very long global histories in the 100-200 bits range. The GEHL predictor features several predictor tables T(i) (e.g. 8) indexed through independent functions of the global branch history and branch address. The set of used global history lengths forms a geometric series, i.e., L(j) = α<sup>j-1</sup>L(1). This allows the GEHL predictor to efficiently capture correlation on recent branch outcomes as well as on very old branches. As on perceptron predictors, the prediction is computed through the addition of the predictions read on the predictor tables. The O-GEHL predictor further improves the ability of the GEHL predictor to exploit very long histories through the addition of dynamic history fitting and dynamic threshold fitting. The O-GEHL predictor can be ahead pipelined to provide in time predictions on every cycle.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.13","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431573","","Accuracy;Application software;Best practices;Computer architecture;Counting circuits;Fitting;History;Performance gain;Pipelines;Space exploration","computational geometry;parallel architectures;pipeline processing","GEHL predictor;O-GEHL predictor;dynamic history fitting;dynamic threshold fitting;optimized geometric history length branch predictor;perceptron predictors;predictor tables","","16","1","30","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Store vulnerability window (SVW): re-execution filtering for enhanced load optimization","Roth, A.","Dept. of Comput. & Inf. Sci., Pennsylvania Univ., Philadelphia, PA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","458","468","The load-store unit is a performance critical component of a dynamically-scheduled processor. It is also a complex and non-scalable component. Several recently proposed techniques use some form of speculation to simplify the load-store unit and check this speculation by re-executing some of the loads prior to commit. We call such techniques load optimizations. One recent load optimization improves load queue (LQ) scalability by using re-execution rather than associative search to check speculative intra- and inter- thread memory ordering. A second technique improves store queue (SQ) scalability by speculatively filtering some load accesses and some store entries from it and re-executing loads to check that speculation. A third technique speculatively removes redundant loads from the execution engine; re-execution detects false eliminations. Unfortunately, the benefits of a load optimization are often mitigated by re-execution itself Re-execution contends for cache bandwidth with store commit, and serializes load re-execution with subsequent store com-mit. If a given load optimization requires a sufficient number of load re-executions, the aggregate re-execution cost may overwhelm the benefits of the technique entirely and even cause drastic slowdowns. Store vulnerability window (SVW) is a new mechanism that significantly reduces the re-execution requirements of a given load optimization. SVW is based on monotonic store sequence numbering and an adaptation of Bloom filtering. The cost of a typical SVW implementation is a 1KB buffer and a 16-bit field per LQ entry. Across the three optimizations we study, SVW reduces re-executions by an average of 85%. This reduction relieves cache port contention and removes many of the dynamic serialization events that contribute the bulk of re-execution's cost, allows these load optimizations to perform up to their full potential. For the speculative SQ, this means the chance to perform at all, as without SVW it posts significant slowdowns.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.48","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431578","","Buffer storage;Computer architecture;Costs;Engines;Information filtering;Information filters;Information science;Pipelines;Retirement;Yarn","cache storage;dynamic scheduling;multi-threading;optimisation;processor scheduling;queueing theory;storage management","SVW implementation;bloom filtering;cache bandwidth;dynamically-scheduled processor;enhanced load optimization;inter-thread memory ordering;intra-thread memory ordering;load optimizations;load queue scalability;load-store unit;monotonic store sequence numbering;re-execution filtering;redundant loads;store queue scalability;store vulnerability window","","17","3","25","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Architecture for protecting critical secrets in microprocessors","Lee, R.B.; Kwan, P.C.S.; McGregor, J.P.; Dwoskin, Jeffrey; Zhenghong Wang","Dept. of Electr. Eng., Princeton Univ., NJ, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","2","13","We propose ""secret-protected (SP)"" architecture to enable secure and convenient protection of critical secrets for a given user in an on-line environment. Keys are examples of critical secrets, and key protection and management is a fundamental problem - often assumed but not solved <sup>n</sup>derlying the use of cryptographic protection of sensitive files, messages, data and programs. SP-processors contain a minimalist set of architectural features that can be built into a general-purpose microprocessor to provide protection of critical secrets and their computations, without expensive or inconvenient auxiliary hardware. SP-architecture also requires a trusted software module, a few modifications to the operating system, a secure I/O path to the user, and a secure installation process. Unique aspects of our architecture include: decoupling of user secrets from the devices, enabling users to securely access their keys from different networked computing devices; the use of symmetric master keys rather than more costly public-private key pairs; and the avoidance of any permanent or factory-installed device secrets.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431541","","Computer architecture;Computer networks;Cryptography;Data security;Hardware;IP networks;Microprocessors;Military computing;Operating systems;Protection","authorisation;computer architecture;microprocessor chips;public key cryptography","SP-processor;cryptographic protection;general-purpose microprocessor;secret-protected architecture;secure I/O path;secure installation;software module","","25","1","38","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Committees","","","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","xvi","xvi","Provides a listing of current committee members.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.17","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431539","","","","","","0","","","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Design and implementation of the AEGIS single-chip secure processor using physical random functions","Suh, G.E.; O'Donnell, C.W.; Ishan Sachdev; Devadas, S.","Comput. Sci. & Artificial Intelligence Lab. (CSAIL), Massachusetts Inst. of Technol., Cambridge, MA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","25","36","Secure processors enable new applications by ensuring private and authentic program execution even in the face of physical attack. In this paper, we present the AEGIS secure processor architecture, and evaluate its RTL implementation on FPGAs. By using physical random functions, we propose a new way of reliably protecting and sharing secrets that is more secure than existing solutions based on non-volatile memory. Our architecture gives applications the flexibility of trusting and protecting only a portion of a given process, unlike prior proposals which require a process to be protected in entirety. We also put forward a specific model of how secure applications can be programmed in a high-level language and compiled to run on our system. Finally, we evaluate a fully functional FPGA implementation of our processor, assess the implementation tradeoffs, compare performance, and demonstrate the benefits of partially protecting a program.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431543","","Application software;Artificial intelligence;Computer architecture;Computer science;Cryptography;Data security;Field programmable gate arrays;Hardware;Pervasive computing;Protection","authorisation;cryptography;field programmable gate arrays;high level languages;microprocessor chips;program processors;random functions","AEGIS single-chip secure processor;FPGA;RTL implementation;authentic program execution;high-level language;nonvolatile memory;physical random function","","19","12","24","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"BugNet: continuously recording program execution for deterministic replay debugging","Narayanasamy, S.; Pokam, G.; Calder, B.","Dept. of Comput. Sci. & Eng., California Univ., San Diego, CA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","284","295","Significant time is spent by companies trying to reproduce and fix the bugs that occur for released code. To assist developers, we propose the BugNet architecture to continuously record information on production runs. The information collected before the crash of a program can be used by the developers working in their execution environment to deterministically replay the last several million instructions executed before the crash. BugNet is based on the insight that recording the register file contents at any point in time, and then recording the load values that occur after that point can enable deterministic replaying of a program's execution. BugNet focuses on being able to replay the application's execution and the libraries it uses, but not the operating system. But our approach provides the ability to replay an application's execution across context switches and interrupts. Hence, BugNet obviates the need for tracking program I/O, interrupts and DMA transfers, which would have otherwise required more complex hardware support. In addition, BugNet does not require a final core dump of the system state for replaying, which significantly reduces the amount of data that must be sent back to the developer.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431564","","Application software;Computer architecture;Computer bugs;Computer crashes;Computer science;Continuous production;Core dumps;Debugging;Hardware;Software systems","computer architecture;input-output programs;instruction sets;operating systems (computers);program debugging;programming environments","BugNet architecture;DMA transfers;deterministic replay debugging;hardware support;operating system;program execution;tracking program I/O","","42","14","28","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Piecewise linear branch prediction","Jimenez, D.A.","Dept. of Comput. Sci., Rutgers Univ., USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","382","393","Improved branch prediction accuracy is essential to sustaining instruction throughput with today's deep pipelines. We introduce piecewise linear branch prediction, an idealized branch predictor that develops a set of linear functions, one for each program path to the branch to be predicted, that separate predicted taken from predicted not taken branches. Taken together, all of these linear functions form a piecewise linear decision surface. We present a limit study of this predictor showing its potential to greatly improve predictor accuracy. We then introduce a practical implementable branch predictor based on piecewise linear branch prediction. In making our predictor practical, we show how a parameterized version of it unifies the previously distinct concepts of perceptron prediction and path-based neural prediction. Our new branch predictor has implementation costs comparable to current prominent predictors in the literature while significantly improving accuracy. For a deeply pipelined simulated microarchitecture our predictor with a 256 KB hardware budget improves the harmonic mean normalized instructions-per-cycle rate by 8% over both the original path-based neural predictor and 2Bc-gskew. The average misprediction rate is decreased by 16% over the path-based neural predictor and by 22% over 2Bc-gskew.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.40","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431572","","Accuracy;Computer science;Costs;Delay;Hardware;Microarchitecture;Piecewise linear techniques;Pipelines;Predictive models;Throughput","microprogramming;parallel architectures;piecewise linear techniques;pipeline processing","implementable branch predictor;linear functions;path-based neural prediction;path-based neural predictor;perceptron prediction;piecewise linear branch prediction;piecewise linear decision surface;predictor accuracy;simulated microarchitecture","","9","1","22","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"A tree based router search engine architecture with single port memories","Baboescu, F.; Tullsen, D.M.; Rosu, G.; Singh, S.","Dept. of Comput. Sci. & Eng., California Univ., San Diego, CA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","123","133","Pipelined forwarding engines are used in core router to meet speed demands. Tree-based searches are pipelined across a number of stages to achieve high throughput, but this results in unevenly distributed memory. To address this imbalance, conventional approaches use either complex dynamic memory allocation schemes or over-provision each of the pipeline stages. This paper describes the microarchitecture of a novel network search processor which provides both high execution throughput and balanced memory distributor by dividing the tree into subtrees and allocating each subtree separately, allowing searches to begin at any pipeline stage. The architecture is validated by implementing and simulating state of the art solutions for IPv4 lookup, VPN forwarding and packet classification. The new pipeline scheme and memory allocator can provide searches with a memory allocation, efficiency that is within 1% of non-pipelined schemes.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.7","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431551","","Computer architecture;Computer science;Costs;Databases;High-speed networks;Microarchitecture;Pipelines;Search engines;Throughput;Virtual private networks","distributed memory systems;memory architecture;pipeline processing;routing protocols;search engines;storage allocation;storage management;tree searching","IPv4 lookup;VPN forwarding;balanced memory distributor;dynamic memory allocation;memory allocator;network search processor;packet classification;pipelined forwarding engine;single port memories;tree based router search engine","","30","2","28","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Scalable load and store processing in latency tolerant processors","Amit Gandhi; Akkary, H.; Rajwar, R.; Srinivasasn, S.T.; Konrad Lai","Electr. & Comput. Eng., Portland State Univ., OH, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","446","457","Memory latency tolerant architectures support thousands of in-flight instructions without scaling cycle-critical processor resources, and thousands of useful instructions can complete in parallel with a miss to memory. These architectures however require large queues to track all loads and stores executed while a miss is pending. Hierarchical designs alleviate cycle time impact of these structures but the CAM and search functions required to enforce memory ordering and provide data forwarding place high demand on area and power. We present new load-store processing algorithms for latency tolerant architectures. We augment primary load and store queues with secondary buffers. The secondary load buffer is a set associative structure, similar to a cache. The secondary store buffer, the Store Redo Log, is a first-in first-out structure recording the program order of all stores completed in parallel with a miss, and has no CAM and search functions. Instead of the secondary store queue, a cache provides temporary forwarding. The SRL enforces memory ordering by ensuring memory updates occur in program order once the miss returns. The new algorithms eliminate the CAM and search functions in the secondary load and store buffers, and remove fundamental sources of complexity, power, and area inefficiency in load/store processing. The new organization, while being area and power efficient, is competitive in performance compared to hierarchical designs.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431577","","Buffer storage;CADCAM;Computer aided manufacturing;Computer architecture;Delay;Memory architecture;Pipelines;Processor scheduling;Proposals;Registers","cache storage;computer aided manufacturing;instruction sets;memory architecture;parallel processing;program processors;queueing theory","CAM;Store Redo Log;cycle-critical processor resources;data forwarding;latency tolerant processors;load-store processing algorithms;memory latency tolerant architectures;memory ordering;search functions;secondary load buffer;secondary store queue;set associative structure","","17","1","17","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Program Chairs' message","","","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","x","xv","Presents the welcome message from the conference proceedings.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431538","","","","","","0","","","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"An ultra low power system architecture for sensor network applications","Hempstead, M.; Tripathi, N.; Mauro, P.; Gu-Yeon Wei; Brooks, D.","Div. of Eng. & Appl. Sci., Harvard Univ., Cambridge, MA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","208","219","Recent years have seen a burgeoning interest in embedded wireless sensor networks with applications ranging from habitat monitoring to medical applications. Wireless sensor networks have several important attributes that require special attention to device design. These include the need for inexpensive, long-lasting, highly reliable devices coupled with very low performance requirements. Ultimately, the ""holy grail"" of this design space is a truly untethered device that operates off of energy scavenged from the ambient environment. In this paper, we describe an application-driven approach to the architectural design and implementation of a wireless sensor device that recognizes the event-driven nature of many sensor-network workloads. We have developed a full-system simulator for our sensor node design to verify and explore our architecture. Our simulation results suggest one to two orders of magnitude reduction in power dissipation over existing commodity-based systems for an important class of sensor network applications. We are currently in the implementation stage of design, and plan to tape out the first version of our system within the next year.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431558","","Biomedical monitoring;Central Processing Unit;Computer architecture;Energy consumption;Microcontrollers;Power systems;Sensor phenomena and characterization;Sensor systems;Sensor systems and applications;Wireless sensor networks","wireless sensor networks","commodity-based system;full-system simulator;habitat monitoring;low power system architecture;magnitude reduction;medical application;power dissipation;sensor node design;wireless sensor network","","34","1","28","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Author index","","","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","556","557","The author index contains an entry for each author and coauthor included in the proceedings record.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.15","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431586","","","","","","0","","","","","8-8 June 2005","","IEEE","IEEE Conference Publications"
"Improving multiprocessor performance with coarse-grain coherence tracking","Cantin, J.F.; Lipasti, M.H.; Smith, J.E.","Dept. of Electr. & Comput. Eng., Wisconsin Univ., Madison, WI, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","246","257","To maintain coherence in conventional shared-memory multiprocessor systems, processors first check other processors' caches before obtaining data from memory. This coherence checking adds latency to memory requests and leads to large amounts of interconnect traffic in broadcast-based systems. Our results for a set of commercial, scientific and multiprogrammed workloads show that on average 67% (and up to 94%) of broadcasts are unnecessary. Coarse-grain coherence tracking is a new technique that supplements a conventional coherence mechanism and optimizes the performance of coherence enforcement. The coarse-grain coherence mechanism monitors the coherence status of large regions of memory, and uses that information to avoid unnecessary broadcasts. Coarse-grain coherence tracking is shown to eliminate 55-97% of the unnecessary broadcasts, and improve performance by 8.8% on average (and up to 21.7%).","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.31","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431561","","Computer architecture;Network interfaces","broadcasting;cache storage;multiprocessor interconnection networks;shared memory systems","broadcast-based systems;coarse-grain coherence tracking;coherence enforcement;interconnect traffic;multiprogrammed workloads;shared-memory multiprocessor systems","","24","6","28","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"The V-Way cache: demand-based associativity via global replacement","Qureshi, M.K.; Thompson, D.; Patt, Y.N.","Dept. of Electr. & Comput. Eng., Univ. of Texas at Austin, TX, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","544","555","As processor speeds increase and memory latency becomes more critical, intelligent design and management of secondary caches becomes increasingly important. The efficiency of current set-associative caches is reduced because programs exhibit a non-uniform distribution of memory accesses across different cache sets. We propose a technique to vary the associativity of a cache on a per-set basis in response to the demands of the program. By increasing the number of tag-store entries relative to the number of data lines, we achieve the performance benefit of global replacement while maintaining the constant hit latency of a set-associative cache. The proposed variable-way, or V-Way, set-associative cache achieves an average miss rate reduction of 13% on sixteen benchmarks from the SPEC CPU2000 suite. This translates into an average IPC improvement of 8%.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.52","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431585","","Costs;Delay;Energy consumption;Engineering management;Hardware;History;Memory management;Microprocessors;Optimized production technology;Upper bound","cache storage;digital storage;memory architecture","IPC improvement;SPEC CPU2000 suite;V-Way cache;constant hit latency;demand-based associativity;global replacement;intelligent design;memory access;memory latency;miss rate reduction;processor speed;secondary cache management;set-associative cache;tag-store entry;variable-way","","11","1","19","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"Energy optimization of subthreshold-voltage sensor network processors","Nazhandali, L.; Bo Zhai; Olson, J.; Reeves, A.; Minuth, M.; Helfand, R.; Pant, S.; Austin, T.; Blaauw, D.","Michigan Univ., USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","197","207","Sensor network processors and their applications are a growing area of focus in computer system research and design. Inherent to this design space is a reduced processing performance requirement and extremely high energy constraints, such that sensor network processors must execute low-performance tasks for long durations on small energy supplies. In this paper, we demonstrate that subthreshold-voltage circuit design (400 mV and below) lends itself well to the performance and energy demands of sensor network processors. Moreover, we show that the landscape for microarchitectural energy optimization dramatically changes in the subthreshold domain. The dominance of leakage power in the subthreshold regime demands architectures that i) reduce overall area; ii) increase the utility of transistors; while iii) maintaining acceptable CPI efficiency. We confirm these observations by performing SPICE-level analysis of 21 sensor network processors and memory architectures. Our best sensor platform, implemented in 130nm CMOS and operating at 235 mV, only consumes 1.38 pJ/instruction, nearly an order of magnitude less energy than previously published sensor network processor results. This design, accompanied by bulk-silicon solar cells for energy scavenging, has been manufactured by IBM and is currently being tested.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431557","","Application software;CMOS process;Circuit synthesis;Computer networks;Manufacturing;Memory architecture;Microarchitecture;Performance analysis;Photovoltaic cells;Sensor systems and applications","CMOS integrated circuits;SPICE;memory architecture;microprocessor chips;optimisation;program processors;sensor fusion;solar cells","130 nm;235 mV;CMOS;CPI efficiency;SPICE-level analysis;bulk-silicon solar cell;leakage power;memory architecture;microarchitectural energy optimization;sensor network processor;subthreshold-voltage circuit design","","30","2","17","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"RegionScout: exploiting coarse grain sharing in snoop-based coherence","Moshovos, A.","Electr. & Comput. Eng., Toronto Univ., Ont., Canada","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","234","245","It has been shown that many requests miss in all remote nodes in shared memory multiprocessors. We are motivated by the observation that this behavior extends to much coarser grain areas of memory. We define a region to be a continuous, aligned memory area whose size is a power of two and observe that many requests find that no other node caches a block in the same region even for regions as large as 16K bytes. We propose RegionScout, a family of simple filter mechanisms that dynamically detect most non-shared regions. A node with a RegionScout filter can determine in advance that a request will miss in all remote nodes. RegionScout filters are implemented as a layered extension over existing snoop-based coherence systems. They require no changes to existing coherence protocols or caches and impose no constraints on what can be cached simultaneously. Their operation is completely transparent to software and the operating system. RegionScout filters require little additional storage and a single additional global signal. These characteristics are made possible by utilizing imprecise information about the regions cached in each node. Since they rely on dynamically collected information RegionScout filters can adapt to changing sharing patterns. We present two applications of RegionScout: In the first RegionScout is used to avoid broadcasts for non-shared regions thus reducing bandwidth. In the second RegionScout is used to avoid snoop induced tag lookups thus reducing energy.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431560","","Bandwidth;Broadcasting;Costs;Electronic mail;File servers;Information filtering;Information filters;Multimedia databases;Operating systems;Protocols","cache storage;protocols;shared memory systems","RegionScout filter;coarse grain sharing;coherence protocols;node caches;shared memory multiprocessors;snoop-based coherence","","35","23","34","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"A high throughput string matching architecture for intrusion detection and prevention","Lin Tan; Sherwood, T.","Dept. of Comput. Sci., California Univ., Santa Barbara, CA, USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","112","122","Network intrusion detection and prevention systems have emerged as one of the most effective ways of providing security to those connected to the network, and at the heart of almost every modern intrusion detection system is a string matching algorithm. String matching is one of the most critical elements because it allows for the system to make decisions based not just on the headers, but the actual content flowing through the network. Unfortunately, checking every byte of every packet to see if it matches one of a set of ten thousand strings becomes a computationally intensive task as network speeds grow into the tens, and eventually hundreds, of gigabits/second. To keep up with these speeds a specialized device is required, one that can maintain tight bounds on worst case performance, that can be updated with new rules without interrupting operation, and one that is efficient enough that it could be included on chip with existing network chips or even into wireless devices. We have developed an approach that relies on a special purpose architecture that executes novel string matching algorithms specially optimized for implementation in our design. We show how the problem can be solved by converting the large database of strings into many tiny state machines, each of which searches for a portion of the rules and a portion of the bits of each rule. Through the careful co-design and optimization of our architecture with a new string matching algorithm we show that it is possible to build a system that is 10 times more efficient than the currently best known approaches.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.5","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431550","","Computer architecture;Computer networks;Computer science;Computer security;Databases;Heart;Intrusion detection;Network-on-a-chip;Protection;Throughput","computer architecture;optimisation;security of data;string matching;telecommunication security","network chips;network intrusion detection;network prevention system;network security;network speed;string matching architecture;wireless device","","15","27","31","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
"RENO: a rename-based instruction optimizer","Petric, V.; Sha, T.; Roth, A.","Dept. of Comput. & Inf. Sci., Pennsylvania Univ., USA","Computer Architecture, 2005. ISCA '05. Proceedings. 32nd International Symposium on","20050620","2005","","","98","109","RENO is a modified MIPS R10000 register renamer that uses map-table ""short-circuiting"" to implement dynamic versions of several well-known static optimizations: move elimination, common subexpression elimination, register allocation, and constant folding. Because it implements these optimizations dynamically, RENO can apply optimizations in certain situations where static compilers cannot. Cycle-level simulation shows that RENO dynamically eliminates (i.e. optimizes away) 22% of the dynamic instructions in both SPECint2000 and MediaBench. RENO<sub>CF</sub> is responsible for 12% and 17% of the eliminations, respectively. Because dataflow dependences are collapsed around eliminated instructions, performance improves by 8% and 13%, respectively. Alternatively, because eliminated instructions do not consume issue queue entries, physical registers, or issue, bypass, register file, and execution bandwidth, RENO can be used to absorb the performance impact of a significantly scaled-down execution core.","1063-6897","0-7695-2270-X","","10.1109/ISCA.2005.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431549","","","data flow computing;instruction sets;optimisation;optimising compilers;storage allocation","MIPS R10000 register renamer;MediaBench;RENO;SPECint2000;common subexpression elimination;constant folding;cycle-level simulation;move elimination;register allocation;rename-based instruction optimizer;static optimization","","10","2","27","","","4-8 June 2005","","IEEE","IEEE Conference Publications"
