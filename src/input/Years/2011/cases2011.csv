"http://ieeexplore.ieee.org/search/searchresult.jsp?bulkSetSize=2000&queryText%3DCompilers%2C+Architectures+and+Synthesis+for+Embedded+Systems+.LB.CASES.RB.%2C+2011",2015/06/23 15:20:58
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN","ISBN","EISBN","DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Enabling Parametric Feasibility Analysis in real-time calculus driven performance evaluation","Simalatsar, A.; Ramadian, Y.; Passerone, R.; Lampka, K.; Perathoner, S.; Thiele, L.","EPFL, Lausanne, Switzerland","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","155","164","This paper advocates a rigorously formal and compositional style for obtaining key performance and/or interface metrics of systems with real-time constraints. We propose a hierarchical approach that couples the independent and different by nature frameworks of Modular Performance Analysis with Real-time Calculus (MPARTC) and Parametric Feasibility Analysis (PFA). Recent work on Real-time Calculus (RTC) has established an embedding of state-based component models into RTC-driven performance analysis for dealing with more expressive component models. However, with the obtained analysis infrastructure it is possible to analyze components only for a fixed set of parameters, e. g., fixed CPU speeds, fixed buffer sizes etc., such that a big space of parameters remains unstudied. In this paper, we overcome this limitation by integrating the method of parametric feasibility analysis in an RTC-based modeling environment. Using the PFA tool-flow, we are able to find regions for component parameters that maintain feasibility and worst-case properties. As a result, the proposed analysis infrastructure produces a broader range of valid design candidates, and allows the designer to reason about the system robustness.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062041","Feasibility areas;System-level Design;Tool integration;Worst-case Analysis","Automata;Clocks;Gold;Synchronization","formal specification;object-oriented programming;process algebra;program diagnostics;reasoning about programs;software metrics;software performance evaluation","CPU speed;PFA tool-flow;RTC-based modeling environment;analysis infrastructure;buffer size;component analysis;compositional;interface metrics;modular performance analysis with real-time calculus;parametric feasibility analysis;real-time calculus driven performance evaluation;real-time constraint;state-based component models;system robustness","","1","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"A novel thread scheduler design for polymorphic embedded systems","Krishnamurthy, V.; Ponpandi, S.D.; Tyagi, A.","Dept. of Comput. Sci., Iowa State Univ., Ames, IA, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","75","84","The complexity of current day embedded systems is steadily on the rise due to innovative content consumption applications. Embedded systems have to be adaptable and scalable to meet the unique resource demands of such applications to deliver satisfactory performance. Effective sharing of system resources by content consumption applications is imperative for user satisfaction. In this paper, we address the challenge of coming up with a design for an efficient scheduler for Multiple Application and Multi-threaded polymorphic embedded system with user satisfaction as its objective function. Randomly generated application graphs serve as benchmarks to evaluate the performance of the proposed polymorphic scheduler framework. We also discuss the impact of our scheduler on the user satisfaction of a multimedia application as a real world case study. The performance enhancements obtained using the proposed greedy polymorphic thread scheduling algorithm are demonstrated by comparison against classical approaches such as First Come First Serve(FCFS) and priority scheduling schemes.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062033","Morphisms;Sigmoid;User satisfaction","Computer architecture;Embedded systems;Hardware;Instruction sets;Measurement;Throughput","embedded systems;multi-threading;multimedia systems;scheduling;user interfaces","content consumption application;first come first serve scheduling scheme;multimedia application;multiple application embedded system;multithreaded embedded system;polymorphic embedded system;priority scheduling scheme;randomly generated application graph;thread scheduler design;user satisfaction","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"A unified approach to eliminate memory accesses early","Islam, M.M.; Stenstrom, P.","Software Platforms, Mechatron., Volvo Technol. Corp., Sweden","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","55","64","This paper introduces the notion of silent loads to classify load accesses that can be satisfied by already available values of the physical register file and proposes a new architectural concept to exploit such loads. The paper then unifies different approaches of eliminating memory accesses early by contributing with a new architectural scheme. We show that our unified approach covers previously proposed techniques of exploiting forwarded and small-value loads in addition to silent loads. Forwarded loads obtain values through load-to-load and store-to-load forwarding whereas small-value loads return small values that can be coded with 8 bits or less. We find that 22%, 31% and 24% of all dynamic loads are forwarded, small-value and silent, respectively. We demonstrate that the prevalence of such loads is mostly inherent in applications. We establish that a hypothetical scheme that encompasses all the categories can eliminate as many as 42% of all dynamic loads and about 18% of all committed stores. Finally, we contribute with a new architectural technique to implement the unified scheme. We show that our proposed scheme reduces execution time to provide noticeable speedup and reduces overall energy dissipation with very low area overhead.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062031","Forwarded Load;Silent Load;Silent Store;Small-Value Load","Art;Memory management;Pipelines;Program processors;Radio frequency;Registers;Static VAr compensators","memory architecture;power aware computing;storage management","architectural scheme;dynamic loads;early memory access elimination;energy dissipation reduction;forwarded loads;load access classification;load-to-load forwarding;physical register file;silent loads;small-value loads;store-to-load forwarding;unified scheme","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Realizing near-true voltage scaling in variation-sensitive L1 caches via fault buffers","Mahmood, T.; Soontae Kim","Dept. of Comput. Sci., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","85","94","Voltage scaling can be applied to cache memories to reduce their energy consumptions. However, reduced supply voltage to the cache memories increases defective SRAM cells due to process variations, which will decrease their yields and performance nullifying the benefits of voltage scaling. To mitigate this problem, we propose a fault buffer-based scheme for L1 caches. Faults are identified and isolated at the granularity of individual words in the L1 caches. Actively used faulty cache words are allocated in the fault buffers dynamically. The fault buffers are organized as multiple banks for low cost implementation and can be reconfigured dynamically to reflect varying performance demands of programs. This dynamic scheme is shown to be more energy- and area-efficient than, and to be performing comparably to the previously proposed static schemes.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062034","L1 cache;Process variation;Voltage scaling","Arrays;Fault tolerance;Fault tolerant systems;Random access memory;Tagging;Transistors","cache storage;fault tolerant computing;power aware computing","defective SRAM cells;energy consumption;fault buffer based scheme;faulty cache word;near-true voltage scaling;variation sensitive L1 cache memory","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"An efficient heuristic for instruction scheduling on clustered VLIW processors","Xuemeng Zhang; Hui Wu; Jingling Xue","Sch. of Comput. Sci. & Eng., Univ. of New South Wales, Sydney, NSW, Australia","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","35","44","Clustering is a well-known technique for improving the scalability of classical VLIW processors. A clustered VLIW processor consists of multiple clusters, each of which has its own register file and functional units. This paper presents a novel phase coupled priority-based heuristic for scheduling a set of instructions in a basic block on a clustered VLIW processor. Our heuristic converts the instruction scheduling problem into the problem of scheduling a set of instructions with a common deadline. The priority of each instruction vi is the l<sub>max</sub>(v<sub>i</sub>)-successor-tree-consistent deadline which is the upper bound on the latest completion time of vi in any feasible schedule for a relaxed problem where the precedence-latency constraints between vi and all its successors, as well as the resource constraints are considered. We have simulated our heuristic, UAS heuristic and Integrated heuristic on the 808 basic blocks taken from the MediaBench II benchmark suite using six processor models. On average, for the six processor models, our heuristic improves 25%, 25%, 33%, 23%, 26%, 27% over UAS heuristic, respectively, and 15%, 16%, 15%, 9%, 20%, 8% over Integrated heuristic, respectively.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062029","Clustered VLIW Processor;Instruction Scheduling;Inter-cluster Communication Latency;Inter-instructional Latency","Clustering algorithms;Educational institutions;Processor scheduling;Program processors;Registers;Schedules;VLIW","multiprocessing systems;processor scheduling;trees (mathematics)","MediaBench II benchmark suite;clustered VLIW processors;instruction scheduling problem;l<sub>max</sub>(v<sub>i</sub>)-successor-tree-consistent deadline;phase coupled priority-based heuristic;precedence-latency constraints;resource constraints;very long instruction word","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"[Title page]","","","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","c1","c1","The following topics are dealt with: compiling; runtime support; mobile platform; compiler smart; system software; memory architecture; cache reliability; safety; error tolerance; performance evaluation; accelerated computing; embedded computing and multicore computing.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062052","","","cache storage;memory architecture;mobile computing;multiprocessing systems;performance evaluation;program compilers","accelerated computing;cache reliability;compiler smart;compiling;embedded computing;error tolerance;memory architecture;mobile platform;multicore computing;performance evaluation;runtime support;safety;system software","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"System-level modeling and synthesis of flow-based microfluidic biochips","Minhass, W.H.; Pop, P.; Madsen, J.","DTU Inf., Tech. Univ. of Denmark, Lyngby, Denmark","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","225","233","Microfluidic biochips are replacing the conventional biochemical analyzers and are able to integrate the necessary functions for biochemical analysis on-chip. There are several types of microfluidic biochips, each having its advantages and limitations. In this paper we are interested in flow-based biochips, in which the flow of liquid is manipulated using integrated microvalves. By combining several microvalves, more complex units, such as micropumps, switches, mixers, and multiplexers, can be built. Although researchers have proposed significant work on the system-level synthesis of droplet-based biochips, which manipulate droplets on a two-dimensional array of electrodes, no research on system-level synthesis of flow-based biochips has been reported so far. The focus has been on application modeling and component-level simulation. Therefore, for the first time to our knowledge, we propose a system-level modeling and synthesis approach for flow-based biochips. We have developed a topology graph-based model of the biochip architecture, and we have used a sequencing graph to model the biochemical applications. We consider that the architecture of the biochip is given, and we are interested to synthesize an implementation, consisting of the binding of operations in the application to the functional units of the architecture, the scheduling of operations and the routing and scheduling of the fluid flows, such that the application completion time is minimized. We propose a List Scheduling-based heuristic for solving this problem. The proposed heuristic has been evaluated using two real-life case studies and a set of four synthetic benchmarks.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062048","Microfluidics;biochips;modeling;synthesis","Biological system modeling;Mixers;Reservoirs;Routing;Solid modeling;System-on-a-chip;Valves","graph theory;lab-on-a-chip;microfluidics;micropumps;microvalves","biochemical analysis on-chip;biochip architecture;component-level simulation;droplet-based biochips;flow-based microfluidic biochip;fluid flow routing;fluid flow scheduling;integrated microvalves;micropump;mixers;multiplexers;sequencing graph;switches;system-level modeling;system-level synthesis;topology graph-based model","","2","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"FFT-Cache: A Flexible Fault-Tolerant Cache architecture for ultra low voltage operation","BanaiyanMofrad, A.; Homayoun, H.; Dutt, N.","Center for Embedded Comput. Syst., Univ. of California, Irvine, CA, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","95","104","Caches are known to consume a large part of total microprocessor power. Traditionally, voltage scaling has been used to reduce both dynamic and leakage power in caches. However, aggressive voltage reduction causes process-variation-induced failures in cache SRAM arrays, which compromise cache reliability. In this paper, we propose Flexible Fault-Tolerant Cache (FFT-Cache) that uses a flexible defect map to configure its architecture to achieve significant reduction in energy consumption through aggressive voltage scaling, while maintaining high error reliability. FFT-Cache uses a portion of faulty cache blocks as redundancy - using block-level or line-level replication within or between sets - to tolerate other faulty caches lines and blocks. Our configuration algorithm categorizes the cache lines based on degree of conflict of their blocks to reduce the granularity of redundancy replacement. FFT-Cache thereby sacrifices a minimal number of cache lines to avoid impacting performance while tolerating the maximum amount of defects. Our experimental results on SPEC2K benchmarks demonstrate that the operational voltage can be reduced down to 375mV, which achieves up to 80% reduction in dynamic power and up to 48% reduction in leakage power with small performance impact and area overhead.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062035","Fault-tolerant cache;Flexible fault remapping;Low power cache","Circuit faults;Computer architecture;Fault tolerant systems;Frequency division multiplexing;Random access memory;Redundancy","SRAM chips;cache storage;fault tolerant computing;memory architecture;redundancy","FFT cache;block level replication;cache SRAM arrays;cache reliability;configuration algorithm;fault tolerant cache architecture;flexible fault tolerant;line level replication;redundancy;voltage scaling","","2","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Selective just-in-time compilation for client-side mobile JavaScript engine","Seong-Won Lee; Soo-Mook Moon","Sch. of Electr. Eng. & Comput. Sci., Seoul Nat. Univ., Seoul, South Korea","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","5","13","Smart phone's full web browsing requires a high-performance JavaScript engine because JavaScript execution takes a non-trivial portion of the loading time for many web sites. The current wisdom of speeding up JavaScript engine is simply turning on its just-in-time compilation (JITC), which compiles JavaScript code to machine code on the fly and executes it instead of interpretation. Unfortunately, we found that JITC actually increases the loading time tangibly for some JavaScript-heavy web pages compared to interpretation, while it can still reduce the running time for JavaScript benchmarks. We observed that the web page JavaScript behaves differently from the benchmark JavaScript in the sense that hot spots rarely exist. This would lower the reuse ratio of the compiled machine code, making the compilation overhead higher than its benefit. This is especially true for a JavaScript engine which compiles all executed functions at their first invocation, as the SFX engine in the WebKit. In order to overcome this problem, we introduce selective compilation to the SFX engine so as to compile only hot functions detected during interpretation. This reduces the slowdown of the SFX for web page JavaScript, while accelerating JavaScript benchmarks. However, selective compilation for web page JavaScript shows a different behavior from other environment, and we discuss it.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062026","JavaScript engines;SFX;hot spot detection;selective compilation","Benchmark testing;Browsers;Engines;HTML;Loading;Reactive power;Web pages","Java;Web sites;mobile computing;mobile handsets;program compilers","SFX engine;Web browsing;Web sites;WebKit;client side mobile Javascript engine;machine code;selective just-in-time compilation;smart phone","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"CASES keynote: Automatic generation of hardware/software interfaces","Arvind","Comput. Sci. & Artificial Intell. Lab., Massachusetts Inst. of Technol., Cambridge, MA, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","1","1","Specialized hardware is necessary to reduce power consumption in mobile devices. Current design methodologies require an early partitioning of the application, allowing the hardware and software to be developed simultaneously, each adhering to a rigid interface contract. Early specification of detailed interface contracts is difficult and prevents the later migration of functionality across the interface. We address this problem using the Bluespec Codesign Language~(BCL) which permits the designer to specify the hardware-software partition in the source code, allowing the compiler to synthesize efficient software and hardware along with transactors for communication between the partitions. We will present preliminary results generated using our compiler for various hardware-software decompositions of several applications.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062024","Design","Companies;Computer aided software engineering;Computer science;Contracts;Hardware;Laboratories;Software","formal specification;hardware-software codesign;mobile computing;program compilers;source coding","Bluespec codesign language;compiler;hardware-software interfaces;interface contracts specification;mobile devices;source code;transactors","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Architecting processors to allow voltage/reliability tradeoffs","Sartori, J.; Kumar, R.","Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","115","124","Escalating variations in modern CMOS designs have become a threat to Moore's law. While previous works have proposed techniques for tolerating variations by trading reliability for reduced voltage (energy) [10], the benefits of such techniques are limited, because voltage/reliability tradeoffs in conventional processors often introduce more errors than can be gainfully tolerated [14]. Recent work has proposed circuit and design-level optimizations [14, 15] that manipulate the error rate behavior of a design to increase the potential for energy savings from voltage/reliability tradeoffs. In this paper, we investigate whether architectural optimizations can also manipulate error rate behavior to significantly increase the energy savings from voltage/reliability tradeoffs. To this end, we demonstrate how error rate behavior indeed depends on processor architecture, and that architectural optimizations can be used to manipulate the error rate behavior of a processor. We show that architectural optimizations can significantly enhance voltage/reliability tradeoffs, achieving up to 29% additional energy savings for processors that employ Razor-based error resilience.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062037","energy efficiency;error resilience;microarchitecture;timing speculation","Delay;Error analysis;Optimization;Program processors;Registers;Reliability","CMOS integrated circuits;circuit optimisation;computer architecture;error statistics;integrated circuit reliability;microprocessor chips","CMOS design;Moore law;architectural optimization;design-level optimization;energy saving;error rate behavior;processor architectural optimization;razor-based error resilience;voltage-reliability tradeoffs","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Compositional analysis of real-time embedded systems","Phan, L.T.X.; Insup Lee; Sokolsky, O.","Dept. of Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","237","238","This tutorial is concerned with various aspects of component-based design and compositional analysis of real-time embedded systems. It will first give an overview of component-based frameworks and their underlying principles. It will then go in-depth into abstraction methods for real-time components and techniques for computing their optimal interfaces, for both systems implemented on uniprocessor and multiprocessor platforms, as well as extensions to multi-mode systems. Besides theoretical aspects, the tutorial will also present an implementation of the compositional analysis framework on Xen virtualization and a demonstration of the CARTS toolset with several examples seeing the techniques in action. It will also include two case studies highlighting the utility of the framework, including the ARINC-653 avionics software and a smart-phone application. We will conclude the tutorial with a number of open challenges and research opportunities in this domain.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062050","Compositional schedulability analysis;Hierarchical scheduling;Multi-mode systems;Real-time interfaces","Computational modeling;Computers;Educational institutions;Embedded systems;Real time systems;Tutorials","embedded systems;multiprocessing systems;object-oriented programming;processor scheduling;user interfaces;virtualisation","ARINC-653 avionics software;CARTS demonstration;Xen virtualization;a smart-phone application;abstraction methods;component-based design;component-based frameworks;compositional analysis framework;multimode systems;multiprocessor platforms;optimal interfaces;real-time components;real-time embedded systems;underlying principles;uniprocessor platforms","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Smart cache cleaning: Energy efficient vulnerability reduction in embedded processors","Jeyapaul, R.; Shrivastava, A.","Compiler Microarchit. Lab., Arizona State Univ., Tempe, AZ, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","105","114","Incessant and rapid technology scaling has brought us to a point where todays, and future transistors are susceptible to transient errors induced by energy carrying particles, called soft errors. Within a processor, the sheer size and nature of data in the caches render it most vulnerable to electrical interferences on static data in the cache. Data in the cache is vulnerable to corruption by soft errors, for the time it remains in the cache. Write-through and early-write-back [17] cache configurations reduce the time for vulnerable data in the cache, at the cost of increased memory writes and therefore energy. We propose a smart cache cleaning methodology, that enables copying of only specific vulnerable cache blocks into the memory at chosen times, thereby ensuring data cache protection with minimal memory writes. Our experiments over LINPACK and Livermore benchmarks demonstrate 26% reduced energy-vulnerability product compared to that of hardware cache configurations.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062036","cache write-back;energy efficient;hybrid technique;smart cache architecture;soft error;vulnerability","Arrays;Cleaning;Embedded systems;Hardware;Instruments;Program processors;Reliability","cache storage","data cache protection;early-write-back cache configuration;embedded processor;energy efficient vulnerability reduction;smart cache cleaning;soft error;write-through cache configuration","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Graph-coloring and treescan register allocation using repairing","Colombet, Q.; Boissinot, B.; Brisk, P.; Hack, S.; Rastello, F.","LIP, INRIA, Lyon, France","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","45","54","Graph coloring and linear scan are two appealing techniques for register allocation as the underlying formalism are extremely clean and simple. This paper advocates a decoupled approach that first lowers the register pressure by spilling variables, and then performs live ranges splitting/coalescing /coloring in a separate phase; this enables the design of simpler, cleaner, and more efficient register allocators. This paper gives a new and more general approach to deal with register constraints. This approach called repairing does not require pre live range splitting and does not introduce additional spill code. It ignores register constraints during coloring/coalescing, and repairs violated constraints afterwards. We applied this method to both graph based and scan based allocators into a decoupled approach. Here, the Iterated Register Coalescer (IRC) and a scan algorithm that uses Static Single Assignment (SSA) properties, the trees can. Moreover, this paper provides a survey on existing and new techniques of bias coloring during scan approaches. Our experimental evaluation shows for the graph based approach, that we reduced the number of vertices (edges) in the interference graph by 26% (33%) without compromising the quality of the generated code. The treescan algorithm improved the compile time of the allocation process by 6.97× over IRC while providing comparable results for the quality of the generated code.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062030","Coalescing;Coloring;Fast register allocation;Register constraints;SSA form","Algorithm design and analysis;Color;Interference;Maintenance engineering;Registers;Resource management;Round robin","graph colouring;optimising compilers","bias coloring technique;graph coloring;interference graph;iterated register coalescer;linear scan;range coalescing;range coloring;range splitting;repairing approach;scan algorithm;static single assignment property;treescan algorithm;treescan register allocation","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Studying optimal spilling in the light of SSA","Colombet, Q.; Brandner, F.; Darte, A.","INRIA, ENS-Lyon, Lyon, France","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","25","34","Recent developments in register allocation, mostly linked to static single assignment (SSA) form, have shown that it is possible to decouple the problem in two successive phases: a first spilling phase places load and store instructions so that the register pressure at all program points is small enough, a second assignment and coalescing phase maps the remaining variables to physical registers and reduces the number of move instructions among registers. This paper focuses on the first phase, for which many open questions remain: in particular, we study the notion of optimal spilling (what can be expressed?) and the impact of SSA form (does it help?). To identify the important features for optimal spilling on load-store architectures, we develop a new integer linear programming formulation, more accurate and expressive than previous approaches. Among other features, we can express SSA φ-functions, memory-to-memory copies, and the fact that a value can be stored simultaneously in a register and in memory. Based on this formulation, we present a thorough analysis of the results obtained for the SPECINT 2000 and EEMBC 1.1 benchmarks, from which we draw, among others, the following conclusions: a) rematerialization is extremely important, b) SSA complicates the formulation of optimal spilling, especially because of memory coalescing when the code is not in CSSA, c) micro-architectural features are significant and thus have to be accounted for, d) significant savings can be obtained in terms of static spill costs, cache miss rates, and dynamic instruction counts.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062028","Algorithms;Experimentation;Performance;Theory","Force;Load modeling;Maintenance engineering;Memory management;Optimization;Registers;Resource management","cache storage;instruction sets;integer programming;linear programming;optimising compilers;program diagnostics;storage management","CSSA;EEMBC 1.1 benchmarks;SPECINT 2000 benchmarks;SSA φ-functions;SSA form;cache miss rates;coalescing phase maps;dynamic instruction counts;integer linear programming formulation;load-store architectures;memory coalescing;memory-to-memory copy;microarchitectural features;optimal spilling;physical registers;register allocation;second assignment;spilling phase;static single assignment form;static spill costs","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Vector class on Limited Local Memory (LLM) multi-core processors","Ke Bai; Di Lu; Shrivastava, A.","Compiler Microarchit. Lab., Arizona State Univ., Tempe, AZ, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","215","224","Limited Local Memory (LLM) multi-core architecture is a promising solution for scalable memory hierarchy. LLM architecture, e.g., IBM Cell/B.E. is a purely distributed memory architecture in which each core can directly access only its small local memory, and that is why it is extremely power-efficient. Vector is a popular container class in the C++ Standard Template Library (STL), which provides the functionality similar to a dynamic array. Due to the small non-virtualized memory in the LLM architecture, vector library implementation cannot be used as it is. In this paper, we propose and implement a scheme to manage vector class in the local memory present in each core of LLM multi-core architecture. Our scalable solution can transparently maintain vector data between the shared global memory and the local memories. In addition, different data transfer granularities are provided by our vector class to achieve better performance. We also propose a mechanism to ensure the validity of pointers-to-elements when the vector elements are moved into the global memory. Experimental result shows that our vector class can improve the programmability of vector class significantly while the overhead can be contained within 7%.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062047","IBM Cell;MPI;PS3;Vector;embedded system;local memory;multi-core processor;scratch pad memory","Containers;Instruction sets;Memory management;Message systems;Multicore processing;Vectors","C++ language;distributed shared memory systems;electronic data interchange;memory architecture","C+ +;LLM architecture;data transfer;distributed memory architecture;dynamic array;limited local memory;multicore architecture;multicore processors;non virtualized memory;scalable memory hierarchy;shared global memory;standard template library;vector class","","1","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Low-overhead virtualization of mobile platforms","Heiser, Gernot","NICTA, Univ. of New South Wales, Sydney, NSW, Australia","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","3","3","Summary form only given. Mobile platforms are becoming as powerful as PCs were not too long ago. The complexity of their software stacks also starts rivalling those of PCs, and increasingly they run operating systems which originated in the desktop world. It should therefore not be too surprising that another phenomenon familiar from the server and desktop world, virtualization, is taking a foothold in mobile platforms. The talk will outline the motivation for using virtualization on mobile devices, especially smartphones. These mostly relate to efficient use and management of hardware resources, cost and security issues. We will also discuss the overheads imposed by a high-performance hypervisor.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062025","Virtual machines;hypervisors;processor consolidation;security;virtualization","Mobile communication;Operating systems;Reliability engineering;Security;Servers;Virtual machine monitors","mobile computing;operating systems (computers);security of data;virtual reality","cost issue;hardware resource management;high performance hypervisor;low overhead visualization;mobile device virtualization;mobile platform;operating system;security issue;server desktop world;server world;smartphone;software stack complexity","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"An evaluation of different modeling techniques for iterative compilation","Eunjung Park; Kulkarni, S.; Cavazos, J.","Dept. of Comput. & Inf. Sci., Univ. of Delaware, Newark, DE, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","65","74","Iterative compilation techniques, which involve iterating over different sets of optimizations, have proven useful in helping compilers choose the right set of optimizations for a given program. However, compilers typically have a large number of optimizations to choose from, making it impossible to iterate over a significant fraction of the entire optimization search space. Recent research has proposed to “intelligently” iterate over the optimization search space using predictive methods. In particular, state-the-art methods in iterative compilation use characteristics of the code being optimized to predict good optimization sequences to evaluate. Thus, an important step in developing predictive methods for compilation is deciding how to model the problem of choosing the right optimizations. In this paper, we evaluate three different ways of modeling the problem of choosing the right optimization sequences using machine learning techniques. We evaluate a novel prediction modeling technique, namely a tournament predictor, that is able to effectively predict good optimization sequences. We show that our tournament predictor can outperform current state-of-the-art predictors and the most aggressive setting of the Open64 compiler (-Ofast) on an average by 75% in just 10 iterations over a set of embedded and scientific kernels. Moreover, using our tournament predictor, we achieved on average 10% improvement over -Ofast for a set of MiBench applications.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062032","compiler optimization;iterative compilation;machine learning;regression","Data models;Kernel;Machine learning algorithms;Optimization;Predictive models;Radiation detectors;Training data","formal specification;iterative methods;learning (artificial intelligence);optimising compilers;search problems","-Ofast;MiBench application;Open64 compiler;code optimization;iterative compilation technique;machine learning technique;modeling technique;optimization search space;optimization sequence;predictive method;program optimization;tournament predictor","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Hardware/software architecture for flash memory storage systems","Sang Lyul Min; Eyec Hyun Nam","Sch. of Comput. Sci. & Eng., Seoul Nat. Univ., Seoul, South Korea","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","235","236","This tutorial deals with various hardware/software issues in designing and implementing flash memory storage systems. It will be split into three parts - the first part is on flash memory internals and flash memory management software called the flash translation layer, the second on solid state disks that emulate hard disk drives using flash memory, and finally the third on reliability issues arising from various asynchronous/synchronous faults.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062049","Crash recovery;Flash memory;Flash translation layer;Garbage collection;Solid state disk;Wear-leveling","Computer science;Educational institutions;Embedded systems;Flash memory;Reliability;Tutorials","disc drives;fault diagnosis;flash memories;hard discs;hardware-software codesign;memory architecture;reliability;storage management","asynchronous faults;flash memory internals;flash memory management software;flash memory storage systems;flash translation layer;hard disk drive;hardware-software architecture;reliability issues;solid state disks","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"A hybrid strategy for mapping multiple throughput-constrained applications on MPSoCs","Singh, A.K.; Kumar, A.; Srikanthan, T.","Nanyang Technol. Univ., Singapore, Singapore","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","175","184","Modern embedded systems are based on Multiprocessor-Systems-on-Chip (MPSoCs) to meet the strict timing deadlines of multiple applications. MPSoC resources must be utilized efficiently by mapping the applications in throughput-aware manner in order to meet throughput constraints for each of them. A design-time methodology is applicable only to predefined set of applications with static behavior, which is incapable of handling dynamism in applications. On the other hand, a run-time approach can cater to the dynamism but cannot provide timing guarantees for all the applications due to large computation requirements at run-time. This paper presents a hybrid flow which performs compute intensive analysis at design-time to derive multiple resource-throughput trade-off points and selects one of these at runtime subject to available resources and desired throughput. Experimental results show that the design-time analysis is faster by 39%, provides better trade-off points and the runtime mapping is speeded up by 93% when compared to state-of-the-art techniques.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062043","Multiprocessor;Synchronous Dataflow;design-time analysis;run-time mapping;throughput","Bandwidth;Databases;Decoding;Optimization;Throughput;Tiles;Timing","microprocessor chips;system-on-chip","MPSoC resources;design-time methodology;hybrid flow;multiple resource-throughput trade-off points;multiple throughput-constrained application mapping;multiprocessor-systems-on-chip;run-time approach;static behavior","","1","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"A method-based ahead-of-time compiler for Android applications","Chih-Sheng Wang; Perez, G.A.; Yeh-Ching Chung; Wei-Chung Hsu; Wei-Kuan Shih; Hong-Rong Hsu","Nat. Tsing Hua Univ., Hsinchu, Taiwan","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","15","24","The execution environment of Android system is based on a virtual machine called Dalvik virtual machine (DVM) in which the execution of an application program is in interpret-mode. To reduce the interpretation overhead of DVM, Google has included a trace-based just-in-time compiler (JITC) in the latest version of Android. Due to limited resources and the requirement for reasonable response time, the JITC is unable to apply deep optimizations to generate high quality code. In this paper, we propose a method-based ahead-of-time compiler (AOTC), called Icing, to speed up the execution of Android applications without the modification of any components of Android framework. The main idea of Icing is to convert the hot methods of an application program from DEX code to C code and uses the GCC compiler to translate the C code to the corresponding native code. With the Java Native Interface (JNI) library, the translated native code can be called by DVM. Both AOTC and JITC have their strength and weakness. In order to combine the strength and avoid the weakness of AOTC and JITC, in Icing, we have proposed a cost model to determine whether a method should be handled by AOTC or JITC during profiling. To evaluate the performance of Icing, four benchmarks used by Google JITC are used as test cases. The performance results show that, with Icing, the execution time of an application is two to three times faster than that without JITC, and 25% to 110% faster than that with JITC.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062027","Ahead-of-time compiler;Android;Dalvik bytecode;just-in-time compiler;reverse engineering;static profiling","Androids;Google;Humanoid robots;Java;Libraries;Optimization;Registers","Java;mobile computing;program compilers;virtual machines","AOTC;Android applications;C code;DEX code;Dalvik virtual machine;GCC compiler;Google JITC;Icing;Java native interface library;cost model;method-based ahead-of-time compiler;trace-based just-in-time compiler","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"WCET-driven branch prediction aware code positioning","Plazar, S.; Kleinsorge, J.; Marwedel, P.; Falk, H.","Comput. Sci. 12, Tech. Univ. Dortmund, Dortmund, Germany","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","165","174","In the past decades, embedded system designers moved from simple, predictable system designs towards complex systems equipped with caches, branch prediction units and speculative execution. This step was necessary in order to fulfill increasing requirements on computational power. Static analysis techniques considering such speculative units had to be developed to allow the estimation of an upper bound of the execution time of a program. This bound is called worst-case execution time (WCET). Its knowledge is crucial to verify whether hard real-time systems satisfy their timing constraints, and the WCET is a key parameter for the design of embedded systems. In this paper, we propose a WCET-driven branch prediction aware optimization which reorders basic blocks of a function in order to reduce the amount of jump instructions and mispredicted branches. We employed a genetic algorithm which rearranges basic blocks in order to decrease the WCET of a program. This enables a first estimation of the possible optimization potential at the cost of high optimization runtimes. To avoid time consuming repetitive WCET analyses, we developed a new algorithm employing integer-linear programming (ILP). The ILP models the worst-case execution path (WCEP) of a program and takes branch prediction effects into account. This algorithm enables short optimization runtimes at slightly decreased optimization results. In a case study, the genetic algorithm is able to reduce the benchmarks' WCET by up to 24.7% whereas our ILP-based approach is able to decrease the WCET by up to 20.0%.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062042","Branch Prediction;Code Positioning;WCET","Equations;Layout;Mathematical model;Optimization;Pipelines;Prediction algorithms;Timing","embedded systems;genetic algorithms;integer programming;linear programming;software engineering","branch prediction aware code positioning;embedded system design;genetic algorithm;integer-linear programming;static analysis technique;worst case execution time;worst-case execution path","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Evaluation of an accelerator architecture for Speckle Reducing Anisotropic Diffusion","Nilakantan, S.; Annangi, S.; Gulati, N.; Sangaiah, K.; Hempstead, M.","Drexel Univ., Philadelphia, PA, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","185","194","Increasing chip power density has brought application specific accelerator architectures to the forefront as an energy and area efficient solution. While GPGPU systems take advantage of specialized hardware to perform computationally intensive tasks faster than chip multiprocessor (CMP) systems, accelerators are hardware units that are designed to execute a specific application efficiently. Real-time ultrasound imaging applications require the removal of multiplicative noise while maintaining a steady frame-rate, and are good candidates to explore accelerator-based systems. In this paper, we propose and evaluate the architecture of an accelerator designed to improve performance of SRAD image enhancing algorithm. We compare the projected performance of the SRAD accelerator to software implementations on a multi-core CPU and a CPU+GPU system. The proposed architecture achieves higher throughput by eliminating redundant fetches from memory and by storing intermediate data locally. The speedup of the GPU is found to be 3.2× over the CPU, while the accelerator achieved a speedup of 24×. The area efficiency of the GPU and accelerator is up to 1.6× and 370× better than the CPU, respectively. In comparison with the CPU, we find that the energy consumed for operation on a single frame is found to be 1.5× lesser on the GPU and up to 580× lesser on the accelerator.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062044","Accelerator;GPU;Performance;SRAD","Arrays;Clocks;Graphics processing unit;Hardware;Registers;Speckle","computer graphic equipment;coprocessors;diffusion;image denoising;image enhancement;multiprocessing systems;redundancy;speckle;ultrasonic imaging","CPU+GPU system;GPGPU systems;SRAD image enhancing algorithm;accelerator based system;application specific accelerator architecture;chip power density;energy consumption;intermediate data storing;multicore CPU;multiplicative noise removal;real-time ultrasound imaging application;redundant fetches;software implementation;speckle reducing anisotropic diffusion","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"WCET-driven cache-aware code positioning","Falk, H.; Kotthaus, H.","Inst. of Embedded Syst. /Real-Time Syst., Ulm Univ., Ulm, Germany","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","145","154","Code positioning is a well-known compiler optimization aiming at the improvement of the instruction cache behavior. A contiguous mapping of code fragments in memory avoids overlapping of cache sets and thus decreases the number of cache conflict misses. We present a novel cache-aware code positioning optimization driven by worst-case execution time (WCET) information. For this purpose, we introduce a formal cache model based on a conflict graph which is able to capture a broad class of cache architectures. This cache model is combined with a formal WCET timing model, resulting in a cache conflict graph weighted with WCET data. This conflict graph is then exploited by heuristics for code positioning of both basic blocks and entire functions. Code positioning is able to decrease the accumulated cache misses for a total of 18 real-life benchmarks by 15.5% on average for an automotive processor featuring a 2-way set-associative cache. These cache miss reductions translate to average WCET reductions by 6.1%. For direct-mapped caches, even larger savings of 18.8% (cache misses) and 9.0% (WCET) were achieved.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062040","Cache;Cache Miss;Code Positioning;Locality;WCET","Analytical models;Context;Data models;Mathematical model;Optimization;Real time systems;Timing","cache storage;graph theory;optimisation;program compilers","WCET driven cache aware code positioning;cache conflict misses;cache sets;compiler optimization;conflict graph;formal cache model;instruction cache behavior;worst case execution time","","1","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Stochastic computing: Embracing errors in architecture and design of processors and applications","Sartori, J.; Sloan, J.; Kumar, R.","Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","135","144","As device sizes shrink, device-level manufacturing challenges have led to increased variability in physical circuit characteristics. Exponentially increasing circuit density has not only brought about concerns in the reliable manufacturing of circuits, but has also exaggerated variations in dynamic circuit behavior. The resulting uncertainty in performance, power, and reliability imposed by compounding static and dynamic non-determinism threatens to halt the continuation of Moore's law, which has been arguably the primary driving force behind technology and innovation for decades. As the marginal benefits of technology scaling continue to languish, a new vision for stochastic computing has begun to emerge. Rather than hiding variations under expensive guardbands, designers have begun to relax traditional correctness constraints and deliberately expose hardware variability to higher levels of the compute stack, thus tapping into potentially significant performance and energy benefits, while exploiting software and hardware error resilience to tolerate errors. In this paper, we present our vision for design, architecture, compiler, and application-level stochastic computing techniques that embrace errors in order to ensure the continued viability of semiconductor scaling.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062039","error resilience;stochastic computing","Computer architecture;Error analysis;Hardware;Optimization;Program processors;Resilience;Timing","circuit reliability;computer architecture;microprocessor chips;program compilers;stochastic processes","Moore's law;application-level stochastic computing;circuit density;compiler;device-level manufacturing;dynamic circuit behavior;physical circuit characteristic;processor design;semiconductor scaling","","5","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Localizing globals and statics to make C programs thread-safe","Smith, A.R.; Kulkarni, P.A.","Dept. of Electr. Eng. & Comput. Sci., Univ. of Kansas, Lawrence, KS, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","205","214","Challenges emerging from the exponential growth in CPU power dissipation and chip hot spots with increasing clock frequencies have forced manufacturers to employ multicore processors as the ubiquitous platform in all computing domains. Embedded mobile devices are increasingly adopting multicore processors to improve program performance and responsiveness at low power levels. However, harnessing these performance and power benefits requires the construction of parallel programs, a task significantly more complex than writing sequential code. Parallel code development is also made more difficult by differences in the use of several programming language constructs. Therefore, it is critical to provide programmers with tools to ease the formidable task of parallelizing existing sequential code or developing new parallel code for multicore processors. In this work we focus on the use of static and global variables that are commonly employed in C/C++ programs, the languages of choice for developing embedded systems applications. Unprotected use of such variables produces functions that are not thread-safe, thereby preventing the program from being parallelized. Manually eliminating global and static variables from existing sequential code is tedious, time-consuming and highly error-prone. While no good solution to this problem currently exists, researchers have proposed partial mitigation techniques that require massive changes to linkers and runtime systems. In this work we study the characteristics and effects of static and global variables in traditional benchmark programs, and propose, construct, and explore a compiler-based, semi-automatic and interactive technique to handle such variables and generate thread-safe code for parallel programs.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062046","Globals;Thread-safe","Instruction sets;Libraries;Multicore processing;Programming;Runtime;Writing","C++ language;embedded systems;multiprocessing systems;parallel programming;program compilers","CPU power dissipation;chip hot spots;compiler-based interactive technique;compiler-based semiautomatic technique;embedded mobile devices;embedded system application development;global localization;multicore processors;parallel code development;parallel programs;partial mitigation techniques;static localization;thread-safe C++ program","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"Cost-effective safety and fault localization using distributed temporal redundancy","Meyer, B.H.; Calhoun, B.H.; Lach, J.; Skadron, K.","Comput. Sci., Univ. of Virginia, Charlottesville, VA, USA","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","125","134","Cost pressure is driving vendors of safety-critical systems to integrate previously distributed systems. One natural approach we have previous introduced is On-Demand Redundancy (ODR), which allows safety-critical and non-critical tasks, traditionally isolated to limit interference, to execute on shared resources. Our prior work has shown that relaxed dedication (RD), one ODR strategy which allows non-critical tasks (NCTs) to execute on idle critical task resources (CTRs), significantly increases NCT throughput. Unfortunately, there are circumstances under which, in spite of this opportunity, it is difficult to effectively schedule NCTs. In this paper, we introduce distributed temporal redundancy (DTR), which allows critical tasks, which traditionally execute in lockstep, to execute asynchronously. In doing so, DTR increases scheduling flexibility, resulting in systems that achieve much closer to the optimal NCT throughput than with relaxed dedication alone; in one set of experiments, DTR schedules no less 93% of the theoretical NCT cycles across a variety of synthetic benchmarks, outperforming RD by over 11%, on average. Furthermore, by distributing all redundant tasks across different resources, triple-modular redundancy, and therefore fault localization, can be achieved. We demonstrate that this can be accomplished with little additional cost and complexity: in practice, relatively few DTR tasks are in fight simultaneously, limiting the additional buffering needed to support DTR.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062038","Safety-critical;on-demand redundancy;system-level design","Hardware;Interference;Processor scheduling;Redundancy;Schedules;Throughput","benchmark testing;fault tolerant computing;processor scheduling;redundancy;resource allocation","DTR;ODR strategy;cost-effective safety;critical task resources;distributed temporal redundancy;fault localization;noncritical tasks;ondemand redundancy;optimal NCT throughput;relaxed dedication;resource sharing;safety-critical system;scheduling flexibility;synthetic benchmarks;triple-modular redundancy","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"An FPGA-based heterogeneous coarse-grained dynamically reconfigurable architecture","Ferreira, R.; Vendramini, J.G.; Mucida, L.; Pereira, M.M.; Carro, L.","Dept. de Inf., Univ. Fed. de Vicosa, Vicosa, Brazil","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","195","204","Coarse-grained reconfigurable architecture has emerged as a promising model for embedded systems as a solution to reduce the complexity of FPGA synthesis and mapping steps, consequently reducing reconfiguration time. Despite these advantages, CGRA usage has been limited due to the lack of commercial CGRA circuits. This work proposes a virtual and dynamic CGRA implemented on top of an FPGA. This approach allows the usage of commercial-off-the-shelf FPGA devices combined with the advantages of CGRAs. The proposed architecture consists of a set of heterogeneous functional units (FU) and a global interconnection network. The global network allows any FU to be used at each cycle, which reduces significantly the placement complexity. In addition, we introduce a polynomial mapping algorithm which includes scheduling, placement and routing steps (SPR). Moreover, the proposed approach performs a very fast placement and routing in comparison to similar CGRA approaches. The three SPR steps are computed in few milliseconds. The feasibility of this approach is demonstrated for a suite of digital signal processing benchmarks.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062045","CGRA;FPGA;Interconnections;Multistage;Placement;Reconfigurable Architectures;Routing;Scheduling","Adders;Complexity theory;Computer architecture;Field programmable gate arrays;Processor scheduling;Registers;Routing","circuit complexity;embedded systems;field programmable gate arrays;network routing;reconfigurable architectures","CGRA circuits;CGRA usage;FPGA synthesis;FPGA-based heterogeneous reconfigurable architecture;coarse-grained dynamically reconfigurable architecture;commercial-off-the-shelf FPGA devices;digital signal processing benchmarks;embedded systems;global interconnection network;heterogeneous functional units;mapping steps;placement complexity reduction;polynomial mapping algorithm;scheduling-placement-and-routing steps","","3","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"CASES'11 author index","","","Compilers, Architectures and Synthesis for Embedded Systems (CASES), 2011 Proceedings of the 14th International Conference on","20111027","2011","","","1","4","Presents an index of the authors whose papers are published in the conference.","","978-1-4503-0713-0","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062051","","","","","","0","","","","","9-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"HW-SW implementation of a decoupled FPU for ARM-based Cortex-M1 SoCs in FPGAs","Joven, J.; Strict, P.; Castells-Rufas, D.; Bagdia, A.; De Micheli, G.; Carrabina, J.","LSI, EPFL, Lausanne, Switzerland","Industrial Embedded Systems (SIES), 2011 6th IEEE International Symposium on","20110714","2011","","","1","8","Nowadays industrial monoprocessor and multiprocessor systems make use of hardware floating-point units (FPUs) to provide software acceleration and better precision due to the necessity to compute complex software applications. This paper presents the design of an IEEE-754 compliant FPU, targeted to be used with ARM Cortex-M1 processor on FPGA SoCs. We face the design of an AMBA-based decoupled FPU in order to avoid changing of the Cortex-M1 ARMv6-M architecture and the ARM compiler, but as well to eventually share it among different processors in our Cortex-M1 MPSoC design. Our HW-SW implementation can be easily integrated to enable hardware-assisted floating-point operations transparently from the software application. This work reports synthesis results of our Cortex-M1 SoC architecture, as well as our FPU in Altera and Xilinx FPGAs, which exhibit competitive numbers compared to the equivalent Xilinx FPU IP core. Additionally, single and double precision tests have been performed under different scenarios showing best case speedups between 8.8× and 53.2× depending on the FP operation when are compared to FP software emulation libraries.","","978-1-61284-818-1","978-1-61284-819-8","10.1109/SIES.2011.5953649","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953649","","Computer architecture;Fabrics;Field programmable gate arrays;Hardware;Protocols;Registers;Software","field programmable gate arrays;floating point arithmetic;hardware-software codesign;multiprocessing systems;system-on-chip","AMBA-based decoupled FPU;ARM compiler;ARM-based Cortex-M1 SoCs;Cortex-M1 MPSoC design;FPGAs;HW-SW implementation;IEEE-754 compliant FPU;hardware floating-point units;industrial monoprocessor;multiprocessor systems;software acceleration","","1","","19","","","15-17 June 2011","","IEEE","IEEE Conference Publications"
