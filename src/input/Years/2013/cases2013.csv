"http://ieeexplore.ieee.org/search/searchresult.jsp?bulkSetSize=2000&queryText%3DCompilers%2C+Architectures+and+Synthesis+for+Embedded+Systems+.LB.CASES.RB.%2C+2013",2015/06/23 15:21:21
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN","ISBN","EISBN","DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Platform-dependent code generation for embedded real-time software","BaekGyu Kim; Phan, L.T.X.; Sokolsky, O.; Insup Lee","Univ. of Pennsylvania, Philadelphia, PA, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Code generation for embedded systems is challenging, since the generated code (e.g., C code) is expected to run on a heterogeneous set of target platforms with different characteristics, such as hardware/software architectures and programming interfaces. We propose a code generation framework that provides the flexibility to generate different source code that is executable on each target platform. In our framework, the platform-dependent characteristics of a target platform are explicitly specified by an Architectural Analysis Description Language (AADL) model and a code snippet repository. The AADL model captures hardware/software architectural aspects of the platform, such as periodic/aperiodic threads and their interactions with sensors and actuators. The code snippet repository contains platform-dependent code snippets that are categorized according to the functions required to implement the components of the AADL model. These two elements of the platform capability are then used by the code generation algorithm to generate platform-dependent code for the given platform. We demonstrate the applicability of our framework using a case study of code generation for two infusion pump systems.","","","","10.1109/CASES.2013.6662512","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662512","Code generation, Embedded software, AADL, Model;based development","Hardware;Instruction sets;Message systems;Ports (Computers);Programming;Semantics","embedded systems;hardware-software codesign;program compilers;software architecture;software development management;source coding","AADL model;architectural analysis description language;code snippet repository;embedded real-time software;embedded system;hardware-software architecture;infusion pump system;platform dependent code snippet;source code generation","","2","","13","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Organization","","","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","1","Provides a listing of current committee members and society officers.","","","","10.1109/CASES.2013.6662502","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662502","","","","","","0","","","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"CAeSaR: Unified cluster-assignment scheduling and communication reuse for clustered VLIW processors","Porpodas, V.; Cintra, M.","Sch. of Inf., Univ. of Edinburgh, Edinburgh, UK","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Clustered architectures have been proposed as a solution to the scalability problem of wide ILP processors. VLIW architectures, being wide-issue by design, benefit significantly from clustering. Such architectures, being both statically scheduled and clustered, require specialized code generation techniques, as they require explicit Inter-Cluster Copy instructions (ICCs) be scheduled in the code stream. In this work we propose CAeSaR, a novel instruction scheduling algorithm that improves code generation for such architectures. It combines cluster assignment, instruction scheduling and inter-cluster communication reuse all in one single unified algorithm. The proposed algorithm improves performance by any phase-ordering issues among these three code generation and optimization steps. We evaluate CAeSaR on the MediabenchII and SPEC CINT2000 benchmarks and compare it against the state-of-the-art instruction scheduling algorithm. Our results show an improvement in execution time of up to 20.3%, and 13.8% on average, over the current state-of-the-art across the benchmarks.","","","","10.1109/CASES.2013.6662513","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662513","Cluster Assignment;Clustered VLIW;Instruction Scheduling","Clustering algorithms;Program processors;Registers;Schedules;Scheduling;Scheduling algorithms;VLIW","instruction sets;parallel algorithms;parallel architectures;pattern clustering;processor scheduling;program compilers","CAeSaR;ICCs;ILP processors;MediabenchII;SPEC CINT2000 benchmarks;clustered VLIW processors;clustered architectures;code generation techniques;explicit inter-cluster copy instructions;instruction scheduling algorithm;intercluster communication reuse;unified cluster-assignment scheduling and communication reuse","","0","","37","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Cyber physical systems: Systems engineering of industrial embedded systems — Barriers, enablers and opportunities","Jacobson, C.A.; Schooler, R.; Laurence, M.","","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","3","Cyber physical systems: systems engineering of industrial embedded systems-barriers, enablers and opportunities; high-performance, scalable, general-purpose processors to accelerate high-throughput networking and security applications; Low-power high-performance asynchronous processors.","","","","10.1109/CASES.2013.6662503","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662503","","","embedded systems;multiprocessing systems;security of data;software engineering","cyber physical systems;general-purpose processors;high-performance processors;high-throughput networking;industrial embedded systems;low-power high-performance asynchronous processors;scalable processors;systems engineering","","0","","","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"SPM-Sieve: A framework for assisting data partitioning in scratch pad memory based systems","Chakraborty, P.; Panda, P.R.","Intel Technol. India Pvt. Ltd., Bangalore, India","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Modern system architectures sometimes include scratch pad memories (SPM) in their memory hierarchy to take advantage of their simpler design, in an attempt to meet the system area, performance, and power budget. These systems employing SPM can be broadly categorized as: (a) cacheless systems with only SPM, (b) hybrid systems with both cache and SPM, and (c) reconfigurable systems with the provision to reconfigure local memory as either cache, SPM, or a combination of the two. However SPM based systems have needed larger efforts spent on their programming, mainly due to allocating data and orchestrating data transfers explicitly by soft-ware. Tight product development cycles require faster development and porting of diverse applications to multiple SPM based architectures. In this paper we present SPM-Sieve, a profile-based tool and framework targeted for SPM based architectures that generates partitioning decisions of the first level memory in the system hierarchy, and suggests object mapping amongst the memory partitions without resorting to detailed simulation of all configurations. This is done by natively executing an application and using minimal target architecture specification, which not only provides early information influencing data organization in the application, but also provides a foundation for other more sophisticated algorithms to produce optimized allocations. We demonstrate the utility and generality of SPM-Sieve by evaluating it on a large number of SPEC2000 benchmarks targeted for a 128KB first level memory. We evaluate its effectiveness by performing simulation studies comparing the partition suggested by the tool against varying partition sizes, and observe that its suggestions are very competitive for SPM based architectures with and without caches.","","","","10.1109/CASES.2013.6662527","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662527","Memory Allocation;Scratch Pad Memory;Software Cache","Arrays;Dynamic scheduling;Hardware;Instruments;Resource management;Software","cache storage;memory architecture;pattern classification;reconfigurable architectures;resource allocation;software tools","SPEC2000 benchmark;SPM based architecture;SPM-Sieve;cache memory;cacheless system;data allocation;data partitioning assesment;data transfer orchestration;hybrid system;memory hierarchy;memory partition size;object mapping;product development cycle;profile-based tool;reconfigurable system;scratch pad memory based system","","0","","44","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Hybrid compile and run-time memory management for a 3D-stacked reconfigurable accelerator","Gauthier, L.; Ueno, S.; Inoue, K.","Dept. of Inf., Kyushu Univ., Fukuoka, Japan","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","This paper presents a hybrid compile and run-time memory management technique for a 3D-stacked reconfigurable accelerator including a memory layer composed of multiple memory units whose parallel access allows a very high bandwidth. The technique inserts allocation, free and data transfers into the code for using the memory layer and avoids memory overflows by adding a limited number of additional copies to and from the host memory. When compile-time information is lacking, the technique relies on run-time decisions for controlling these memory operations. Experiments show that, compared to a pessimistic approach, the overhead for avoiding overflows can be cut on average by 27%, 45% and 63% when the size of each memory unit is respectively 1kB, 128kB and 1MB.","","","","10.1109/CASES.2013.6662514","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662514","","Arrays;Data transfer;Libraries;Memory management;Resource management;Tiles","data flow analysis;program compilers;reconfigurable architectures;storage management","3D-stacked reconfigurable accelerator;compile-time information;data transfers;hybrid compile memory management;memory layer;memory overflow avoidance;multiple memory units;parallel access;run-time decisions;run-time memory management","","0","","29","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Tutorial: Methodologies and tools for embedded multisensory systems based on ARM cortex M processors","Zilic, Z.","Dept. ECE, McGill Univ., Montreal, QC, Canada","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","8","Modern embedded systems require design methodologies that are productive, yet manage to harvest continuing innovations in embedded processor and sensing technologies. This course outlines techniques for mastering the design using the new tools and software abstractions based on ARM Cortex M processor family. Methodologies are outlined that enhance productivity in large-scale designs, while also employing the advanced hardware accelerators present in modern embedded processors. We then elaborate the ways to design and use platforms for networked systems that build on multi-sensor integration functions.","","","","10.1109/CASES.2013.6662504","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662504","Embedded Systems;Microcontrollers;Software Design","Computer architecture;Embedded systems;Hardware;Program processors;Reliability;Unified modeling language","embedded systems;microprocessor chips;sensor fusion","ARM Cortex M processor family;advanced hardware accelerators;design methodologies;embedded multisensory systems;large-scale designs;modern embedded processors;modern embedded systems;multisensor integration functions;networked systems;sensing technologies;software abstractions","","0","","11","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Fault detection and recovery efficiency co-optimization through compile-time analysis and runtime adaptation","Hao Chen; Chengmo Yang","Dept. of Electr. & Comput. Eng., Univ. of Delaware, Newark, DE, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","The ever scaling-down feature size and noise margin keep elevating hardware failure rates, requiring the incorporation of fault tolerance into computer systems. One fault tolerance scheme that receives a lot of research attention is redundant execution. However, existing solutions are developed under the assumption that the fault rate is low. These techniques either solely focus on fault detection, or sometimes even increase recovery cost to reduce fault detection overhead. The lack of overall efficiency makes them insufficient and inappropriate for embedded systems with tight energy and cost budget. Our study shows that checkpoint frequency and fault rate are two critical parameters determining the overall fault detection and recovery overhead. To co-optimize detection and recovery, we statically construct a mathematical model, capable of taking application and architecture characteristics into consideration and identifying the optimal checkpoint frequency of an application for a given fault rate. Moreover, as the fault rate is infeasible to predict a priori, we furthermore propose a set of heuristics, enabling the system to dynamically monitor the fault rate and adapt the checkpoint frequency accordingly. The efficacy of the static and the adaptive optimizations is evaluated through detailed instructionlevel simulation. The results show that the optimal checkpoint frequency identified by the static model is very close to the actual value (6% deviation) and the run-time adaptation scheme effectively reduces the overhead caused by the unpredictability in fault rate.","","","","10.1109/CASES.2013.6662528","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662528","","Adaptation models;Checkpointing;Fault detection;Fault tolerant systems;Mathematical model;Registers;Runtime","checkpointing;fault tolerant computing;program compilers;program diagnostics","checkpoint frequency;compile-time analysis;embedded systems;fault detection efficiency co-optimization;fault detection overhead;fault rate;fault recovery efficiency co-optimization;fault recovery overhead;fault tolerance;feature size;instruction-level simulation;noise margin;redundant execution;runtime adaptation","","2","","34","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Simultaneously optimizing DRAM cache hit latency and miss rate via novel set mapping policies","Hameed, F.; Bauer, L.; Henkel, J.","Dept. of Embedded Syst., Karlsruhe Inst. of Technol. (KIT), Karlsruhe, Germany","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Two key parameters that determine the performance of a DRAM cache based multi-core system are DRAM cache hit latency (HL) and DRAM cache miss rate (MR), as they strongly influence the average DRAM cache access latency. Recently proposed DRAM set mapping policies are either optimized for HL or for MR. None of these policies provides a good HL and MR at the same time. This paper presents a novel DRAM set mapping policy that simultaneously targets both parameters with the goal of achieving the best of both to reduce the overall DRAM cache access latency. For a 16-core system, our proposed set mapping policy reduces the average DRAM cache access latency (depends upon HL and MR) compared to state-of-the-art DRAM set mapping policies that are optimized for either HL or MR by 29.3% and 12.1%, respectively.","","","","10.1109/CASES.2013.6662515","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662515","","Arrays;Electric breakdown;Memory management;Multicore processing;Organizations;Random access memory;Round robin","DRAM chips;cache storage;multiprocessing systems;optimisation;performance evaluation","16-core system;DRAM cache based multicore system performance;DRAM cache hit latency;DRAM cache miss rate;DRAM set mapping policies;average DRAM cache access latency;simultaneous DRAM cache HL-MR optimization","","1","","15","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"[Front cover]","","","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","1","Presents the front cover or splash screen of the proceedings record.","","","","10.1109/CASES.2013.6662499","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662499","","","","","","0","","","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Automatic Extraction of pipeline parallelism for embedded heterogeneous multi-core platforms","Cordes, D.; Engel, M.; Neugebauer, O.; Marwedel, P.","Tech. Univ. Dortmund, Dortmund, Germany","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Automatic parallelization of sequential applications is the key for efficient use and optimization of current and future embedded multi-core systems. However, existing approaches often fail to achieve efficient balancing of tasks running on heterogeneous cores of an MPSoC. A reason for this is often insufficient knowledge of the underlying architecture's performance. In this paper, we present a novel parallelization approach for embedded MPSoCs that combines pipeline parallelization for loops with knowledge about different execution times for tasks on cores with different performance properties. Using Integer Linear Programming, an optimal solution with respect to the model used is derived implementing tasks with a well-balanced execution behavior. We evaluate our pipeline parallelization approach for heterogeneous MPSoCs using a set of standard embedded benchmarks and compare it with two existing state-of-the-art approaches. For all benchmarks, our parallelization approach obtains significantly higher speedups than either approach on heterogeneous MPSoCs.","","","","10.1109/CASES.2013.6662508","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662508","Automatic Parallelization;Embedded Software;Heterogeneity;Integer Linear Programming;MPSoC;Pipeline","Benchmark testing;Computer architecture;Integer linear programming;Parallel processing;Pipelines;Program processors","embedded systems;integer programming;linear programming;multiprocessing systems;parallel processing;pipeline processing;resource allocation;system-on-chip","automatic parallelization;embedded heterogeneous multicore platforms;embedded multicore systems;heterogeneous MPSoC;heterogeneous cores;integer linear programming;optimal solution;pipeline parallelism automatic extraction;pipeline parallelization approach;sequential applications;standard embedded benchmarks;task balancing","","0","","24","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Aging-aware hardware-software task partitioning for reliable reconfigurable multiprocessor systems","Das, A.; Kumar, A.; Veeravalli, B.","Dept. of Electr. & Comput. Eng., Nat. Univ. of Singapore, Singapore, Singapore","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Homogeneous multiprocessor systems with reconfigurable area (also known as Reconfigurable Multiprocessor Systems) are emerging as a popular design choice in current and future technology nodes to meet the heterogeneous computing demand of a multitude of applications enabled on these platforms. Application specific mapping decisions on such a platform involve partitioning a given application into software tasks (executed on one or more of the general purpose processors, GPPs) and the hardware tasks (realized as dedicated hardware on the reconfigurable area) to optimize and/or satisfy design constraints such as reliability, performance and design cost. Improving the reliability considering transient faults by increasing the number of checkpoints negatively impacts the reliability considering permanent faults. This trade-off is ignored in all prior studies on task mapping and scheduling. This paper proposes an optimization technique to decide the optimal number of checkpoints for the software tasks which minimizes aging of the GPPs while maximizing the transient fault-tolerance of the overall platform (GPPs and the reconfigurable area) and satisfying design cost and performance. Experiments conducted with synthetic and real-life application task graphs (cyclic and acyclic) demonstrate that the proposed technique minimizes aging and improves the platform lifetime by an average 60% as compared to the existing transient fault-aware techniques. Further, a gradient-based heuristic is proposed to minimize the design space exploration time by upto 500× with less than 5% deviation from optimal solution.","","","","10.1109/CASES.2013.6662505","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662505","","Aging;Fault tolerance;Fault tolerant systems;Mathematical model;Program processors;Transient analysis","checkpointing;fault tolerant computing;gradient methods;graph theory;microprocessor chips;multiprocessing systems;optimisation","GPP;aging-aware hardware-software task partitioning;application specific mapping decisions;application task graphs;checkpoints;design choice;design constraints;design space exploration time;gradient-based heuristic;hardware tasks;optimal checkpoints number;optimization technique;permanent faults;reliable reconfigurable multiprocessor systems;software tasks;transient fault-aware techniques;transient fault-tolerance","","4","","37","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Global property violation detection and diagnosis for wireless sensor networks","Man Wang; Zhiyuan Li","Comput. Sci. Dept., Purdue Univ., West Lafayette, IN, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Run-time error detection and deterministic off-line error replay have received wide attention in recent years as a technique to enhance the programmer's ability to find software errors. To apply this technique to wireless sensor networks (WSN), one must be able to deal with the severe constraint on the memory, the communication bandwidth and the energy source on the sensor motes and the highly dynamic and unpredictable operating environment. All these make it difficult for the application programmer to manually insert operations required for error detection and replay. This paper makes three contributions towards making error detection and replay automatic for WSNs: (i) a domain-specific language, called SensorC, for specifying WSN global properties that must be satisfied when the system and its application software are deployed; (ii) a method to automatically decompose such global properties into a set of local operations to detect global property violations, with the goal to minimize the communication traffic for state information exchanges; and (iii) a new program analysis to identify program sub-traces that can be skipped for replay without losing the accuracy of diagnosis. The proposed techniques, which are implemented in a compiler, are shown by experiments to successfully catch real WSN software errors and to substantially reduce message exchanges for run time error detection.","","","","10.1109/CASES.2013.6662529","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662529","Wireless sensor network;ants;invari;program debugging;property decomposition","Base stations;Message passing;Production;Routing;Routing protocols;Software;Wireless sensor networks","program compilers;program debugging;program diagnostics;wireless sensor networks","SensorC language;WSN software errors;application software;communication bandwidth constraint;communication traffic minimization;compilers;deterministic offline error replay;domain-specific language;energy source constraint;global property violation detection;global property violation diagnosis;local operations;memory constraint;message exchange reduction;operating environment;program analysis;program subtrace identification;run-time error detection;sensor motes;software errors;state information exchanges;wireless sensor networks","","0","","28","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Power-performance modeling on asymmetric multi-cores","Pricopi, M.; Muthukaruppan, T.S.; Venkataramani, V.; Mitra, T.; Vishin, S.","Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Asymmetric multi-core architectures have recently emerged as a promising alternative in a power and thermal constrained environment. They typically integrate cores with different power and performance characteristics, which makes mapping of workloads to appropriate cores a challenging task. Limited number of performance counters and heterogeneous memory hierarchy increase the difficulty in predicting the performance and power consumption across cores in commercial asymmetric multi-core architectures. In this work, we propose a software-based modeling technique that can estimate performance and power consumption of workloads for different core types. We evaluate the accuracy of our technique on ARM big. LITTLE asymmetric multi-core platform.","","","","10.1109/CASES.2013.6662519","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662519","","Analytical models;Estimation;Hardware;Multicore processing;Pipelines;Program processors;Radiation detectors","memory architecture;multiprocessing systems;performance evaluation","LITTLE asymmetric multicore platform;commercial asymmetric multicore architectures;core types;heterogeneous memory hierarchy;performance counters;performance estimation;performance prediction;power constrained environment;power consumption prediction;power-performance modeling;software-based modeling technique;thermal constrained environment;workload mapping","","2","","23","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"From software to accelerators with LegUp high-level synthesis","Canis, A.; Jongsok Choi; Fort, B.; Ruolong Lian; Qijing Huang; Calagar, N.; Gort, M.; Jia Jun Qin; Aldham, M.; Czajkowski, T.; Brown, S.; Anderson, J.","ECE Dept., Univ. of Toronto, Toronto, ON, Canada","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","9","Embedded system designers can achieve energy and performance benefits by using dedicated hardware accelerators. However, implementing custom hardware accelerators for an application can be difficult and time intensive. LegUp is an open-source high-level synthesis framework that simplifies the hardware accelerator design process [8]. With LegUp, a designer can start from an embedded application running on a processor and incrementally migrate portions of the program to hardware accelerators implemented on an FPGA. The final application then executes on an automatically-generated software/hardware coprocessor system. This paper presents on overview of the LegUp design methodology and system architecture, and discusses ongoing work on profiling, hardware/software partitioning, hardware accelerator quality improvements, Pthreads/OpenMP support, visualization tools, and debugging support.","","","","10.1109/CASES.2013.6662524","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662524","FPGA;Hardware Accelerators;High Level Synthesis","Clocks;Field programmable gate arrays;Hardware;Optimization;Pipeline processing;Radiation detectors;Software","coprocessors;data visualisation;embedded systems;field programmable gate arrays;hardware-software codesign;program debugging;public domain software;software tools","FPGA;LegUp design methodology;LegUp high-level synthesis;OpenMP support;Pthreads;automatically-generated software/hardware coprocessor system;custom hardware accelerators;debugging support;dedicated hardware accelerators;embedded application;embedded system designers;energy benefits;hardware accelerator design process;hardware accelerator quality improvements;hardware partitioning;open-source high-level synthesis framework;performance benefits;software partitioning;system architecture;visualization tools","","4","","34","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"[Copyright notice]","","","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","1","","","","","10.1109/CASES.2013.6662500","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662500","","","","","","0","","","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Scrubbing unit repositioning for fast error repair in FPGAs","Nazar, G.L.; Santos, L.P.; Carro, L.","Inst. de Inf., Univ. Fed. do Rio Grande do Sul, Porto Alegre, Brazil","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Field Programmable Gate Arrays (FPGAs) are very successful platforms that rely on large configuration memories to store the circuit functions required by users. Faults affecting such memories are a major dependability threat for these devices, and the applicability of FPGAs on critical systems depends on efficient means to mitigate their effects. The main means to effectively remove such faults, namely configuration scrubbing, consists in rewriting the desired contents of this memory and suffers from high power consumption and a long mean time to repair (MTTR). In this work we propose Scrubbing Unit Repositioning for Fast Error Repair (SURFER), a novel approach to exploit partial dynamic reconfiguration coupled with fine-grained redundancy to greatly reduce the MTTR for FPGAs subject to upsets in their configuration memories.","","","","10.1109/CASES.2013.6662506","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662506","FPGA;Mean Time to Repair;Single Event Upset","Acceleration;Circuit faults;Field programmable gate arrays;Histograms;Maintenance engineering;Memory management;Redundancy","fault tolerant computing;field programmable gate arrays;power aware computing;redundancy","FPGA;MTTR;SURFER;critical systems;fine-grained redundancy;large configuration memories;long mean time to repair;partial dynamic reconfiguration;power consumption;scrubbing unit repositioning for fast error repair","","4","","22","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Effective code discovery for ARM/Thumb mixed ISA binaries in a static binary translator","Jiunn-Yeu Chen; Bor-Yeh Shen; Quan-Huei Ou; Wuu Yang; Wei-Chung Hsu","Nat. Chiao Tung Univ., Hsinchu, Taiwan","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Code discovery has been a main challenge for static binary translation, especially when the source ISA (Instruction Set Architecture) has variable-length instructions, such as the X86 architectures. Due to embedded data such as PC-relative data, jump tables, or paddings in the code section, a binary translator may be misled to translate data as instructions. With variable length instructions, once data is mis-translated as instructions, subsequent decoding of instructions could be wrong. This paper concerns static binary translation for the ARM architectures, which dominate the embedded-system market. Although ARM is considered RISC (Reduced Instruction Set Computing) in many aspects of processors, it does allow the mix of 32-bit instructions (ARM) with 16-bit instructions (Thumb) in the ARM/Thumb mixed executables. Since the instruction lengths of ARM and Thumb are not equal, the locations of the instructions could be 4-byte or 2-byte aligned addresses, respectively. Furthermore, because ARM and Thumb instructions share encoding space, a 4-byte word could be decoded as one ARM instruction or two Thumb instructions. The correct decoding of this 4-byte word is actually determined at run time by the least significant bit of the program counter. For unstripped binaries, mapping symbols can be used to identify ARM code regions and Thumb code regions. However, for stripped binaries, such mapping symbols are not available to assist translation. We have proposed a novel solution to statically translate the stripped executables for the ARM/Thumb mixed ISA. Our static binary translator includes a translation pass which guarantees the correctness of the translated executable by generating multiple versions of translated code for runtime selection. The binary translator also includes a series of optimization analyses which discover and remove most of the code generated in the baseline translation. Based on the SPEC2006 benchmark suite, stripped ARM/Thumb mixed binaries transl- ted by our static binary translator achieve good performance with only 25% of code size increase.","","","","10.1109/CASES.2013.6662525","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662525","Code discovery problem;Reverse engineering;Static binary translation","Computer architecture;Educational institutions;Optimization;Radiation detectors;Reduced instruction set computing;Thumb","embedded systems;instruction sets;program interpreters","16-bit instructions;32-bit instructions;4-byte word;ARM architectures;ARM code regions;ARM-Thumb mixed ISA binaries;RISC;SPEC2006 benchmark suite;Thumb code regions;X86 architectures;code discovery;data translation;embedded data;embedded system market;instruction set architecture;optimization analysis;reduced instruction set computing;runtime selection;static binary translator;translation pass;variable-length instructions","","0","","22","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Bitcoin and the age of Bespoke Silicon","Taylor, M.B.","Univ. of California, San Diego, La Jolla, CA, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Recently, the Bitcoin cryptocurrency has been an international sensation. This paper tells the story of Bitcoin hard-ware: how a group of early-adopters self-organized and financed the creation of an entire new industry, leading to the development of machines, including ASICs, that had orders of magnitude better performance than what Dell, Intel, NVidia, AMD or Xilinx could provide. We examine this story for clues as to how we can foster greater innovation in the semiconductor industry and enable this phenomenon to occur more broadly for more application areas, spawning a new age of hardware innovation tailored to emerging application domains-an Age of Bespoke Silicon.","","","","10.1109/CASES.2013.6662520","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662520","Bitcoin;Dark Silicon;Specialization","Application specific integrated circuits;Data mining;Exchange rates;Field programmable gate arrays;Graphics processing units;Hardware;Technological innovation","cryptography","AMD;ASIC;Dell;Intel;NVidia;Xilinx;bespoke silicon;bitcoin cryptocurrency;bitcoin hardware;hardware innovation;semiconductor industry","","2","","3","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Minimizing code size via page selection optimization on partitioned memory architectures","Yuan Mengting; Xue, C.J.; Chen Yong; Li Qing'an; Yingchao Zhao","Wuhan Univ., Wuhan, China","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","For 8-bit microcontrollers, bank-switching is commonly used to increase memory capacity. The disadvantage of this technique is that bank (page) selection instructions are introduced when switching active data (program) bank. The page selection problem is to minimize the number of page selection instructions inserted. While previous efforts work on optimizing bank selection instructions for the data segment, our work focuses on minimizing page selection instructions for the program segment. Minimizing page selection instructions is a more challenging problem as the size of each procedure being allocated is affected by the number of inserted page selection instructions. In this paper, we first give a formal definition of the page selection problem, and then we formulate the problem as an Integer Linear Programming (ILP) to find the optimal solution. We introduce a tabu search heuristic algorithm, TMSEARCH, to solve the problem efficiently. The experimental results show that ILP can find optimal solutions for small-scale problems, and TMSEARCH is able to find good solutions for all benchmarks within reasonable time. Com-pared to a commercial compiler, TMSEARCH reduces total code size between 0.04% and 19.3%, and reduces page selection instructions between 24.3% and 78.7%.","","","","10.1109/CASES.2013.6662516","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662516","Microcontrollers; Page Selection Problem, Bank;Partitioned Memory Architecture;ROM Allocation;switching","Educational institutions;Heuristic algorithms;Memory architecture;Microcontrollers;Partitioning algorithms;Registers;Switches","instruction sets;integer programming;linear programming;memory architecture;minimisation;paged storage;search problems","ILP;TMSEARCH;active data program bank switching;bank page selection instructions;bank-switching;code size minimization;data segment;inserted page selection instructions;integer linear programming;memory capacity;microcontrollers;optimal solution;page selection instructions minimization;page selection optimization;page selection problem;partitioned memory architectures;small-scale problems;tabu search heuristic algorithm;word length 8 bit","","0","","31","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Message from the Program Co-Chairs","","","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","1","Presents the introductory welcome message from the conference proceedings.","","","","10.1109/CASES.2013.6662501","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662501","","","","","","0","","","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"An efficient run-time encryption scheme for non-volatile main memory","Xian Zhang; Chao Zhang; Guangyu Sun; Jia Di; Tao Zhang","Peking Univ., Beijing, China","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Emerging non-volatile memories (NVMs) have been considered as promising alternatives of DRAM for future main memory design. The NVM main memory has advantages of low standby power, high density, and good scalability. Its non-volatility, however, induces a security design challenge that data retained in memory after power-off need to be protected from malicious attacks. Although several approaches have been proposed to solve this problem through data encryption, they have some limitations such as high design complexity and non-trivial timing/energy overhead. Moreover, these techniques decrease the lifetime of NVM main memory due to extra write operations caused by encryption. In order to overcome these limitations, we propose an efficient PAD-XOR based encryption scheme in this work. A novel PAD generator based on a randomizer and several sub-PAD tables is introduced. With the PAD generator, our encryption scheme can provide run-time data protection to all data in NVM memory with low timing and power overhead. In addition, the encryption process can co-operate with wear-leveling of NVM to reduce design complexity. More important, our encryption technique has no impact on lifetime because no extra writes are incurred. Experimental results demonstrate that, compared to prior approaches, our design can achieve the same security strength with substantial lower overhead in respect of timing, energy consumption, and design complexity.","","","","10.1109/CASES.2013.6662530","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662530","","Complexity theory;Encryption;Generators;Nonvolatile memory;Random access memory","DRAM chips;computational complexity;cryptography;integrated circuit design","DRAM;NVM memory;PAD generator;PAD-XOR based encryption scheme;data encryption;design complexity reduction;energy consumption;future main memory design;malicious attacks;nontrivial energy overhead;nontrivial timing overhead;nonvolatile main memory;run-time data protection;run-time encryption scheme;security strength;subPAD tables","","1","","21","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Compiled multithreaded data paths on FPGAs for dynamic workloads","Halstead, R.J.; Najjar, W.","Dept. of Comput. Sci., Univ. of California, Riverside, Riverside, CA, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Hardware supported multithreading can mask memory latency by switching the execution to ready threads, which is particularly effective on irregular applications. FPGAs provide an opportunity to have multithreaded data paths customized to each individual application. In this paper we describe the compiler generation of these hardware structures from a C subset targeting a Convey HC-2ex machine. We describe how this compilation approach differs from other C to HDL compilers. We use the compiler to generate a multithreaded sparse matrix vector multiplication kernel and compare its performance to existing FPGA, and highly optimized software implementations.","","","","10.1109/CASES.2013.6662507","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662507","","Arrays;Field programmable gate arrays;Instruction sets;Kernel;Sparse matrices;Vectors","C language;field programmable gate arrays;matrix multiplication;multi-threading;program compilers;sparse matrices","C compilers;C subset;Convey HC-2ex machine;FPGA;HDL compilers;compilation approach;compiled multithreaded data paths;compiler generation;dynamic workloads;hardware structures;hardware supported multithreading;memory latency;multithreaded sparse matrix vector multiplication kernel","","2","","32","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"ILP<inf>c</inf>: A novel approach for scalable timing analysis of synchronous programs","Jia Jie Wang; Roop, P.S.; Andalam, S.","","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Synchronous programs have been widely used in the design of safety critical systems such as the flight control of Airbus A-380. To validate the implementations of synchronous programs, it is necessary to map the program's logical time (measured in logical ticks) to physical time (the execution time on a given processor). The static computation of the worst case execution time of logical ticks is called Worst Case Reaction Time (WCRT) analysis. Several approaches for WCRT analysis exist: max-plus algebra, model checking, reachability and integer linear programming (ILP). Of these approaches, reachability, model checking and ILP provide reasonably precise worst case estimates at the expense of longer analysis time. Apart from max-plus based approaches, which can produce large overestimates, the existing approaches suffer from the state space explosion problem. In this paper, we develop a new ILP based approach, called ILPc-which exploits the concurrency explicitly in the ILP formulation to avoid the state space explosion problem. Through extensive bench-marking we demonstrate the efficacy of the approach: for complex programs, ILPc is often orders of magnitude faster compared to the existing approaches, while achieving same level of precision. Thus, this paper paves the way for scalable WCRT analysis of complex embedded systems designed using the synchronous approach.","","","","10.1109/CASES.2013.6662526","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662526","Integer linear programming;Scability;Static timing analysis;Synchronous languages","Computational modeling;Concurrent computing;Instruction sets;Linear programming;Model checking;Semantics;Timing","aerospace control;integer programming;linear programming;program diagnostics;program verification;reachability analysis;safety-critical software","Airbus A-380;ILP<sub>c</sub>;WCRT analysis;complex embedded systems;execution time;flight control;integer linear programming;logical ticks;max-plus algebra;model checking;physical time;program logical time map;reachability;safety critical system design;scalable timing analysis;state space explosion problem;static computation;synchronous languages;synchronous programs;worst case execution time;worst case reaction time analysis","","0","","19","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Dynamic hardware specialization-using moore's bounty without burning the chip down","Sankaralingam, K.","Univ. of Wisconsin-Madison, Madison, WI, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","1","Summary form only given. The era of faster, smaller, greener (more power efficient) transistors in every successive generation appears to be dead. Due to slowing voltage scaling power has becoming a primary design constraint. Using conventional microprocessor techniques does not provide performance improvements without excessive power consumption. Instead, processor architects and microarchitects are going to be partially burdened with power-efficiently and energy-efficiently improving performance with technology scaling providing density improvements “alone”. The DySER project investigates ways for dynamically specializing datapaths to energy-efficiently improve performance. DySER attempts to provide a truly general purpose accelerator, avoiding radical changes to software development, ISA, or microarchitecture. The DySER accelerator is based on three principles: i) Exploit frequently executed, specializable code regions. ii) Dynamically configure the DySER accelerator hardware for particular regions. iii) Integrate the accelerator tightly, but non-intrusively, to a processor pipeline.We have completed a full prototype implementation of DySER integrated into the OpenSPARC processor (called SPARCDySER), a co-designed compiler in LLVM, and a detailed performance evaluation on an FPGA system, which runs an Ubuntu Linux distribution and full applications. Through the prototype, we evaluate the fundamental principles of DySER acceleration, namely: exploiting specializable regions, dynamically specializing hardware, and tight processor integration. To this end, we explore the accelerator's performance, power, and area, and consider comparisons to state-of-the-art microprocessors using energy/performance frontier analysis of both the prototype and simulated DySERaccelerated cores. Compared to the OpenSPARC processor, DySER provides 6.2X performance improvements and 4X energy reduction. DySER's approach of dynamic specialization is a promising way to add- ess the imminent power challenges.","","","","10.1109/CASES.2013.6662522","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662522","","Acceleration;Educational institutions;Green products;Hardware;Power demand;Prototypes;Transistors","Linux;energy conservation;field programmable gate arrays;formal specification;power aware computing;program compilers","DySER accelerator;FPGA system;Moore bounty;OpenSPARC processor compiler;Ubuntu Linux distribution;conventional microprocessor techniques;datapaths specalization;design constraint;dynamic hardware specialization;dynamically specializing hardware principle;exploiting specializable regions principle;field programmable gate array;general purpose accelerator;power consumption;processor integration principle;software development;transistors;voltage scaling power","","0","","","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Exploiting phase inter-dependencies for faster iterative compiler optimization phase order searches","Jantz, M.R.; Kulkarni, P.A.","Electr. Eng. & Comput. Sci., Univ. of Kansas, Lawrence, KS, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","The problem of finding the most effective set and ordering of optimization phases to generate the best quality code is a fundamental issue in compiler optimization research. Unfortunately, the exorbitantly large phase order search spaces in current compilers make both exhaustive as well as heuristic approaches to search for the ideal optimization phase combination impractical in most cases. In this paper we show that one important reason existing search techniques are so expensive is because they make no attempt to exploit well-known independence relationships between optimization phases to reduce the search space, and correspondingly improve the search time. We explore the impact of two complementary techniques to prune typical phase order search spaces. Our first technique studies the effect of implicit application of cleanup phases, while the other partitions the set of phases into mutually independent groups and develops new multi-stage search algorithms that substantially reduce the search time with no effect on best delivered code performance. Together, our techniques prune the exhaustive phase order search space size by 89%, on average, (96.75% total search space reduction) and show immense potential at making iterative phase order searches more feasible and practical. The pruned search space enables us to find a small set of distinct phase sequences that reach near-optimal phase ordering performance for all our benchmark functions as well as to improve the behavior of our genetic algorithm based heuristic search.","","","","10.1109/CASES.2013.6662511","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662511","iterative compilation;optimization ordering;search space pruning","Algorithm design and analysis;Benchmark testing;Hardware;Heuristic algorithms;Optimization;Registers;Search problems","genetic algorithms;heuristic programming;optimising compilers;search problems","benchmark functions;compiler optimization research;distinct phase sequences;exhaustive phase order search space size;genetic algorithm based heuristic search;heuristic approaches;iterative compiler optimization phase;large phase order search spaces;multistage search algorithms;near-optimal phase ordering performance;order searches;phase inter-dependencies;quality code;search space reduction","","0","","27","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"A novel compilation approach for image processing graphs on a many-core platform with explicitly managed memory","Lepley, T.; Paulin, P.; Flamand, E.","STMicroelectron., Grenoble, France","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Explicitly managed memory many-cores (EMM) have been a part of the industrial landscape for the last decade. The IBM CELL processor, general-purpose graphics processing units (GP-GPU) and the STHORM embedded many-core of STMicroelectronics are representative examples. This class of architecture is expected to scale well and to deliver good performance per watt and per mm<sup>2</sup> of silicon. As such, it is appealing for application problems with regular data access patterns. However, this moves significant complexity to the programmer who must master parallelization and data movement. High level programming tools are therefore essential in order to allow the effective programming of EMM many-cores to a wide class of programmers. This paper presents a novel approach designed for simplifying the programming of EMM many-core architectures. It initially addresses the image processing application domain and has been targeted to the STHORM platform. It takes a high-level description of the computation kernel algorithm and generates an OpenCL kernel optimized for the target architecture, while managing the parallelization and data movements across the hierarchy in a transparent fashion. The goal is to provide both high productivity and high performance without requiring parallel computing expertise from the programmer, nor the need for application code specialization for the target architecture.","","","","10.1109/CASES.2013.6662510","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662510","Compiler, Parallelization, Productivity, Many;DMA;Image Processing;OpenCL;Performance;Tiling;core","Abstracts;Computer architecture;Image processing;Kernel;Optimization;Programming;Tiles","graph theory;graphics processing units;image processing;memory architecture;multiprocessing systems","EMM many-core architectures;GP-GPU;IBM CELL processor;OpenCL kernel generation;STHORM embedded many-core;STMicroelectronics;compilation approach;computation kernel algorithm;data access patterns;data movement;explicitly managed memory many-core platform;general-purpose graphics processing units;high level programming tools;image processing application domain;image processing graphs;parallelization movement","","0","","14","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Expandable process networks to efficiently specify and explore task, data, and pipeline parallelism","Schor, L.; Hoeseok Yang; Bacivarov, L.; Thiele, L.","Comput. Eng. & Networks Lab., ETH Zurich, Zurich, Switzerland","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Running each application of a many-core system on an isolated (virtual) guest machine is a widely considered solution for performance and reliability issues. When a new application is started, the guest machine is assigned with an amount of computing resources that depends on the overall workload of the system and is not known to the designer at specification time. For instance, the computing resources might consist of many slow or a few fast processing elements. If the application is statically specified, as, for example, with Kahn process networks, the number of processing elements usable by an application is upper bounded by its number of processes. Similarly, the inter-process communication overhead might limit the maximum performance if the number of processing elements is significantly smaller than the number of processes. In this paper, we propose a formal extension for streaming programming models called expandable process networks (EPNs) that tackles this challenge by abstracting several possible granularities in a single specification. This enables the automatic exploration of task, data, and pipeline parallelism by two basic design transformation techniques, namely replication and unfolding. Then, the EPN semantics facilitates the synthesis of multiple design implementations that are all derived from one high-level specification. At runtime, the best fitting implementation for the given computing resources is selected to maximize the performance. Finally, we demonstrate the effectiveness of the proposed model on Intel's 48-core SCC processor.","","","","10.1109/CASES.2013.6662509","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662509","","Algorithm design and analysis;Computer architecture;Network topology;Parallel processing;Pipelines;Semantics;Topology","multiprocessing systems;parallel processing;pipeline processing","EPNs;Intel 48-core SCC processor;Kahn process networks;computing resources;data parallelism;expandable process networks;fast processing elements;high-level specification;inter-process communication overhead;isolated guest machine;many-core system;pipeline parallelism;replication design transformation techniques;streaming programming models;task parallelism;unfolding design transformation techniques;upper bound;virtual machine","","2","","29","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"Hardware acceleration for programs in SSA form","Mohr, M.; Grudnitsky, A.; Modschiedler, T.; Bauer, L.; Hack, S.; Henkel, J.","Karlsruhe Inst. of Technol., Karlsruhe, Germany","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","Register allocation is one of the most time-consuming parts of the compilation process. Depending on the quality of the register allocation, a large amount of shuffle code to move values between registers is generated. In this paper, we propose a processor architecture extension to provide register file permutations by which the shuffle code can be implemented more efficiently. We present compiler support to utilize this extension, an evaluation regarding performance and compilation time using the SPEC CINT2000 benchmark, as well as an analysis of area and frequency overhead of our architecture implementation. We find that using our extension, the number of executed instructions is reduced by up to 5.1 % while the compilation time is unaffected.","","","","10.1109/CASES.2013.6662518","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662518","","Encoding;Equations;Hardware;Interference;Registers;Resource management;Semantics","optimising compilers","SPEC CINT2000 benchmark;SSA form;area overhead;compilation process;frequency overhead;hardware acceleration;processor architecture extension;register allocation;shuffle code;static single assignment form","","0","","32","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"EVA: An efficient vision architecture for mobile systems","Clemons, J.; Pellegrini, A.; Savarese, S.; Austin, T.","Dept. of Electr. Eng. & Comput. Sci., Univ. of Michigan, Ann Arbor, MI, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on","20131114","2013","","","1","10","The capabilities of mobile devices have been increasing at a momentous rate. As better processors have merged with capable cameras in mobile systems, the number of computer vision applications has grown rapidly. However, the computational and energy constraints of mobile devices have forced computer vision application developers to sacrifice accuracy for the sake of meeting timing demands. To increase the computational performance of mobile systems we present EVA. EVA is an application-specific heterogeneous multicore having a mix of computationally powerful cores with energy efficient cores. Each core of EVA has computation and memory architectural enhancements tailored to the application traits of vision codes. Using a computer vision benchmarking suite, we evaluate the efficiency and performance of a wide range of EVA designs. We show that EVA can provide speedups of over 9× that of an embedded processor while reducing energy demands by as much as 3×.","","","","10.1109/CASES.2013.6662517","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662517","Architecture;Computer Vision;Mobile","Computer vision;Feature extraction;Mobile communication;Multicore processing;Program processors;Registers;Vectors","computer vision;embedded systems;energy conservation;memory architecture;microprocessor chips;mobile computing;multiprocessing systems","EVA designs;application-specific heterogeneous multicore;cameras;computational constraints;computational performance;computationally powerful cores;computer vision applications;computer vision benchmarking;embedded processor;energy constraints;energy demands;energy efficient cores;memory architectural enhancements;mobile devices capabilities;mobile systems;processors;vision architecture;vision codes","","2","","42","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
"CMSM: An efficient and effective Code Management for Software Managed Multicores","Ke Bai; Jing Lu; Shrivastava, A.; Holton, B.","Compiler Microarchitecture Lab., Arizona State Univ., Tempe, AZ, USA","Hardware/Software Codesign and System Synthesis (CODES+ISSS), 2013 International Conference on","20131111","2013","","","1","9","As we scale the number of cores in a multicore processor, scaling the memory hierarchy is a major challenge. Software Managed Multicore (SMM) architectures are one of the promising solutions. In an SMM architecture, there are no caches, and each core has only a local scratchpad memory. If all the code and data of the task mapped to a core do not fit on its local scratchpad memory, then explicit code and data management is required. In this paper, we solve the problem of efficiently managing code on an SMM architecture. We extend the state of the art by: i) correctly calculating the code management overhead, ii) even in the presence of branches in the task, and iii) developing a heuristic CMSM (Code Mapping for Software Managed multicores) that results in efficient code management execution on the local scratchpad memory. Our experimental results collected after executing applications from MiBench suite [1] on the Cell SPEs (Cell is an SMM architecture) [2], demonstrate that correct management cost calculation and branch consideration can improve performance by 12%. Our heuristic CMSM can reduce runtime in more than 80% of the cases, and by up to 20% on our set of benchmarks.","","","","10.1109/CODES-ISSS.2013.6658998","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658998","Code;SPM;embedded systems;instruction;local memory;multi-core processor;scratchpad memory","Estimation;Interference;Memory management;Multicore processing;Software;Switches","multiprocessing systems;storage management","Cell SPE;SMM architecture;branch consideration;code management;code mapping for software managed multicores;data management;heuristic CMSM;management cost calculation;scratchpad memory;software managed multicores","","0","","22","","","Sept. 29 2013-Oct. 4 2013","","IEEE","IEEE Conference Publications"
