"http://ieeexplore.ieee.org/search/searchresult.jsp?bulkSetSize=2000&queryText%3DCompilers%2C+Architectures+and+Synthesis+for+Embedded+Systems+.LB.CASES.RB.%2C+2014",2015/06/23 15:21:43
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN","ISBN","EISBN","DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Energy-efficient VFI-partitioned multicore design using wireless NoC architectures","Kim, R.; Guangshuo Liu; Wettin, P.; Marculescu, R.; Marculescu, D.; Pande, P.P.","Sch. of EECS, Washington State Univ., Pullman, WA, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","9","In recent years, multiple Voltage Frequency Island (VFI)-based designs have increasingly made their way into both commercial and research multicore platforms. On the other hand, the wireless Network-on-Chip (WiNoC) architecture has emerged as an energy-efficient and high bandwidth communication backbone for massively integrated multicore platforms. It becomes therefore possible to exploit the small-world effects induced by the wireless links of a WiNoC to achieve efficient inter-VFI data exchanges. In this work, we demonstrate that WiNoCs can provide better latency and energy profiles compared to traditional mesh-like architecture for VFI-partitioned multicore designs. The performance gains and energy efficiency are achieved due to the low-power wireless shortcuts in conjunction with the small-world architecture. Indeed, our experimental results show energy improvements as large as 40% for multithreaded application benchmarks.","","","","10.1145/2656106.2656120","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972456","NoC, Wireless, Multicore, VFI, low;power","Benchmark testing;Energy efficiency;Multicore processing;Ports (Computers);Routing;Wireless communication","energy conservation;multiprocessing systems;network-on-chip;power aware computing","VFI-based design;commercial multicore platform;energy efficiency;energy-efficient VFI-partitioned multicore design;inter-VFI data exchange;low-power wireless shortcuts;massively integrated multicore platform;multithreaded application benchmark;performance gain;research multicore platform;voltage frequency island;wireless NoC architecture;wireless network-on-chip","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"A high-level model of embedded flash energy consumption","Pallister, J.; Eder, K.; Hollis, S.J.; Bennett, J.","Univ. of Bristol, Bristol, UK","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","9","The alignment of code in the flash memory of deeply embedded SoCs can have a large impact on the total energy consumption of a computation. We investigate the effect of code alignment in six SoCs and find that a large proportion of this energy (up to 15% of total SoC energy consumption) can be saved by changes to the alignment. A flexible model is created to predict the read-access energy consumption of flash memory on deeply embedded SoCs, where code is executed in place. This model uses the instruction level memory accesses performed by the processor to calculate the flash energy consumption of a sequence of instructions. We derive the model parameters for five SoCs and validate them. The error is as low as 5%, with a 11% average normalized RMS deviation overall. The scope for using this model to optimize code alignment is explored across a range of benchmarks and SoCs. Analysis shows that over 30% of loops can be better aligned. This can significantly reduce energy while increasing code size by less than 4%. We conclude that this effect has potential as an effective optimization, saving significant energy in deeply embedded SoCs.","","","","10.1145/2656106.2656108","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972473","","Ash;Energy consumption;Memory management;Nonvolatile memory;Optimization;Random access memory;System-on-chip","embedded systems;energy consumption;flash memories;system-on-chip","RMS deviation;code alignment;deeply embedded SoC;embedded flash energy consumption;energy proportion;flash memory;instruction level memory access;root mean square deviation;system-on-chip","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"A low-cost memory interface for high-throughput accelerators","Jing Huang; Yuanjie Huang; Temam, O.; Ienne, P.; Yunji Chen; Chengyong Wu","State Key Lab. of Comput. Archit., Inst. of Comput. Technol., Beijing, China","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","Heterogeneous multi-cores, a mix of cores and accelerators, are becoming prevalent. These accelerators are designed for both speed and energy improvements, and thus, they increasingly come with a large number of load/store ports for achieving a high degree of parallelism. However, beyond GPG-PUs, accelerators such as ASICs and CGRAs are increasingly capable of accelerating computations with irregular control flow and memory accesses; as a result, such accelerators need to be plugged to caches instead of scratchpads, and few studies focus on accelerator-to-cache interfaces. The main existing alternative are Load/Store Queues (LSQs) traditionally used to connect superscalar processors to caches and memory, but in the context of accelerators, they are overkill and could significantly reduce the area and power benefits of accelerators. Moreover, we show that they are just not fit for accelerators plugged to multi-banked caches. In this article, we propose a fast accelerator-to-cache interface with a moderate area and power footprint compared to LSQs, even for a large number of load/store ports. For that purpose, we introduce a set of low-overhead techniques for ensuring in-order delivery of requests to/from cache banks. We synthesize and layout at 65nm the design of both our interface and an LSQ specially adapted to accelerators for a fair comparison. We find that our interface achieves on average 78% of the performance of an LSQ using only 16% of the area and 24% of the power.","","","","10.1145/2656106.2656109","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972464","","Acceleration;Application specific integrated circuits;Computer architecture;Layout;Out of order;Ports (Computers)","application specific integrated circuits;cache storage;graphics processing units;integrated circuit design;multiprocessing systems","ASIC;CGRA;GPG-PU;LSQ;accelerator-to-cache interfaces;cache banks;control flow;heterogeneous multicores;high-throughput accelerators;load-store queues;memory accesses;memory interface;multibanked caches;superscalar processors","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Reducing cache leakage energy for hybrid SPM-cache architectures","Hao Wen; Wei Zhang","Dept. of Electr. & Comput. Eng., Virginia Commonwealth Univ., Richmond, VA, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","9","In this paper, we study how to reduce the cache leakage energy efficiently in a hybrid SPM (Scratch-Pad Memory) and cache architecture. Since SPM can reduce the access frequency to the cache, we find it is possible to place the cache lines of the hybrid SPM-cache into the low power mode more aggressively than traditional leakage management for regular caches, which can reduce more leakage energy without significant performance degradation. Also, we propose a Hybrid Drowsy-Gated Vdd (HDG) technique, which can adaptively exploit both short and long idle intervals to minimize leakage energy with insignificant performance overhead.","","","","10.1145/2656106.2656124","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972474","Pad Memory (SPM);Scratch;cache;cache decay;drowsy cache;leakage energy","Computer architecture;Degradation;Logic gates;Program processors;Radiation detectors;Resource management;Switches","cache storage;low-power electronics;memory architecture;power aware computing","HDG technique;access frequency;cache leakage energy reduction;cache lines;hybrid SPM-cache architectures;hybrid drowsy-gated Vdd;idle intervals;leakage management;low power mode;scratch-pad memory","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"AdaPNet: Adapting process networks in response to resource variations","Schor, L.; Bacivarov, I.; Hoeseok Yang; Thiele, L.","Comput. Eng. & Networks Lab., ETH Zurich, Zurich, Switzerland","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","A widely considered strategy to prevent interference issues on multi-processor systems is to isolate the execution of the individual applications by running each of them on a dedicated virtual guest machine. The amount of computing power available to a single application, however, depends on the other applications running on the system and may change over time. A promising approach to maximize the performance under such conditions is to adapt the application's degree of parallelism when the resources allocated to the application are changed. This enables an application to exploit not more parallelism than required, thereby reducing inter-process communication and scheduling overheads. In this paper, we introduce AdaPNet, a run-time system to execute streaming applications, which are modeled as process networks, efficiently on platforms with dynamic resource al-location. AdaPNet responds to changes in the available resources by first calculating a process network that maximizes the performance of the application on the new resources. Then, AdaPNet transparently transforms the application into the alternative network without discarding the program state. Targeting two many-core systems, we demonstrate that AdaPNet outperforms comparable run-time systems, which do not adapt the degree of parallelism, in terms of speed-up and memory usage.","","","","10.1145/2656106.2656112","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972475","Streaming applications;multi-processor systems;optimization;process networks;run-time adaptivity","Contracts;Fires;Parallel processing;Radiation detectors;Resource management;Throughput;Transforms","media streaming;multiprocessing systems;parallel processing;resource allocation","AdaPNet;adaptive runtime system;dynamic resource allocation;many-core systems;parallelism degree;process networks;streaming applications","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"A compiler framework for automatically mapping data parallel programs to heterogeneous MPSoCs","Chandramohan, K.; O'Boyle, M.F.P.","Sch. of Inf., Univ. of Edinburgh, Edinburgh, UK","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","Many of today's embedded devices are based on MultiProcessor System-on-Chips(MPSoCs) Such devices are usually heterogeneous, containing DSPs and specialized accelerators as well as one or more CPUs. This heterogeneity allows efficient implementations in specialized domains but is a barrier to their wider use. They are difficult to program as only the CPU is directly exposed to the programmer with access to other resources restricted to narrow library interfaces. This paper enables the exploitation of heterogeneous resources from a high level parallel programming model. It presents an LLVM based compiler that maps OpenMP programs to the underlying heterogeneous cores using an SPMD model of computation. It partitions data and computation across the cores, managing synchronization and memory coherence across different memory domains and operating systems. We evaluate its performance on the OMAP4 MPSoC on a range of data parallel benchmarks. On average it gives a 2.75x speedup over using the low-level library approach. Further-more, it gives a speedup of 1.38x and an improved energy efficiency of 1.4x over using the two A9 cores alone.","","","","10.1145/2656106.2656107","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972462","Compiler, SPMD, data;heterogeneous processors;parallel","Arrays;Computational modeling;Digital signal processing;Instruction sets;Programming;Synchronization","high level languages;multiprocessing systems;parallel programming;program compilers;synchronisation;system-on-chip","A9 cores;LLVM based compiler;OMAP4 MPSoC;Open Multimedia Applications Platform(OMAP);OpenMP programs;SPMD model;Single Program Multiple Data;compiler framework;data parallel benchmarks;data parallel program mapping;energy efficiency;heterogeneous MPSoCs;heterogeneous cores;high level parallel programming model;memory coherence;multiprocessor system-on-chips;operating systems;synchronization","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Construction of GCCFG for inter-procedural optimizations in Software Managed Manycore (SMM) architectures","Holton, B.; Bai, K.; Shrivastava, A.; Ramaprasad, H.","Arizona State Univ., Tempe, AZ, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","Software Managed Manycore (SMM) architectures - in which each core has only a scratch pad memory (instead of caches), - are a promising solution for scaling memory hierarchy to hundreds of cores. However, in these architectures, the code and data of the tasks mapped to the cores must be explicitly managed in the software by the compiler. State-of-the-art compiler techniques for SMM architectures require inter-procedural information and analysis. A call graph of the program does not have enough information, and Global CFG, i.e., combining all the control flow graphs of the program has too much information, and becomes too big. As a result, most new techniques have informally defined and used GCCFG (Global Call Control Flow Graph) - a whole program representation which captures the control-flow as well as function call information in a succinct way - to perform inter-procedural analysis. However, how to construct it has not been shown yet. We find that for several simple call and control flow graphs, constructing GCCFG is relatively straightforward, but there are several cases in common applications where unique graph transformation is needed in order to formally and correctly construct the GCCFG. This paper fills this gap, and develops graph transformations to allow the construction of GCCFG in (almost) all cases. Our experiments show that by using succinct representation (GCCFG) rather than elaborate representation (GlobaICFG) the compilation time of state-of-the-art code management technique [4] can be improved by an average of 5X, and that of stack management [20] can be improved by an average of 4X.","","","","10.1145/2656106.2656122","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972471","","Abstracts;Computer architecture;Data mining;Flow graphs;Optimization;Program processors","flow graphs;memory architecture;multiprocessing systems;optimisation","GCCFG;Global Call Control Flow Graph;SMM architectures;call graph;compiler techniques;interprocedural optimizations;memory hierarchy scaling;scratch pad memory;software managed manycore architectures","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Heuristics for greedy transport triggered architecture interconnect exploration","Viitanen, T.; Kultala, H.; Jaaskelainen, P.; Takala, J.","Dept. of Pervasive Comput., Tampere Univ. of Technol., Tampere, Finland","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","7","Most power dissipation in Very Large Instruction Word (VLIW) processors occurs in their large, multi-port register files. Transport Triggered Architecture (TTA) is a VLIW variant whose exposed datapath reduces the need for RF accesses and ports. However, the comparative advantage of TTAs suffers in practice from a wide instruction word and complex interconnection network (IC). We argue that these issues are at least partly due to suboptimal design choices. The design space of possible TTA architectures is very large, and previous automated and ad-hoc design methods often produce inefficient architectures. We propose a reduced design space where efficient TTAs can be generated in a short time using excecution trace-driven greedy exploration. The proposed approach is evaluated by optimizing the equivalent of a 4-issue VLIW architecture. The algorithm finishes quickly and produces a processor with 10% reduced core energy product compared to a fully-connected TTA. Since the generated processor has low IC power and a shorter instruction word than a typical 4-issue VLIW, the results support the hypothesis that these drawbacks of TTA can be worked around with efficient IC design.","","","","10.1145/2656106.2656123","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972455","TTA;VLIW;design space exploration;port sharing","Benchmark testing;Computer architecture;Integrated circuits;Ports (Computers);Program processors;Radio frequency;VLIW","greedy algorithms;instruction sets;multiprocessor interconnection networks;parallel architectures","IC design;IC power;RF accesses;TTA;VLIW processors;ad-hoc design methods;automated design methods;complex interconnection network;datapath;design space;excecution trace-driven greedy exploration;greedy transport triggered architecture interconnect exploration;heuristics;large multiport register files;ports;power dissipation;reduced core energy product;very large instruction word processors","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"A system-level simulation framework for evaluating task migration in MPSoCs","Wei Quan; Pimentel, A.D.","Inf. Inst. Univ. of Amsterdam The Netherlands, Amsterdam, Netherlands","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","9","Task migration is the transfer of the execution of a process (task) from one processing element to another. It originates from the massive deployment of distributed systems in the parallel computing field to enable dynamic load distribution, fault resilience and to enhance data access locality. With the development of MultiProcessor System-on-Chip (MPSoC) architectures, the topic of task migration has recently regained research interest in the embedded systems domain. In this paper, we present a high-level simulation framework to study task migration for MPSoC systems. With this framework, different migration methodologies on different underlying hardware systems can be easily and rapidly modeled, simulated and evaluated during the early stages of design. By using this high-level simulation framework, a designer can study the migration impact on the overall performance of the system by exploring different task migration mechanisms (determining what and how to migrate) or using different migration policies (determining when to migrate which tasks whereto) in a specific task migration mechanism. Using a number of experiments, we demonstrate the capabilities of our simulation framework.","","","","10.1145/2656106.2656111","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972466","Embedded systems;MPSoC;simulation;task migration","Communication channels;Computational modeling;Computer architecture;Context;Hardware;Load modeling;Program processors","digital simulation;embedded systems;multiprocessing systems;system-on-chip","MPSoC architectures;embedded systems;multiprocessor system-on-chip;system-level simulation framework;task migration evaluation","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Team up: Cooperative memory management in embedded systems","Stilkerich, I.; Taffner, P.; Erhardt, C.; Dietrich, C.; Wawersich, C.; Stilkerich, M.","Friedrich-Alexander Univ., Erlangen-Nuremberg, Germany","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","The use of a managed, type-safe languages such as Java in realtime and embedded systems can offer productivity and, in particular, safety and dependability benefits over the dominating unsafe languages at reasonable costs. A JVM that has dynamic memory-management needs to provide an implicit memory-management strategy, that is, for example, a garbage collector (GC) or stack allocation provided by the escape analysis of the JVM's compiler: Explicit management of dynamically allocated memory (i.e., by use of functions such as C's malloc () and free()) is vulnerable to programming errors such as neglected or false memory release operations causing memory leaks or dangling pointers. Such operations have the potential to break the soundness of the type system and are therefore usually not available for strongly typed languages. Type-safe languages in combination with static analyses - which respect hardware as well as system-specific information - can efficiently be employed to provide a runtime system including memory management (MM) that is specifically suited to an embedded application on a particular hardware device. In the context of this paper, we present novel memory-management strategy we implemented in our KESO JVM. It is a latency-aware garbage-collection algorithm called LAGC. Also, we introduce the static analyses that can assist LAGC. The application developers have to ensure that there is enough time for the GCs to run. Hardware characteristics such as soft-error proneness of the hardware or the memory layout can also be taken into consideration as demanded by the system configuration. This is achieved by integrating the GCs in the design process of the whole system just as any other user application, which is the reason why this approach is called cooperative memory management. The suggested strategies require reasonably low overhead.","","","","10.1145/2656106.2656129","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972463","Design;Garbage Collection;General Terms Memory Management;Languages","Embedded systems;Hardware;Java;Memory management;Random access memory;Resource management;Runtime","Java;program compilers;program diagnostics;storage management","JVM compiler;Java;KESO JVM;LAGC;cooperative memory management;dynamic memory-management;embedded system;latency-aware garbage-collection algorithm;soft-error proneness;stack allocation;static analysis;type-safe language","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Auto-parallelization of data structure operations for GPUs","Nasre, R.","IIT Madras, Chennai, India","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","We present an auto-parallelization technique for generating GPU implementation of data-structure operations from a sequential specification. The technique partitions the data-structure operations into barrier-separated phases such that each phase executes only homogeneous operations. Homogeneity is dictated by the method type, which is derived from the specification. Two key aspects of our technique are: (i) it ensures linearizability of the data-structure, and (ii) it is capable of composing multiple data-structure operations with the guarantee of optimal barrier placement, which we formally prove. We illustrate the usefulness of our techniques by synthesizing efficient GPU implementations of practical graph algorithms like single-source shortest paths which uses a concurrent worklist, Delaunay mesh refinement that uses a worklist and a mesh, and a doubly linked-list supporting arbitrary insertion and deletion.","","","","10.1145/2656106.2656115","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972460","Auto;GPGPU;data structures;parallelization","Data structures;Graphics processing units;Instruction sets;Kernel;Libraries;Synchronization;Synthesizers","data structures;graphics processing units;parallel processing","Delaunay mesh refinement;GPU;auto-parallelization technique;barrier-separated phase;concurrent worklist;data structure operation;doubly linked-list supporting arbitrary deletion;doubly linked-list supporting arbitrary insertion;graphics processing unit;homogeneous operation;optimal barrier placement;sequential specification","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"SDCTune: A model for predicting the SDC proneness of an application for configurable protection","Qining Lu; Pattabiraman, K.; Gupta, M.S.; Rivers, J.A.","","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","Silent Data Corruption (SDC) is a serious reliability issue in many domains, including embedded systems. However, current protection techniques are brittle, and do not allow programmers to trade off performance for SDC coverage. Further, many of them require tens of thousands of fault injection experiments, which are highly time-intensive. In this paper, we propose an empirical model to predict the SDC proneness of a program's data called SDCTune. SDCTune is based on static and dynamic features of the program alone, and does not require fault injections to be performed. We then develop an algorithm using SDCTune to selectively protect the most SDC-prone data in the program subject to a given performance overhead bound. Our results show that our technique is highly accurate at predicting the relative SDC rate of an application, and outperforms full duplication by a factor of 0.83 to 1.87x in efficiency of detection (i.e., ratio of SDC coverage provided to performance overhead).","","","","10.1145/2656106.2656127","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972476","Compiler;Modeling;Reliability","Benchmark testing;Computer crashes;Detectors;Hardware;Registers;Reliability;Transient analysis","data protection;embedded systems;fault tolerant computing;performance evaluation","SDC coverage;SDC proneness prediction;SDC rate;SDC-prone data;SDCTune;configurable protection;dynamic features;embedded systems;performance overhead bound;reliability issue;silent data corruption;static features","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Splitting functions into single-entry regions","Hepp, S.; Brandner, F.","Inst. of Comput. Languages, Vienna Univ. of Technol., Vienna, Austria","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","As the performance requirements of today's real-time systems are on the rise, system engineers are increasingly forced to optimize and tune the execution time of real-time software. Apart from usual optimizations targeting the average-case performance of a program, the worst-case execution time bound (WCET) delivered by program analysis tools often has to be improved to meet all the deadlines and ensure a safe operation of the entire system. Modern computer architectures pose a significant challenge to this task due to their high complexity. Out-of-order execution, speculation, caches, buffers, and branch predictors highly depend on the execution history and are thus difficult to analyze precisely for WCET analysis tools. Time-predictable computer architectures overcome this problems by specifically designed hardware components that are amenable to static program analysis. A recently proposed alternative for caching executable code, i.e., instructions, is the so-called method cache. Instead of a traditional block-based cache design, the method cache operates on larger code blocks under the control of the compiler. Due to its design, the analysis of the method cache is simplified. At the same time, such a system now highly depends on the compiler and its ability to form suitable code blocks for caching. We propose a simple function splitting technique that derives a suitable partitioning of the basic blocks in a program, targeting the method cache of the time-predictable processor Patmos. Our approach exploits dominance properties to form code regions respecting the method cache's parameters as well as constraints of Patmos' instruction set architecture. Experimental results show that the method cache can be competitive with typical instruction cache configurations given the right splitting.","","","","10.1145/2656106.2656128","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972470","Function Splitting;Graph Partitioning;Method Cache;Real-Time Systems;Single-Entry Region","Complexity theory;Computer architecture;Hardware;Partitioning algorithms;Real-time systems;Software;Standards","cache storage;computer architecture;instruction sets;program compilers;program diagnostics","Patmos;WCET analysis tools;average-case performance;branch predictors;buffers;compiler;executable code caching;execution history;function splitting technique;hardware components;instruction cache configurations;instruction set architecture;method cache;out-of-order execution;program analysis tools;real-time software;single-entry regions;static program analysis;time-predictable computer architectures;time-predictable processor;worst-case execution time bound","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Control-layer optimization for flow-based mVLSI microfluidic biochips","Kai Hu; Trung Anh Dinh; Tsung-Yi Ho; Chakrabarty, K.","Duke Univ., Durham, NH, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","Recent advantages in flow-based microfluidic biochips have enabled the emergence of lab-on-a-chip devices for bimolecular recognition and point-of-care disease diagnostics. However, the adoption of flow-based biochips is hampered today by the lack of computer-aided design tools. Manual design procedures not only delay product development but they also inhibit the exploitation of the design complexity that is possible with current fabrication techniques. In this paper, we present the first practical problem formulation for automated control-layer design in flow-based microfluidic VLSI (mVLSI) biochips and propose a systematic approach for solving this problem. Our goal is to find an efficient routing solution for control-layer design with a minimum number of control pins. The pressure-propagation delay, an intrinsic physical phenomenon in mVLSI biochips, is minimized in order to reduce the response time for valves, decrease the pattern set-up time, and synchronize valve actuation. Two fabricated flow-based devices and five synthetic benchmarks are used to evaluate the proposed optimization method. Compared with manual control-layer design and a baseline approach, the proposed approach leads to fewer control pins, better timing behavior, and shorter channel length in the control layer.","","","","10.1145/2656106.2656118","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972469","","Delays;Microchannels;Mixers;Multiplexing;Pins;Routing;Valves","VLSI;biological techniques;lab-on-a-chip;microfluidics;microvalves;optimisation","automated control-layer design;bimolecular recognition;channel length;computer-aided design tool;control pin;control-layer optimization;fabrication technique;flow-based microfluidic VLSI biochip;lab-on-a-chip device;manual design procedure;pattern set-up time;point-of-care disease diagnostic;pressure-propagation delay;response time;synthetic benchmark;valve actuation synchronization;very-large-scale integration","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"A compilation flow for parametric dataflow: Programming model, scheduling, and application to heterogeneous MPSoC","Dardaillon, M.; Marquet, K.; Risset, T.; Martin, J.; Charles, H.-P.","INSA-Lyon, Univ. de Lyon, Villeurbanne, France","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","Efficient programming of signal processing applications on embedded systems is a complex problem. High level models such as Synchronous dataflow (SDF) have been privileged candidates for dealing with this complexity. These models permit to express inherent application parallelism, as well as analysis for both verification and optimization. Parametric dataflow models aim at providing sufficient dynamicity to model new applications, while at the same time maintaining the high level of analyzability needed for efficient real life implementations. This paper presents a new compilation flow that targets parametric dataflows. Built on the LLVM compiler infrastructure, it offers an actor based C++ programming model to describe parametric graphs, a compilation front-end providing graph analysis features, and a retargetable back-end to map the application on real hardware. This paper gives an overview of this flow, with a specific focus on scheduling. The crucial gap between dataflow models and real hardware on which actor firing is not atomic, as well as the consequences on FIFOs sizing and execution pipelining are taken into account. The experimental results illustrate our compilation flow applied to compilation of 3GPP LTE-Advanced demodulation on a heterogeneous MPSoC with distributed scheduling features. This achieves performances similar to time-consuming hand made optimizations.","","","","10.1145/2656106.2656110","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972461","compiler;data flow;heterogeneous MP-SoC;programming model;scheduling","Computational modeling;Hardware;Program processors;Programming;Schedules;Synchronization;System-on-chip","C++ language;data flow computing;graph theory;multiprocessing systems;optimisation;processor scheduling;program compilers;system-on-chip","3GPP LTE-advanced demodulation;C++ programming model;FIFOs sizing;LLVM compiler infrastructure;SDF;application parallelism;compilation flow;compilation front-end;distributed scheduling features;embedded systems;execution pipelining;graph analysis features;heterogeneous MPSoC;high level models;optimization;parametric dataflow models;parametric graphs;signal processing applications;synchronous dataflow;verification","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Context-sensitive timing simulation of binary embedded software","Ottlik, S.; Stattelmann, S.; Viehl, A.; Rosenstiel, W.; Bringmann, O.","FZI Res. Center for Inf. Technol., Karlsruhe, Germany","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","We present an approach to accurately simulate the temporal behavior of binary embedded software based on timing data generated using static analysis. As the timing of an instruction sequence is significantly influenced by the microarchitecture state prior to its execution, which highly depends on the preceding control flow, a sequence must be separately considered for different control flow paths instead of estimating the influence of basic blocks or single instructions in isolation. We handle the thereby arising issue of an excessive or even infinite number of different paths by considering different execution contexts instead of control flow paths. Related approaches using context-sensitive cycle counts during simulation are limited to simulating the control flow that could be considered during analysis. We eliminate this limitation by selecting contexts dynamically, picking a suitable one when no predetermined choice is available, thereby enabling a context-sensitive simulation of unmodified binary code of concurrent programs, including asynchronous events such as interrupts. In contrast to other approximate binary simulation techniques, estimates are conservative, yet tight, making our approach reliable when evaluating performance goals. For a multi-threaded application the simulation deviates only by 0.24% from hardware measurements while the average overhead is only 50% compared to a purely functional simulation.","","","","10.1145/2656106.2656117","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972467","Binary Level Simulation;Instruction Set Simulation;Software Timing Simulation;System Level Design;Virtual Prototyping","Analytical models;Context;Context modeling;Data models;Databases;Software;Timing","concurrency control;embedded systems;program diagnostics;software architecture","binary embedded software;concurrent programs;context-sensitive timing simulation;control flow paths;instruction sequence timing;microarchitecture state;static analysis","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Retargetable automatic generation of compound instructions for CGRA based reconfigurable processor applications","Miniskar, N.R.; Kohli, S.; Haewoo Park; Donghoon Yoo","SAIT, Samsung R&D Inst., Bangalore, India","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","9","Reconfigurable processors such as SRP (Samsung Reconfigurable Processors) have become increasingly important, which enables just enough flexibility of accepting software solutions and providing application specific hardware configurability for faster time-to-market, lower development cost and higher performance while maintaining lower energy consumption and area. The reconfigurable processor compilation framework supports wide range of architectures through architecture description template for different domains of applications such as image processing, multimedia, video, and graphics. These architectures support several domain specific compound instructions (also called as intrinsics), which are computationally efficient when compared to the set of general instructions in the processor. Application developers have to use these intrinsics in their programs according to the architecture, which can result very inefficient usage, tedious and more error-prone. More-over, the intrinsics provided by the architecture need constant reference to the intrinsics file during development. In this paper, we propose a retargetable novel methodology for the automatic generation of compound instructions for a given architecture and application source code at compile time. Our approach is able to consider ~75% of total intrinsics in the architectures with the success rate of > 90% in identifying the intrinsics in the benchmarks such as AVC OpenGL Full Engine and OpenGL Vector benchmarks.","","","","10.1145/2656106.2656125","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972457","","Cloning;Compounds;Computer architecture;Pattern matching;Software;VLIW;Vectors","instruction sets;program compilers;reconfigurable architectures","CGRA;LLVM compiler;SRP;Samsung reconfigurable processors;architecture description template;coarse grain reconfigurable array;compound instructions;retargetable automatic generation","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Fault resilient physical neural networks on a single chip","Weidong Shi; Yuanfeng Wen; Ziyi Liu; Xi Zhao; Boumber, D.; Vilalta, R.; Lei Xu","Univ. of Houston Houston, Houston, TX, USA","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","Device scaling engineering is facing major challenges in producing reliable transistors for future electronic technologies. With shrinking device sizes, the total circuit sensitivity to both permanent and transient faults has increased significantly. Research for fault tolerant processors has primarily focused on the conventional processor architectures. Neural network computing has been employed to solve a wide range of problems. This paper presents a design and implementation of a physical neural network that is resilient to permanent hardware faults. To achieve scalability, it uses tiled neuron clusters where neuron outputs are efficiently forwarded to the target neurons using source based spanning tree routing. To achieve fault resilience in the face of increasing number of permanent hardware failures, the design proactively preserves neural network computing performance by selectively replicating performance critical neurons. Furthermore, the paper presents a spanning tree recovery solution that mitigates disruption to distribution of neuron outputs caused by failed neuron clusters. The proposed neuron cluster design is implemented in Verilog. We studied the fault resilience performance of the described design using a RBM neural network trained for classifying handwritten digit images. Results demonstrate that our approach can achieve improved fault resilience performance by replicating only 5% most important neurons.","","","","10.1145/2656106.2656126","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972477","","Circuit faults;Hardware;Neural networks;Neurons;Probes;Routing;Transient analysis","fault diagnosis;hardware description languages;microprocessor chips;network routing;neural chips;trees (mathematics)","RBM neural network;Verilog;device scaling engineering;device sizes shrinking;electronic technologies;fault resilience performance;fault resilient physical neural networks;fault tolerant processors;handwritten digit images classification;neural network computing;neuron outputs;performance critical neurons;permanent hardware failures;permanent hardware faults;processor architectures;single chip;source based spanning tree routing;spanning tree recovery solution;tiled neuron clusters;total circuit sensitivity;transient faults;transistors","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"COREFAB: Concurrent reconfigurable fabric utilization in heterogeneous multi-core systems","Grudnitsky, A.; Bauer, L.; Henkel, J.","Karlsruhe Inst. of Technol. (KIT), Karlsruhe, Germany","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","Application-specific accelerators may provide considerable speedup in single-core systems with a runtime-reconfigurable fabric (for simplicity called “fabric” in the following). A reconfigurable core, i.e. processor core pipeline coupled to a fabric, can be integrated along with regular general purpose processor cores (GPPs)into a reconfigurable multi-core system with widely improved system performance. As most applications only use a fraction of the available fabric at a time, making the fabric usable by the GPPs (in addition to the reconfigurable core) in such a multi-core system is desirable. Existing work focused on algorithms that decide the amount of fabric that is assigned to each core in a multicore system. However, when multiple cores access the fabric simultaneously, they are either limited to serialized fabric access or, when parallel access is supported, the size of the fabric share assigned to a core is inflexible and tends to be over- or undersized for the running application, thereby not efficiently utilizing the fabric. We propose a novel approach that allows GPPs to access the fabric of the reconfigurable core and that enables concurrent fabric utilization on-the-fly through merging fabric accesses from different cores at run-time. Compared to state-of-the art, our approach improves performance of the GPPs in a reconfigurable multi-core system by 1.3× on average, without reducing the performance of the reconfigurable core.","","","","10.1145/2656106.2656119","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972458","Core;Reconfigurable Fabric Sharing;Reconfigurable Processor; Heterogeneous Multi","Corporate acquisitions;Fabrics;Kernel;Merging;Multicore processing;Ports (Computers);Silicon","concurrency (computers);multiprocessing systems;parallel processing;performance evaluation;pipeline processing;reconfigurable architectures","COREFAB;GPP;application-specific accelerators;concurrent reconfigurable fabric utilization;general purpose processor cores;heterogeneous multicore systems;parallel access;processor core pipeline;reconfigurable multicore system;runtime-reconfigurable fabric;serialized fabric access;single-core systems;system performance","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"CAPED: Context-aware personalized display brightness for mobile devices","Schuchhardt, M.; Jha, S.; Ayoub, R.; Kishinevsky, M.; Memik, G.","","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","The display remains the primary user interface on many computing devices, ranging from traditional devices such as desktops and laptops, to the more pervasive devices such as smartphones and smartwatches. Thus, the overall user experience with these computing devices is greatly determined by the display subsystem. Ideal display brightness is critical to good user experience, but actually predicting the ideal brightness level which would most satisfy the user is a challenge. Finding the right screen brightness is even more challenging on mobile devices (which is the focus of this work), as the screen tends to be one of the most power consuming components. Currently, the control of display brightness is usually done through a simplistic, static one-size-fits-all model which chooses a fixed brightness level for a given ambient light condition. Our user study and survey of research literature on vision and perception establish that the simplistic model currently used for display brightness control is not sufficient. The ideal display brightness level varies from one user to another. Furthermore, in addition to ambient light, we identify additional contextual data that also affect the ideal brightness. We propose a new system, Context-Aware PErsonalized Display (CAPED), that uses online learning to control the display brightness, and is theoretically and practically shown to improve prediction accuracy over time. CAPED enables personalization of brightness control as well as exploitation of richer contextual data to better predict the right display brightness. Our user study shows that CAPED improves the state-of-the-art brightness control techniques with a 41.9% improvement in mean absolute prediction accuracy. Our user study also shows that on average the users had 0.8 point higher satisfaction on a 5-point scale. In other words, CAPED improves the average satisfaction by 23.5% compared to the default scheme.","","","","10.1145/2656106.2656116","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972472","","Adaptation models;Brightness;Context;Context modeling;Mobile communication;Predictive models;Visualization","brightness;computer displays;display devices;mobile radio;ubiquitous computing;user interfaces","CAPED;ambient light condition;brightness control techniques;computing devices;context-aware personalized display brightness;display brightness control;display subsystem;fixed brightness level;ideal display brightness level;mean absolute prediction accuracy;mobile devices;online learning;pervasive devices;power consuming components;primary user interface;screen brightness;smartphones;smartwatches;static one-size-fits-all model;user experience","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"The improbable but highly appropriate marriage of 3D stacking and neuromorphic accelerators","Belhadj, B.; Valentian, A.; Vivet, P.; Duranton, M.; He, L.; Temam, O.","Leti, CEA, Grenoble, France","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","9","3D stacking is a promising technology (low latency/power/area, high bandwidth); its main shortcoming is increased power density. Simultaneously, motivated by energy constraints, architectures are evolving towards greater customization, with tasks delegated to accelerators. Due to the widespread use of machine-learning algorithms and the re-emergence of neural networks (NNs) as the preferred such algorithms, NN accelerators are receiving increased at-tention. They turn out to be well matched to 3D stacking: inherently 3D structures with a low power density and high across-layer bandwidth requirements. We present what is, to the best of our knowledge, the first 3D stacked NN accelerator.","","","","10.1145/2656106.2656130","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972454","3D stacking;neuromorphic accelerator;spiking neural network","Accuracy;Biological neural networks;Hardware;Neuromorphics;Neurons;Stacking;Three-dimensional displays","neural nets;particle accelerators;three-dimensional integrated circuits","3D stacked NN accelerator;3D stacking;3D structures;NN accelerators;across-layer bandwidth requirements;energy constraints;machine-learning algorithms;neural networks;neuromorphic accelerators;power density","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Automatic custom instruction identification in memory streaming algorithms","Haass, M.; Bauer, L.; Henkel, J.","Karlsruhe Inst. of Technol. (KIT), Karlsruhe, Germany","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","9","Application-specific instruction set processors (ASIPs) extend the instruction set of a general purpose processor by dedicated custom instructions (CIs). In the last decade, reconfigurable processors advanced this concept towards runtime reconfiguration to increase the efficiency and adaptivity. Compiler support for automatic identification and implementation of ASIP CIs exists commercially and on research platforms, but these compilers do not support CIs with memory accesses, as ASIP CIs typically work on register file data. While being acceptable for ASIPs, this imposes a limitation for reconfigurable processors as they achieve their performance by exploiting data-level parallelism. Consequently, we propose a novel approach to CI identification for runtime reconfigurable processors with support for memory operations in contrast to previous works that explicitly exclude them. Our algorithm extracts memory access patterns which allows us to abstract from single memory operations and merge accesses to optimally utilize the available memory bandwidth. We implemented our algorithm in a state-of-the-art compiler framework. The largest CI identified in our benchmarks consists of 2071 nodes (average 999 nodes), and a single generated CI can cover a whole computational kernel (up to 99%).","","","","10.1145/2656106.2656114","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972459","Reconfigurable architecture;custom instruction generation;load/store merging;streaming memory access","Arrays;Hardware;Ports (Computers);Program processors;Registers;Runtime","bandwidth allocation;instruction sets;parallel processing;program compilers;reconfigurable architectures","ASIP;CI identification;application-specific instruction set processors;compiler framework;custom instruction identification;data-level parallelism;memory bandwidth utilization;memory streaming algorithms;runtime reconfigurable processors","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"EnVM: Virtual memory design for new memory architectures","Roy, P.; Manoharan, M.; Weng Fai Wong","Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","Virtual memory is optimized for SRAM-based memory devices in which memory accesses are symmetric, i.e., the latency of read and write accesses are similar. Unfortunately, with the emergence of newer non-volatile memory (NVM) technologies that are denser and more energy efficient, this assumption is no longer valid. For example, STT-RAMs are known to have high write latencies and limited write endurance which the virtual memory is unaware of. A popular architecture is a hybrid cache that uses both SRAM and NVM. There are a number of proposals for such architectures at nearly all the levels of the cache. However, these proposals are often self-contained with monitoring and management schemes implemented with special hardware at the level where the cache is deployed. With moves to use NVM at several levels of the memory hierarchy, such solutions may lead to duplication and higher over-heads. Worse, because the management algorithms implemented can be different at different levels of memory, it may lead to negative interference between them resulting in impaired efficiency. In this paper, we propose a virtual memory design, EnVM, that takes into consideration the idiosyncrasies of NVM-based hybrid caches. The new virtual memory layout is implicitly used to allocate data to NVM and SRAM at any level of the memory hierarchy and is not dependant on the particular arrangements of the two partitions. The proposed design successfully filters out write operations and allocates them to SRAM. Moreover, it can be applied to any existing fine-grained data allocation technique to enhance the efficiency of these memories.","","","","10.1145/2656106.2656121","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972465","","Arrays;Hardware;Memory management;Nonvolatile memory;Operating systems;Random access memory;Resource management","SRAM chips;storage management","EnVM;NVM technology;NVM-based hybrid cache;SRAM-based memory devices;cache management;cache monitoring;fine-grained data allocation technique;memory access;memory architecture;memory hierarchy;nonvolatile memory technology;read access;static random access memory;virtual memory design;write access;write operation","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
"Automated ISA branch coverage analysis and test case generation for retargetable instruction set simulators","Wagstaff, H.; Spink, T.; Franke, B.","Univ. of Edinburgh, Edinburgh, UK","Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on","20150108","2014","","","1","10","Processor design tools integrate in their workflows generators for instruction set simulators (ISS) from architecture descriptions. However, it is difficult to validate the correctness of these simu-lators. ISA coverage analysis is insufficient to isolate modelling faults, which might only be exposed in corner cases. We present a novel ISA branch coverage analysis, which considers every possible execution path within an instruction and, on demand, generates new test cases to cover the missing paths. We have applied this analysis to industry standard EEMBC and SPEC CPU2006 benchmarks and show that for an ARM V5T model neither of these benchmark suites provides a sufficient ISA coverage to exercise every path through each instruction of the whole instruction set.","","","","10.1145/2656106.2656113","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972468","","Benchmark testing;Context;Generators;Registers;Semantics;Vectors","fault tolerant computing;instruction sets;program diagnostics;program testing","ARM V5T model;EEMBC benchmark;ISA branch coverage analysis;ISS;SPEC CPU2006 benchmark;instruction execution path;modelling fault isolation;processor design tools;retargetable instruction set simulators;test case generation","","0","","","","","12-17 Oct. 2014","","IEEE","IEEE Conference Publications"
